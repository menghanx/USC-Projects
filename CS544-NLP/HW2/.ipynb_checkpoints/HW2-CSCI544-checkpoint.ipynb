{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xmh91\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import nltk\n",
    "import time\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload After restart\n",
    "w2v_google_model = api.load('word2vec-google-news-300')\n",
    "model = Word2Vec.load(\"tained_word2vec.model\")\n",
    "#model = Word2Vec.load(\"tained_word2vec_random1.model\")\n",
    "w2v_review_model = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload After restart\n",
    "### Raw review data in df\n",
    "df = pd.read_csv(\"250k_classified_reviews.csv\", sep='\\t')\n",
    "\n",
    "# Calculate Accuracy\n",
    "def getAccuracy(out, labels):\n",
    "    _, predict = torch.max(out.data, 1)\n",
    "    total = labels.shape[0]*1.0\n",
    "    correct = (labels == predict).sum().item()\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reload After restart\n",
    "# Cleaned data in df\n",
    "df = pd.read_csv(\"250k_cleaned_review_random.csv\", sep='\\t')\n",
    "# Drop review with nan as value\n",
    "df = df.dropna(subset=['review'])\n",
    "# Strip extra white spaces\n",
    "df['review'] = [review.strip() for review in df['review']]\n",
    "# Replace empty string with NaN\n",
    "df['review'].replace('', np.nan, inplace=True)\n",
    "# Drop review with nan as value\n",
    "df = df.dropna(subset=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm = pd.concat([df[df['class'] == 1], df[df['class'] == 2]])\n",
    "x_sm = df_sm[\"review\"]\n",
    "y_sm = df_sm[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249848, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xmh91\\.conda\\envs\\py39-CS544\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\xmh91\\.conda\\envs\\py39-CS544\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    " raw_df = pd.read_csv(\"amazon_reviews_us_Kitchen_v1_00.tsv.gz\", sep='\\t', error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep Reviews and Ratings\n",
    "<p>Same as in HW1, entries with empty review content is dropped here</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " df = raw_df[['star_rating', 'review_body']].dropna(subset=['review_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### select 50k reviews randomly from each rating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1=df[df['star_rating'] == 1 ].sample(n = 50000, random_state = 1)\n",
    "#df2=df[df['star_rating'] == 2 ].sample(n = 50000, random_state = 2)\n",
    "#df3=df[df['star_rating'] == 3 ].sample(n = 50000, random_state = 3)\n",
    "#df4=df[df['star_rating'] == 4 ].sample(n = 50000, random_state = 4)\n",
    "#df5=df[df['star_rating'] == 5 ].sample(n = 50000, random_state = 5)\n",
    "\n",
    "df1=df[df['star_rating'] == 1 ].sample(n = 50000, random_state = 6)\n",
    "df2=df[df['star_rating'] == 2 ].sample(n = 50000, random_state = 7)\n",
    "df3=df[df['star_rating'] == 3 ].sample(n = 50000, random_state = 8)\n",
    "df4=df[df['star_rating'] == 4 ].sample(n = 50000, random_state = 9)\n",
    "df5=df[df['star_rating'] == 5 ].sample(n = 50000, random_state = 10)\n",
    "\n",
    "frame = [df1, df2, df3, df4, df5]\n",
    "df = pd.concat(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling Reviews:\n",
    "We assume that ratings more than 3 denote positive 1 sentiment (class 1)  <p>rating less than 3 denote negative sentiment (class 2).</p> <p>Reviews with rating 3 are considered to have neutral sentiment (class 3).</p>\n",
    "<p>Similar steps as HW1</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lebal samples according to rating\n",
    "df.loc[df['star_rating'] < 3, 'star_rating'] = 2\n",
    "df.loc[df['star_rating'] > 3, 'star_rating'] = 1\n",
    "df.loc[df['star_rating'] == 3, 'star_rating'] = 3\n",
    "\n",
    "df.columns = [\"class\", \"review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store 250k classified reviews to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('250k_classified_reviews.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload df from csv file to same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"250k_classified_reviews.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['review']\n",
    "y = df['class']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = 1, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130575    I ordered this item but returned it before usi...\n",
       "246655    I love these little spoons more and more every...\n",
       "84533                                                  poor\n",
       "61397     Like many other reviewers I was so pleased wit...\n",
       "191294    Have small sandwich shop and this slicer for t...\n",
       "32434     The item barely blends, within the second use ...\n",
       "239084    I waited to post a review on this set.  I thin...\n",
       "103095    I got this item as a gift for someone and appa...\n",
       "181813    I was looking for a pot that I could cook in a...\n",
       "202662        Very fast shipping! Great product! Thank you!\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Embedding\n",
    "<p>download pretrained model word2vec-google-news-300</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(a) Experimenting with the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# king - man + woman = queen\n",
    "queen = w2v_google_model.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beijing', 0.54819655418396),\n",
       " ('Shanghai', 0.45855337381362915),\n",
       " ('Bejing', 0.44562390446662903),\n",
       " ('Taipei', 0.44435012340545654),\n",
       " ('Beijng', 0.43815258145332336),\n",
       " ('Liaoning_Province', 0.4362034797668457),\n",
       " ('Chongqing_Evening', 0.4351152181625366),\n",
       " ('Hu', 0.4345473051071167),\n",
       " ('Hangzhou', 0.4315878748893738),\n",
       " ('Guangdong', 0.42471086978912354)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# capitalA - countryA + countryB = capitalB?\n",
    "capital = w2v_google_model.most_similar(positive=['Washington', 'China'], negative=['US'])\n",
    "capital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2(b) Train a Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 166.07072591781616 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# To track the training time\n",
    "start_time = time.time()\n",
    "\n",
    "reviews_tokenized = [nltk.word_tokenize(review) for review in x_train.values]\n",
    "\n",
    "model = Word2Vec(sentences=reviews_tokenized, vector_size=300, window=11, min_count=10)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 60.95007944107056 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# To track the training time\n",
    "start_time = time.time()\n",
    "\n",
    "reviews_tokenized = [review.split() for review in x_train.values]\n",
    "\n",
    "model = Word2Vec(sentences=reviews_tokenized, vector_size=300, window=11, min_count=10)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"tained_word2vec.model\")\n",
    "\n",
    "#model.save(\"tained_word2vec_random1.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import self trained model if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"tained_word2vec.model\")\n",
    "#model = Word2Vec.load(\"tained_word2vec_random1.model\")\n",
    "w2v_review_model = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9568785 , -0.68531615,  1.5739079 , -0.45190114, -1.4412427 ,\n",
       "       -2.4282768 ,  1.9175682 , -2.2939672 , -1.1219257 , -0.18551426,\n",
       "        2.1371415 ,  0.8804415 , -1.6587572 , -0.98886263, -1.0349673 ,\n",
       "        0.26877254, -0.36356688,  1.4631878 , -0.11988018,  0.2936088 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_review_model[\"great\"][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.5365573763847351),\n",
       " ('11\\\\\\\\\"', 0.49044883251190186),\n",
       " ('duvet', 0.47028398513793945),\n",
       " ('13&#34;', 0.45197463035583496),\n",
       " ('9&#34;', 0.43679526448249817),\n",
       " ('Tablespoon', 0.4344377815723419),\n",
       " ('XL', 0.43140971660614014),\n",
       " ('3qt', 0.4296371340751648),\n",
       " ('saucepan,', 0.4273172914981842),\n",
       " ('sizing', 0.42034393548965454)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queen = w2v_review_model.most_similar(positive=['king', 'woman'], negative=['man'])\n",
    "queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('USA.<br', 0.5001760125160217),\n",
       " ('Germany', 0.4997105598449707),\n",
       " ('America.', 0.4971461594104767),\n",
       " ('China!', 0.49489426612854004),\n",
       " ('Spain', 0.4916647672653198),\n",
       " ('Germany,', 0.48961275815963745),\n",
       " ('Spain.', 0.4873398542404175),\n",
       " ('England', 0.4848261773586273),\n",
       " ('Thailand.', 0.4846789240837097),\n",
       " ('USA', 0.4741653800010681)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital = w2v_review_model.most_similar(positive=['Washington', 'China'], negative=['US'])\n",
    "capital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you conclude from comparing vectors generated by yourself and the pretrained model?\n",
    "<p>\n",
    "    The classic king - man + woman = queen example worked as intended for both models. The second example I tried was to compute the capital city by subtracting the country then plus another country, the result of pretrained model is correct again, with Beijing being the captial of China, also, there are other Chinese cities listed in the top ten results. However, for the self trained model, the top 10 results are only different countries, I think the self trained model did not ditinguish the difference between a country and a city.\n",
    "</p>\n",
    "<p>\n",
    "    Based on the king-queen and capital city result from above. I think the pretrained model has a much bigger corpus compare to Amazon reviews. Google model has 3 million vocabulary, which have more sentences from articles of political and international news and topics, while Amazon reviews are only focused on the quality of the kitchen product so maybe the relationship between city and country rarely appear in the reviews, thus it's difficult for the self-trained model to learn that the capital of \"China\" is \"Beijing\".  \n",
    "</p>\n",
    "<p>\n",
    "    I think in order to get a good prediction from word2vec, the training data and testing data should share similar text context or even come from the same corpus.\n",
    "</p>\n",
    "\n",
    "### Which of the Word2Vec models seems to encode semantic similarities between words better?\n",
    "<p>\n",
    "As dicussed above, the google-news model is better.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and preprocess df_sm to keep only the important word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sm['review'] = df_sm['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_url(s):\n",
    "    # parse html\n",
    "    soup = BeautifulSoup(s, \"html.parser\")\n",
    "    \n",
    "    for data in soup(['style', 'script']):\n",
    "        # remove tags\n",
    "        data.decompose()\n",
    "    # replace url with empty string and return\n",
    "    return re.sub(r\"http\\S+\", \"\", ' '.join(soup.stripped_strings))\n",
    "\n",
    "# Remove HTML markups and URL in text format\n",
    "#df_sm['review'] = [ remove_html_url(review) for review in df_sm['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "        \"a'ight\": \"alright\",\n",
    "        \"ain't\": \"am not\",\n",
    "        \"amn't\": \"am not\",\n",
    "        \"arencha\": \"are not you\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"‘bout\": \"about\",\n",
    "        \"cannot\": \"can not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"cap’n\": \"captain\",\n",
    "        \"cause\": \"because\",\n",
    "        \"’cept\": \"except\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"dammit\": \"damn it\",\n",
    "        \"daren't\": \"dare not\",\n",
    "        \"daresn't\": \"dare not\",\n",
    "        \"dasn't\": \"dare not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"dunno\": \"do not know\",\n",
    "        \"d'ye\": \"did you\",\n",
    "        \"e'en\": \"even\",\n",
    "        \"e'er\": \"ever\",\n",
    "        \"em\": \"them\",\n",
    "        \"everybody's\": \"everybody is\",\n",
    "        \"everyone's\": \"everyone is\",\n",
    "        \"fo’c’sle\": \"forecastle\",\n",
    "        \"’gainst\": \"against\",\n",
    "        \"g'day\": \"good day\",\n",
    "        \"gimme\": \"give me\",\n",
    "        \"giv'n\": \"given\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"gon't\": \"go not\",\n",
    "        \"gotta\": \"got to\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"had've\": \"had have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"helluva\": \"hell of a\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"here's\": \"here is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"howdy\": \"how do you do\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how're\": \"how are\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'd've\": \"i would have\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"imma\": \"i am about to\",\n",
    "        \"i'm'o\": \"i am going to\",\n",
    "        \"innit\": \"is it not\",\n",
    "        \"ion\": \"i do not\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"iunno\": \"i do not know\",\n",
    "        \"kinda\": \"kind of\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"may've\": \"may have\",\n",
    "        \"methinks\": \"i think\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"‘neath\": \"beneath\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"nal\": \"and all\",\n",
    "        \"ne'er\": \"never\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"o'er\": \"over\",\n",
    "        \"ol'\": \"old\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"‘round\": \"around\",\n",
    "        \"s\": \"is\",\n",
    "        \"shalln't\": \"shall not\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"somebody's\": \"somebody is\",\n",
    "        \"someone's\": \"someone is\",\n",
    "        \"something's\": \"something is\",\n",
    "        \"so're\": \"so are\",\n",
    "        \"so’s\": \"so has\",\n",
    "        \"so’ve\": \"so have\",\n",
    "        \"that'll\": \"that will\",\n",
    "        \"that're\": \"that are\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'll\": \"there will\",\n",
    "        \"there're\": \"there are\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"these're\": \"these are\",\n",
    "        \"these've\": \"these have\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"this's\": \"this is\",\n",
    "        \"those're\": \"those are\",\n",
    "        \"those've\": \"those have\",\n",
    "        \"thout\": \"without\",\n",
    "        \"’til\": \"until\",\n",
    "        \"tis\": \"it is\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"twas\": \"it was\",\n",
    "        \"tween\": \"between\",\n",
    "        \"twere\": \"it were\",\n",
    "        \"wanna\": \"want to\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"whatcha\": \"what are you\",\n",
    "        \"what'd\": \"what did\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what're\": \"what are/what were\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where'll\": \"where will\",\n",
    "        \"where're\": \"where are\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"which'd\": \"which had\",\n",
    "        \"which'll\": \"which will\",\n",
    "        \"which're\": \"which are\",\n",
    "        \"which's\": \"which is\",\n",
    "        \"which've\": \"which have\",\n",
    "        \"who'd\": \"who would\",\n",
    "        \"who'd've\": \"who would have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who're\": \"who are\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why'd\": \"why did\",\n",
    "        \"why're\": \"why are\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"willn't\": \"will not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wonnot\": \"will not\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all'd'n't've\": \"you all would not have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all'ren't\": \"you all are not\",\n",
    "        \"y'at\": \"you at\",\n",
    "        \"yes’m\": \"yes madam\",\n",
    "        \"yessir\": \"yes sir\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"yrs\": \"years\",\n",
    "        \"ur\" : \"your\",\n",
    "        \"urs\" : \"yours\"\n",
    "}\n",
    "    \n",
    "def contractionfunction(s): \n",
    "    for word in s.split(\" \"):\n",
    "        if word in contractions:\n",
    "            s = s.replace(word, contractions[word])\n",
    "    return s\n",
    "\n",
    "#df_sm['review'] = [ contractionfunction(review) for review in df_sm['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetical(s):\n",
    "    # remove single quote in word like \" husband's \"\n",
    "    s = re.sub(r'\\'', '', s)\n",
    "    \n",
    "    # replace non-alphabetical by whitespace\n",
    "    s = re.sub(r\"[^a-zA-Z]\", ' ', s)\n",
    "    \n",
    "    # remove punctuation and return\n",
    "    #return ' '.join([word.strip(string.punctuation) for word in s.split(\" \")])\n",
    "    return s\n",
    "\n",
    "#df_sm['review'] = [ remove_non_alphabetical(review) for review in df_sm['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sm['review'] = [ re.sub(r'\\s+', ' ', review) for review in df_sm['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    # only keep words not in stopwords set\n",
    "    filtered_words = [word for word in s.split(\" \") if word not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "#df_sm['review'] = [ remove_stopwords(review) for review in df_sm['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# simple lemmatize\n",
    "def easy_lemmatize(s):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in s.split(\" \")]\n",
    "    return \" \".join(lemmatized_words)\n",
    "    \n",
    "# use simple lemmatize\n",
    "#df_sm['review'] = [ easy_lemmatize(review) for review in df_sm['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sm['review'] = [review.strip() for review in df_sm['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store cleaned and preprocessed data into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xmh91\\.conda\\envs\\py39-CS544\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"http://www.consumerreports.org/cro/magazine-archive/2011/november/appliances/can-you-stop-stirring/overview/index.htm\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\xmh91\\.conda\\envs\\py39-CS544\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.facebook.com/cherischocolates\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Keep only class=1 and class =2 data\n",
    "#df_sm = pd.concat([df[df['class'] == 1], df[df['class'] == 2]])\n",
    "\n",
    "\n",
    "def data_clean(x):\n",
    "    x = x.str.lower()\n",
    "    x = [ remove_html_url(review) for review in x ]\n",
    "    x = [ contractionfunction(review) for review in x ]\n",
    "    x = [ remove_non_alphabetical(review) for review in x ]\n",
    "    x = [ re.sub(r'\\s+', ' ', review) for review in x ]\n",
    "    x = [ remove_stopwords(review) for review in x ]\n",
    "    x = [ easy_lemmatize(review) for review in x ]\n",
    "    x = [ review.strip() for review in x]\n",
    "    return x\n",
    "\n",
    "df[\"review\"] = data_clean(df[\"review\"])\n",
    "df.replace('', np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sm.to_csv('200k_cleaned_review_binary_classes.csv', sep='\\t', index=False)\n",
    "\n",
    "df.to_csv('250k_cleaned_review_random.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload df_sm if needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sm = pd.read_csv(\"200k_cleaned_review_binary_classes.csv\", sep='\\t')\n",
    "\n",
    "df = pd.read_csv(\"250k_cleaned_review_random.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop review with nan as value\n",
    "df = df.dropna(subset=['review'])\n",
    "# Strip extra white spaces\n",
    "df['review'] = [review.strip() for review in df['review']]\n",
    "# Replace empty string with NaN\n",
    "df['review'].replace('', np.nan, inplace=True)\n",
    "# Drop review with nan as value\n",
    "df = df.dropna(subset=['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199884, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sm = pd.concat([df[df['class'] == 1], df[df['class'] == 2]])\n",
    "\n",
    "df_sm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/test data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm = df_sm.dropna(subset=['review'])\n",
    "# Split train and test data\n",
    "x_sm = df_sm[\"review\"]\n",
    "y_sm = df_sm[\"class\"]\n",
    "\n",
    "x_train_sm, x_test_sm, y_train_sm, y_test_sm = train_test_split(x_sm, y_sm, random_state = 12, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create document vector for each review based on the average vector of each word in each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Vectors for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug DocVec Function\n",
    "def computeDocVec(w2v_model, train_input):\n",
    "    docVec = []\n",
    "    \n",
    "    for review in train_input:\n",
    "        rev_vec = []\n",
    "        for word in review.split():\n",
    "            if word in w2v_model:\n",
    "                rev_vec.append(w2v_model[word])\n",
    "            else:\n",
    "        docVec.append(np.mean(rev_vec, axis=0))\n",
    "    return docVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Vectors\n",
    "review_train_vec = scipy.sparse.csr_matrix(np.array(computeDocVec(w2v_review_model, x_train_sm), dtype=\"float64\"))\n",
    "review_test_vec = scipy.sparse.csr_matrix(np.array(computeDocVec(w2v_review_model, x_test_sm), dtype=\"float64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Vectors\n",
    "google_train_vec = scipy.sparse.csr_matrix(np.array(computeDocVec(w2v_google_model, x_train_sm), dtype=\"float64\"))\n",
    "google_test_vec = scipy.sparse.csr_matrix(np.array(computeDocVec(w2v_google_model, x_test_sm), dtype=\"float64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data output\n",
    "y_train_sm = y_train_sm.astype('int')\n",
    "y_test_sm = y_test_sm.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# set iteration and learning rate\n",
    "max_iter = 1000\n",
    "eta0 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #1. Self-trained W2V Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Self-trained W2V Accuracy:\n",
      "0.8076894214173149\n"
     ]
    }
   ],
   "source": [
    "# Train the model with train dataset and train targets\n",
    "ppn_self_clf = Perceptron()\n",
    "ppn_self_clf.fit(review_train_vec, y_train_sm)\n",
    "self_y_pred = ppn_self_clf.predict(review_test_vec)\n",
    "print(\"Perceptron Self-trained W2V Accuracy:\")\n",
    "print(metrics.accuracy_score(y_test_sm, self_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #2. Pre-trained W2V Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Pretrained W2V Accuracy:\n",
      "0.5128448858093404\n"
     ]
    }
   ],
   "source": [
    "# Train the model with train dataset and train targets\n",
    "ppn_google_clf = Perceptron()\n",
    "ppn_google_clf.fit(google_train_vec, y_train_sm)\n",
    "google_y_pred = ppn_google_clf.predict(review_test_vec)\n",
    "print(\"Perceptron Pretrained W2V Accuracy:\")\n",
    "print(metrics.accuracy_score(y_test_sm, google_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perception TF-IDF Vectors from HW1\n",
    "<p>Accuracy</p>\n",
    "<p>0.89514375</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train the model with train dataset and train targets\n",
    "svm = LinearSVC(random_state=0, tol=1e-5, max_iter=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #3. Self-trained W2V SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 258.5454113483429 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xmh91\\.conda\\envs\\py39-CS544\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "# To track the training time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "svm.fit(review_train_vec, y_train_sm)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on train dataset\n",
    "y_pred = svm.predict(review_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Self-trained W2V Accuracy:\n",
      "0.8467368737023788\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Self-trained W2V Accuracy:\")\n",
    "print(metrics.accuracy_score(y_test_sm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #4. Pre-trained W2V SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(random_state=0, tol=1e-5, max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 47.86609363555908 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# To track the training time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "svm.fit(google_train_vec, y_train_sm)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on train dataset\n",
    "y_pred = svm.predict(review_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Pretrained W2V Accuracy:\n",
      "0.533331665707782\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Pretrained W2V Accuracy:\")\n",
    "print(metrics.accuracy_score(y_test_sm, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Conclusion: </p>\n",
    "<p>The perceptron and SVM trained on the vectors generated by the self-trained Word2Vec model could accurately (80%~85%) predict the sentiment of the review, while the two models trained on vectors generated by pre-trained Word2Vec model only have around 50% accuracy, basically like coin-flip guess. I think the self-trained W2V model performs better because the entire corpus contains only reviews, while the pre-trained W2V model were trained on news articles.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Netword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #5. Self-trained W2V FNN-Binary - DocVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerMLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 50)\n",
    "        self.fc2 = torch.nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = F.relu(self.fc1(x))\n",
    "        out = self.fc2(hidden)\n",
    "        return out\n",
    "\n",
    "# Calculate Accuracy\n",
    "def getAccuracy(out, labels):\n",
    "    _, predict = torch.max(out.data, 1)\n",
    "    total = labels.shape[0]*1.0\n",
    "    correct = (labels == predict).sum().item()\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tensors\n",
    "# Split train and test\n",
    "x_train_sm, x_test_sm, y_train_sm, y_test_sm = train_test_split(x_sm, y_sm, test_size = 0.2)\n",
    "\n",
    "features_train = np.array(computeDocVec(w2v_review_model, x_train_sm), dtype=\"float64\")\n",
    "features_test = np.array(computeDocVec(w2v_review_model, x_test_sm), dtype=\"float64\")\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=torch.from_numpy(features_train).float(),torch.from_numpy(features_test).float(),torch.from_numpy(y_train_sm.to_numpy()).type(torch.LongTensor),torch.from_numpy(y_test_sm.to_numpy()).type(torch.LongTensor)\n",
    "\n",
    "# Update lables to be 0 and 1\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary classification nn - self trained W2V\n",
      "Self W2V Model trainning on GPU...\n",
      "Epoch: 500 train_loss: 0.6085 valid_loss: 0.6100 accuracy: 0.8025\n",
      "Epoch: 1000 train_loss: 0.4403 valid_loss: 0.4441 accuracy: 0.8248\n",
      "Epoch: 1500 train_loss: 0.3991 valid_loss: 0.4042 accuracy: 0.8353\n",
      "Epoch: 2000 train_loss: 0.3809 valid_loss: 0.3862 accuracy: 0.8414\n",
      "Epoch: 2500 train_loss: 0.3700 valid_loss: 0.3755 accuracy: 0.8443\n",
      "Epoch: 3000 train_loss: 0.3617 valid_loss: 0.3673 accuracy: 0.8471\n",
      "Epoch: 3500 train_loss: 0.3548 valid_loss: 0.3605 accuracy: 0.8490\n",
      "Epoch: 4000 train_loss: 0.3486 valid_loss: 0.3544 accuracy: 0.8513\n",
      "Epoch: 4500 train_loss: 0.3428 valid_loss: 0.3490 accuracy: 0.8531\n",
      "Epoch: 5000 train_loss: 0.3374 valid_loss: 0.3441 accuracy: 0.8557\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary classification nn - self trained W2V\")\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Self W2V Model trainning on GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Self+ W2V Model trainning on CPU...\")\n",
    "\n",
    "# Initiate model\n",
    "model = ThreeLayerMLP(300)\n",
    "model.to(device)\n",
    "\n",
    "# Configure Hyperparamters\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.00005\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 5000\n",
    "batch = 10000\n",
    "\n",
    "X_train_cuda = X_train.to(device)\n",
    "X_test_cuda = X_test.to(device)\n",
    "Y_train_cuda = Y_train.to(device)\n",
    "Y_test_cuda = Y_test.to(device)\n",
    "\n",
    "for e in range(1,epochs+1):\n",
    "    y_pred = model(X_train_cuda)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, Y_train_cuda)\n",
    "    optimizer.zero_grad()\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    # Update weights and bias\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Report Accuracy each 100 epochs\n",
    "    if e % 500 == 0:\n",
    "        test_out = model(X_test_cuda)\n",
    "        valid_loss = loss_fn(test_out, Y_test_cuda)\n",
    "        print(\"Epoch: {} train_loss: {:.4f} valid_loss: {:.4f} accuracy: {:.4f}\".format(e, loss, valid_loss, getAccuracy(test_out, Y_test_cuda)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500965888\n",
      "538968064\n"
     ]
    }
   ],
   "source": [
    "# Clear GPU Cache After Training\n",
    "del X_train_cuda\n",
    "del X_test_cuda\n",
    "del Y_train_cuda\n",
    "del Y_test_cuda\n",
    "torch.save(model,\"self_trained_nn_model_binary.pkl\")\n",
    "del model\n",
    "del y_pred\n",
    "del loss\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #6. Pre-trained W2V FNN-Binary - DocVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "x_train_sm, x_test_sm, y_train_sm, y_test_sm = train_test_split(x_sm, y_sm, test_size = 0.2)\n",
    "\n",
    "features_train = np.array(computeDocVec(w2v_google_model, x_train_sm), dtype=\"float64\")\n",
    "features_test = np.array(computeDocVec(w2v_google_model, x_test_sm), dtype=\"float64\")\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=torch.from_numpy(features_train).float(),torch.from_numpy(features_test).float(),torch.from_numpy(y_train_sm.to_numpy()).type(torch.LongTensor),torch.from_numpy(y_test_sm.to_numpy()).type(torch.LongTensor)\n",
    "\n",
    "# Update lables to be 0 and 1\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary classification nn - Google pre-trained W2V\n",
      "Google W2V Model trainning on GPU...\n",
      "Epoch: 500 train_loss: 0.7796 valid_loss: 0.7777 accuracy: 0.6412\n",
      "Epoch: 1000 train_loss: 0.5728 valid_loss: 0.5708 accuracy: 0.7770\n",
      "Epoch: 1500 train_loss: 0.4959 valid_loss: 0.4941 accuracy: 0.7880\n",
      "Epoch: 2000 train_loss: 0.4581 valid_loss: 0.4570 accuracy: 0.8000\n",
      "Epoch: 2500 train_loss: 0.4360 valid_loss: 0.4358 accuracy: 0.8090\n",
      "Epoch: 3000 train_loss: 0.4222 valid_loss: 0.4228 accuracy: 0.8155\n",
      "Epoch: 3500 train_loss: 0.4130 valid_loss: 0.4142 accuracy: 0.8197\n",
      "Epoch: 4000 train_loss: 0.4054 valid_loss: 0.4072 accuracy: 0.8223\n",
      "Epoch: 4500 train_loss: 0.3984 valid_loss: 0.4008 accuracy: 0.8249\n",
      "Epoch: 5000 train_loss: 0.3924 valid_loss: 0.3953 accuracy: 0.8269\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary classification nn - Google pre-trained W2V\")\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Google W2V Model trainning on GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Google W2V Model trainning on CPU...\")\n",
    "\n",
    "# Initiate model\n",
    "model = ThreeLayerMLP(300)\n",
    "model.to(device)\n",
    "\n",
    "# Configure Hyperparamters\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 5000\n",
    "batch = 10000\n",
    "\n",
    "X_train_cuda = X_train.to(device)\n",
    "X_test_cuda = X_test.to(device)\n",
    "Y_train_cuda = Y_train.to(device)\n",
    "Y_test_cuda = Y_test.to(device)\n",
    "\n",
    "for e in range(1,epochs+1):\n",
    "    y_pred = model(X_train_cuda)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, Y_train_cuda)\n",
    "    optimizer.zero_grad()\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    # Update weights and bias\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Report Accuracy each 100 epochs\n",
    "    if e % 500 == 0:\n",
    "        test_out = model(X_test_cuda)\n",
    "        valid_loss = loss_fn(test_out, Y_test_cuda)\n",
    "        print(\"Epoch: {} train_loss: {:.4f} valid_loss: {:.4f} accuracy: {:.4f}\".format(e, loss, valid_loss, getAccuracy(test_out, Y_test_cuda)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearCache():\n",
    "    # Clear GPU Cache After Training\n",
    "    del X_train_cuda\n",
    "    del X_test_cuda\n",
    "    del Y_train_cuda\n",
    "    del Y_test_cuda\n",
    "    torch.save(model,\"pre_trained_nn_model_binary.pkl\")\n",
    "    del model\n",
    "    del y_pred\n",
    "    del loss\n",
    "    del optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_allocated())\n",
    "    print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #7. Self-trained W2V FNN-Ternary - DocVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tri = df[\"review\"]\n",
    "y_tri = df[\"class\"]\n",
    "\n",
    "x_train_tri, x_test_tri, y_train_tri, y_test_tri = train_test_split(x_tri, y_tri, random_state = 11, test_size = 0.2)\n",
    "\n",
    "features_train_tri = np.array(computeDocVec(w2v_review_model, x_train_tri), dtype=\"float64\")\n",
    "features_test_tri = np.array(computeDocVec(w2v_review_model, x_test_tri), dtype=\"float64\")\n",
    "\n",
    "X_train_tri,X_test_tri,Y_train_tri,Y_test_tri=torch.from_numpy(features_train_tri).float(),torch.from_numpy(features_test_tri).float(),torch.from_numpy(y_train_tri.to_numpy()).type(torch.LongTensor),torch.from_numpy(y_test_tri.to_numpy()).type(torch.LongTensor)\n",
    "\n",
    "# Update lables to be 0 and 1\n",
    "Y_train_tri = Y_train_tri - 1\n",
    "Y_test_tri = Y_test_tri - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ternary classification nn - self trained W2V\n",
      "Self W2V Model trainning on GPU...\n",
      "Epoch: 40 train_loss: 1.0155 valid_loss: 1.0160 accuracy: 0.5659\n",
      "Epoch: 80 train_loss: 0.9640 valid_loss: 0.9655 accuracy: 0.6126\n",
      "Epoch: 120 train_loss: 0.9179 valid_loss: 0.9206 accuracy: 0.6293\n",
      "Epoch: 160 train_loss: 0.8805 valid_loss: 0.8842 accuracy: 0.6384\n",
      "Epoch: 200 train_loss: 0.8503 valid_loss: 0.8546 accuracy: 0.6455\n",
      "Epoch: 240 train_loss: 0.8261 valid_loss: 0.8309 accuracy: 0.6520\n",
      "Epoch: 280 train_loss: 0.8067 valid_loss: 0.8119 accuracy: 0.6573\n",
      "Epoch: 320 train_loss: 0.7906 valid_loss: 0.7962 accuracy: 0.6626\n",
      "Epoch: 360 train_loss: 0.7786 valid_loss: 0.7845 accuracy: 0.6663\n",
      "Epoch: 400 train_loss: 0.7697 valid_loss: 0.7758 accuracy: 0.6694\n"
     ]
    }
   ],
   "source": [
    "print(\"ternary classification nn - self trained W2V\")\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Self W2V Model trainning on GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Self W2V Model trainning on CPU...\")\n",
    "\n",
    "# Initiate model\n",
    "model = TriMLP(300)\n",
    "model.to(device)\n",
    "\n",
    "# Configure Hyperparamters\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 400\n",
    "batch = 2000\n",
    "\n",
    "X_train_cuda = X_train_tri.to(device)\n",
    "X_test_cuda = X_test_tri.to(device)\n",
    "Y_train_cuda = Y_train_tri.to(device)\n",
    "Y_test_cuda = Y_test_tri.to(device)\n",
    "\n",
    "for e in range(1,epochs+1):\n",
    "    y_pred = model(X_train_cuda)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.nll_loss(y_pred, Y_train_cuda)\n",
    "    optimizer.zero_grad()\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    # Update weights and bias\n",
    "    optimizer.step()\n",
    "    \n",
    "    if e % 40 == 0:\n",
    "        test_out = model(X_test_cuda)\n",
    "        valid_loss = F.nll_loss(test_out, Y_test_cuda)\n",
    "        print(\"Epoch: {} train_loss: {:.4f} valid_loss: {:.4f} accuracy: {:.4f}\".format(e, loss, valid_loss, getAccuracy(test_out, Y_test_cuda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #8. Pre-trained W2V FNN-Ternary - DocVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tri = df[\"review\"]\n",
    "y_tri = df[\"class\"]\n",
    "\n",
    "x_train_tri, x_test_tri, y_train_tri, y_test_tri = train_test_split(x_tri, y_tri, random_state = 11, test_size = 0.2)\n",
    "\n",
    "features_train_tri = np.array(computeDocVec(w2v_google_model, x_train_tri), dtype=\"float64\")\n",
    "features_test_tri = np.array(computeDocVec(w2v_google_model, x_test_tri), dtype=\"float64\")\n",
    "\n",
    "X_train_tri,X_test_tri,Y_train_tri,Y_test_tri=torch.from_numpy(features_train_tri).float(),torch.from_numpy(features_test_tri).float(),torch.from_numpy(y_train_tri.to_numpy()).type(torch.LongTensor),torch.from_numpy(y_test_tri.to_numpy()).type(torch.LongTensor)\n",
    "\n",
    "# Update lables to be 0 and 1\n",
    "Y_train_tri = Y_train_tri - 1\n",
    "Y_test_tri = Y_test_tri - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ternary classification nn - pre trained W2V\n",
      "Self W2V Model trainning on GPU...\n",
      "Epoch: 40 train_loss: 1.1040 valid_loss: 1.1038 accuracy: 0.2012\n",
      "Epoch: 80 train_loss: 1.0887 valid_loss: 1.0884 accuracy: 0.4942\n",
      "Epoch: 120 train_loss: 1.0717 valid_loss: 1.0713 accuracy: 0.4202\n",
      "Epoch: 160 train_loss: 1.0545 valid_loss: 1.0542 accuracy: 0.4080\n",
      "Epoch: 200 train_loss: 1.0423 valid_loss: 1.0421 accuracy: 0.4172\n",
      "Epoch: 240 train_loss: 1.0318 valid_loss: 1.0318 accuracy: 0.4746\n",
      "Epoch: 280 train_loss: 1.0212 valid_loss: 1.0213 accuracy: 0.5461\n",
      "Epoch: 320 train_loss: 1.0096 valid_loss: 1.0097 accuracy: 0.5883\n",
      "Epoch: 360 train_loss: 0.9964 valid_loss: 0.9965 accuracy: 0.6070\n",
      "Epoch: 400 train_loss: 0.9823 valid_loss: 0.9824 accuracy: 0.6160\n"
     ]
    }
   ],
   "source": [
    "print(\"ternary classification nn - pre trained W2V\")\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Self W2V Model trainning on GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Self W2V Model trainning on CPU...\")\n",
    "\n",
    "# Initiate model\n",
    "model = TriMLP(300)\n",
    "model.to(device)\n",
    "\n",
    "# Configure Hyperparamters\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 400\n",
    "batch = 2000\n",
    "\n",
    "X_train_cuda = X_train_tri.to(device)\n",
    "X_test_cuda = X_test_tri.to(device)\n",
    "Y_train_cuda = Y_train_tri.to(device)\n",
    "Y_test_cuda = Y_test_tri.to(device)\n",
    "\n",
    "for e in range(1,epochs+1):\n",
    "    y_pred = model(X_train_cuda)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.nll_loss(y_pred, Y_train_cuda)\n",
    "    optimizer.zero_grad()\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    # Update weights and bias\n",
    "    optimizer.step()\n",
    "    \n",
    "    if e % 40 == 0:\n",
    "        test_out = model(X_test_cuda)\n",
    "        valid_loss = F.nll_loss(test_out, Y_test_cuda)\n",
    "        print(\"Epoch: {} train_loss: {:.4f} valid_loss: {:.4f} accuracy: {:.4f}\".format(e, loss, valid_loss, getAccuracy(test_out, Y_test_cuda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate 10 Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriMLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 50)\n",
    "        self.fc2 = torch.nn.Linear(50, 10)\n",
    "        self.fc3 = torch.nn.Linear(10, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(self.fc3(x), dim=1)\n",
    "    \n",
    "# Calculate Accuracy\n",
    "def getAccuracy(out, labels):\n",
    "    _, predict = torch.max(out.data, 1)\n",
    "    total = labels.shape[0]*1.0\n",
    "    correct = (labels == predict).sum().item()\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate 10 vec of each review\n",
    "def concatDocVec(w2v_model, train_input):\n",
    "    res = []\n",
    "\n",
    "    for review in train_input:\n",
    "        words = review.split()\n",
    "        doc_vec = []\n",
    "        if len(words) >= 10:\n",
    "            for w in words[:10]:\n",
    "                if w in w2v_model:\n",
    "                    doc_vec = np.concatenate((doc_vec, w2v_model[w]), axis=None)\n",
    "                else:\n",
    "                    doc_vec = np.concatenate((doc_vec, np.zeros(300,)), axis=None)\n",
    "        else:\n",
    "            for w in words:\n",
    "                if w in w2v_model:\n",
    "                    doc_vec = np.concatenate((doc_vec, w2v_model[w]), axis=None)\n",
    "                else:\n",
    "                    doc_vec = np.concatenate((doc_vec, np.zeros(300,)), axis=None)\n",
    "            for i in range(10 - len(words)):\n",
    "                doc_vec = np.concatenate((doc_vec, np.zeros(300,)), axis=None)\n",
    "        res.append(doc_vec)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tensor - self Trained\n",
    "x_tri = df[\"review\"]\n",
    "y_tri = df[\"class\"]\n",
    "\n",
    "x_train_tri, x_test_tri, y_train_tri, y_test_tri = train_test_split(x_tri, y_tri, random_state = 11, test_size = 0.2)\n",
    "\n",
    "features_train_tri = np.array(concatDocVec(w2v_review_model, x_train_tri), dtype=\"float64\")\n",
    "features_test_tri = np.array(concatDocVec(w2v_review_model, x_test_tri), dtype=\"float64\")\n",
    "\n",
    "X_train_tri,X_test_tri,Y_train_tri,Y_test_tri=torch.from_numpy(features_train_tri).float(),torch.from_numpy(features_test_tri).float(),torch.from_numpy(y_train_tri.to_numpy()).type(torch.LongTensor),torch.from_numpy(y_test_tri.to_numpy()).type(torch.LongTensor)\n",
    "\n",
    "# Update lables to be 0 and 1\n",
    "Y_train_tri = Y_train_tri - 1\n",
    "Y_test_tri = Y_test_tri - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #9. Self-trained W2V FNN-Ternary - ConcatVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ternary classification nn - self trained W2V\n",
      "Self W2V Model trainning on GPU...\n",
      "Epoch: 40 train_loss: 0.9053 valid_loss: 0.9075 accuracy: 0.5994\n",
      "Epoch: 80 train_loss: 0.8523 valid_loss: 0.8610 accuracy: 0.6200\n",
      "Epoch: 120 train_loss: 0.8289 valid_loss: 0.8438 accuracy: 0.6283\n",
      "Epoch: 160 train_loss: 0.8143 valid_loss: 0.8349 accuracy: 0.6311\n",
      "Epoch: 200 train_loss: 0.8026 valid_loss: 0.8290 accuracy: 0.6343\n",
      "Epoch: 240 train_loss: 0.7918 valid_loss: 0.8246 accuracy: 0.6363\n",
      "Epoch: 280 train_loss: 0.7812 valid_loss: 0.8213 accuracy: 0.6388\n",
      "Epoch: 320 train_loss: 0.7706 valid_loss: 0.8188 accuracy: 0.6398\n",
      "Epoch: 360 train_loss: 0.7596 valid_loss: 0.8171 accuracy: 0.6413\n",
      "Epoch: 400 train_loss: 0.7482 valid_loss: 0.8164 accuracy: 0.6420\n"
     ]
    }
   ],
   "source": [
    "print(\"ternary classification nn - self trained W2V\")\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Self W2V Model trainning on GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Self W2V Model trainning on CPU...\")\n",
    "\n",
    "# Initiate model\n",
    "model = TriMLP(3000)\n",
    "model.to(device)\n",
    "\n",
    "# Configure Hyperparamters\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 400\n",
    "batch = 2000\n",
    "\n",
    "X_train_cuda = X_train_tri.to(device)\n",
    "X_test_cuda = X_test_tri.to(device)\n",
    "Y_train_cuda = Y_train_tri.to(device)\n",
    "Y_test_cuda = Y_test_tri.to(device)\n",
    "\n",
    "for e in range(1,epochs+1):\n",
    "    y_pred = model(X_train_cuda)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.nll_loss(y_pred, Y_train_cuda)\n",
    "    optimizer.zero_grad()\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    # Update weights and bias\n",
    "    optimizer.step()\n",
    "    \n",
    "    if e % 40 == 0:\n",
    "        test_out = model(X_test_cuda)\n",
    "        valid_loss = F.nll_loss(test_out, Y_test_cuda)\n",
    "        print(\"Epoch: {} train_loss: {:.4f} valid_loss: {:.4f} accuracy: {:.4f}\".format(e, loss, valid_loss, getAccuracy(test_out, Y_test_cuda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #10. Pre-trained W2V FNN-Ternary - ConcatVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tri = df[\"review\"]\n",
    "y_tri = df[\"class\"]\n",
    "\n",
    "x_train_tri, x_test_tri, y_train_tri, y_test_tri = train_test_split(x_tri, y_tri, random_state = 11, test_size = 0.2)\n",
    "\n",
    "features_train_tri = np.array(concatDocVec(w2v_google_model, x_train_tri), dtype=\"float64\")\n",
    "features_test_tri = np.array(concatDocVec(w2v_google_model, x_test_tri), dtype=\"float64\")\n",
    "\n",
    "X_train_tri,X_test_tri,Y_train_tri,Y_test_tri=torch.from_numpy(features_train_tri).float(),torch.from_numpy(features_test_tri).float(),torch.from_numpy(y_train_tri.to_numpy()).type(torch.LongTensor),torch.from_numpy(y_test_tri.to_numpy()).type(torch.LongTensor)\n",
    "\n",
    "# Update lables to be 0 and 1\n",
    "Y_train_tri = Y_train_tri - 1\n",
    "Y_test_tri = Y_test_tri - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ternary classification nn - pre-trained W2V\n",
      "Google W2V Model trainning on GPU...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TriMLP(\n",
       "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"ternary classification nn - pre-trained W2V\")\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Google W2V Model trainning on GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Google W2V Model trainning on CPU...\")\n",
    "\n",
    "# Initiate model\n",
    "model = TriMLP()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Hyperparamters\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 800\n",
    "\n",
    "batch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cuda = X_train_tri.to(device)\n",
    "X_test_cuda = X_test_tri.to(device)\n",
    "Y_train_cuda = Y_train_tri.to(device)\n",
    "Y_test_cuda = Y_test_tri.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Optional] DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80 train_loss: 1.0165 valid_loss: 1.0180 accuracy: 0.5670\n",
      "Epoch: 160 train_loss: 0.9523 valid_loss: 0.9550 accuracy: 0.5892\n",
      "Epoch: 240 train_loss: 0.9083 valid_loss: 0.9127 accuracy: 0.6002\n",
      "Epoch: 320 train_loss: 0.8815 valid_loss: 0.8887 accuracy: 0.6078\n",
      "Epoch: 400 train_loss: 0.8655 valid_loss: 0.8759 accuracy: 0.6122\n",
      "Epoch: 480 train_loss: 0.8545 valid_loss: 0.8682 accuracy: 0.6154\n",
      "Epoch: 560 train_loss: 0.8455 valid_loss: 0.8626 accuracy: 0.6173\n",
      "Epoch: 640 train_loss: 0.8376 valid_loss: 0.8582 accuracy: 0.6189\n",
      "Epoch: 720 train_loss: 0.8298 valid_loss: 0.8544 accuracy: 0.6201\n",
      "Epoch: 800 train_loss: 0.8223 valid_loss: 0.8514 accuracy: 0.6221\n"
     ]
    }
   ],
   "source": [
    "for e in range(1,epochs+1):\n",
    "    y_pred = model(X_train_cuda)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.nll_loss(y_pred, Y_train_cuda)\n",
    "    optimizer.zero_grad()\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    # Update weights and bias\n",
    "    optimizer.step()\n",
    "    \n",
    "    if e % 80 == 0:\n",
    "        test_out = model(X_test_cuda)\n",
    "        valid_loss = F.nll_loss(test_out, Y_test_cuda)\n",
    "        print(\"Epoch: {} train_loss: {:.4f} valid_loss: {:.4f} accuracy: {:.4f}\".format(e, loss, valid_loss, getAccuracy(test_out, Y_test_cuda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #11. Self-trained W2V FNN-Binary - ConcatVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm = pd.concat([df[df['class'] == 1], df[df['class'] == 2]])\n",
    "# Split train and test\n",
    "x_train_sm, x_test_sm, y_train_sm, y_test_sm = train_test_split(x_sm, y_sm, test_size = 0.2)\n",
    "\n",
    "features_train = np.array(concatDocVec(w2v_review_model, x_train_sm), dtype=\"float64\")\n",
    "features_test = np.array(concatDocVec(w2v_review_model, x_test_sm), dtype=\"float64\")\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=torch.from_numpy(features_train).float(),torch.from_numpy(features_test).float(),torch.from_numpy(y_train_sm.to_numpy()).type(torch.LongTensor),torch.from_numpy(y_test_sm.to_numpy()).type(torch.LongTensor)\n",
    "\n",
    "# Update lables to be 0 and 1\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary classification nn - Self pre-trained W2V\n",
      "Google W2V Model trainning on GPU...\n",
      "Epoch: 100 train_loss: 0.5647 valid_loss: 0.5699 accuracy: 0.7644\n",
      "Epoch: 200 train_loss: 0.4921 valid_loss: 0.5039 accuracy: 0.7773\n",
      "Epoch: 300 train_loss: 0.4645 valid_loss: 0.4816 accuracy: 0.7820\n",
      "Epoch: 400 train_loss: 0.4473 valid_loss: 0.4695 accuracy: 0.7854\n",
      "Epoch: 500 train_loss: 0.4336 valid_loss: 0.4616 accuracy: 0.7881\n",
      "Epoch: 600 train_loss: 0.4215 valid_loss: 0.4562 accuracy: 0.7898\n",
      "Epoch: 700 train_loss: 0.4096 valid_loss: 0.4525 accuracy: 0.7906\n",
      "Epoch: 800 train_loss: 0.3976 valid_loss: 0.4502 accuracy: 0.7920\n",
      "Epoch: 900 train_loss: 0.3853 valid_loss: 0.4493 accuracy: 0.7930\n",
      "Epoch: 1000 train_loss: 0.3728 valid_loss: 0.4499 accuracy: 0.7930\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary classification nn - Self pre-trained W2V\")\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Google W2V Model trainning on GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Google W2V Model trainning on CPU...\")\n",
    "\n",
    "# Initiate model\n",
    "model = ThreeLayerMLP(3000)\n",
    "model.to(device)\n",
    "\n",
    "# Configure Hyperparamters\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 1000\n",
    "batch = 10000\n",
    "\n",
    "X_train_cuda = X_train.to(device)\n",
    "X_test_cuda = X_test.to(device)\n",
    "Y_train_cuda = Y_train.to(device)\n",
    "Y_test_cuda = Y_test.to(device)\n",
    "\n",
    "for e in range(1,epochs+1):\n",
    "    y_pred = model(X_train_cuda)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, Y_train_cuda)\n",
    "    optimizer.zero_grad()\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    # Update weights and bias\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Report Accuracy each 100 epochs\n",
    "    if e % 100 == 0:\n",
    "        test_out = model(X_test_cuda)\n",
    "        valid_loss = loss_fn(test_out, Y_test_cuda)\n",
    "        print(\"Epoch: {} train_loss: {:.4f} valid_loss: {:.4f} accuracy: {:.4f}\".format(e, loss, valid_loss, getAccuracy(test_out, Y_test_cuda)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #12. Pre-trained W2V FNN-Binary - ConcatVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "x_train_sm, x_test_sm, y_train_sm, y_test_sm = train_test_split(x_sm, y_sm, test_size = 0.2)\n",
    "\n",
    "features_train = np.array(concatDocVec(w2v_google_model, x_train_sm), dtype=\"float64\")\n",
    "features_test = np.array(concatDocVec(w2v_google_model, x_test_sm), dtype=\"float64\")\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=torch.from_numpy(features_train).float(),torch.from_numpy(features_test).float(),torch.from_numpy(y_train_sm.to_numpy()).type(torch.LongTensor),torch.from_numpy(y_test_sm.to_numpy()).type(torch.LongTensor)\n",
    "\n",
    "# Update lables to be 0 and 1\n",
    "Y_train = Y_train - 1\n",
    "Y_test = Y_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary classification nn - pre-trained W2V\n",
      "Google W2V Model trainning on GPU...\n",
      "Epoch: 100 train_loss: 1.1297 valid_loss: 1.1225 accuracy: 0.7143\n",
      "Epoch: 200 train_loss: 0.7204 valid_loss: 0.7212 accuracy: 0.7327\n",
      "Epoch: 300 train_loss: 0.6112 valid_loss: 0.6149 accuracy: 0.7430\n",
      "Epoch: 400 train_loss: 0.5618 valid_loss: 0.5677 accuracy: 0.7511\n",
      "Epoch: 500 train_loss: 0.5334 valid_loss: 0.5415 accuracy: 0.7563\n",
      "Epoch: 600 train_loss: 0.5146 valid_loss: 0.5248 accuracy: 0.7610\n",
      "Epoch: 700 train_loss: 0.5007 valid_loss: 0.5130 accuracy: 0.7635\n",
      "Epoch: 800 train_loss: 0.4897 valid_loss: 0.5041 accuracy: 0.7653\n",
      "Epoch: 900 train_loss: 0.4805 valid_loss: 0.4970 accuracy: 0.7665\n",
      "Epoch: 1000 train_loss: 0.4725 valid_loss: 0.4914 accuracy: 0.7684\n"
     ]
    }
   ],
   "source": [
    "print(\"Binary classification nn - pre-trained W2V\")\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Google W2V Model trainning on GPU...\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Google W2V Model trainning on CPU...\")\n",
    "\n",
    "# Initiate model\n",
    "model = ThreeLayerMLP(3000)\n",
    "model.to(device)\n",
    "\n",
    "# Configure Hyperparamters\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "epochs = 1000\n",
    "batch = 10000\n",
    "\n",
    "X_train_cuda = X_train.to(device)\n",
    "X_test_cuda = X_test.to(device)\n",
    "Y_train_cuda = Y_train.to(device)\n",
    "Y_test_cuda = Y_test.to(device)\n",
    "\n",
    "for e in range(1,epochs+1):\n",
    "    y_pred = model(X_train_cuda)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, Y_train_cuda)\n",
    "    optimizer.zero_grad()\n",
    "    # Compute gradient\n",
    "    loss.backward()\n",
    "    # Update weights and bias\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Report Accuracy each 100 epochs\n",
    "    if e % 100 == 0:\n",
    "        test_out = model(X_test_cuda)\n",
    "        valid_loss = loss_fn(test_out, Y_test_cuda)\n",
    "        print(\"Epoch: {} train_loss: {:.4f} valid_loss: {:.4f} accuracy: {:.4f}\".format(e, loss, valid_loss, getAccuracy(test_out, Y_test_cuda)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The accuracy for self-trained FNN model is a little bit better (~85%) compare to perceptron and SVM, and the accuracy for pre-trained FNN model is much better than perceptron and SVM. I think the mean reason is that in the neural networks could better classify dataset than the linear models because of the hidden layers. It could use \"multiple lines\" to divide the dataset rather than one line. \n",
    "Note: Due to the high GPU memory usage of my models, I had to restart the kernel for between running model 11 and 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 50 vec for each review\n",
    "def paddedVec(w2v_model, train_input):\n",
    "    res = []\n",
    "\n",
    "    for review in train_input:\n",
    "        docVec = []\n",
    "        words = review.split()\n",
    "        if len(words) >= 50 :\n",
    "            words = words[:50]     \n",
    "        else:\n",
    "            #words = [\"0\" for _ in range(50 - len(words))] + words\n",
    "            docVec = [np.zeros(300,) for _ in range(50 - len(words))]\n",
    "        \n",
    "        for word in words:\n",
    "            if word in w2v_model:\n",
    "                docVec.append(w2v_model[word])\n",
    "            else:\n",
    "                docVec.append(np.zeros(300,))\n",
    "        \n",
    "        res.append(docVec)\n",
    "\n",
    "    return np.array(res, dtype=\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "    \n",
    "class SeqRNN(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,hidden_size,output_size):\n",
    "        super(SeqRNN,self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNN(self.vocab_size,self.hidden_size,batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size,self.output_size)\n",
    "        \n",
    "    def forward(self,input): \n",
    "        batch_size = len(input)\n",
    "        h0 = torch.zeros(1,batch_size,self.hidden_size).to(device)\n",
    "        output , hidden = self.rnn(input,h0)\n",
    "        output = output[ : ,-1, : ]\n",
    "        output = self.linear(output)\n",
    "        output = torch.nn.functional.softmax(output,dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "###### For dataloader enumerate loop ######\n",
    "\n",
    "def test_accuracy_loader(model, test_loader):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for _, (review, label) in enumerate(test_loader):\n",
    "        total += len(label)\n",
    "        pred = predict_review_tensor(review, model)\n",
    "        correct += (pred == label).sum()\n",
    "    return correct.item() * 1.0 / total\n",
    "\n",
    "def predict_review_tensor(review, model):\n",
    "    with torch.no_grad():\n",
    "        out = model(review)\n",
    "        out = torch.argmax(out).item()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #13. Self-trained W2V RNN-Simple-Binary - PaddedVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Cuda Device\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "## Keep only class 1 and 2 for Binary Classification\n",
    "df_sm = pd.concat([df[df['class'] == 1], df[df['class'] == 2]])\n",
    "## Drop Null reviews\n",
    "df_sm = df_sm.dropna(subset=['review'])\n",
    "\n",
    "# Sample less data due to memory constraint\n",
    "df_sm = df_sm.sample(20000)\n",
    "\n",
    "\n",
    "# Split train and test data\n",
    "reviews = df_sm[\"review\"]\n",
    "labels = df_sm[\"class\"]\n",
    "train_set, test_set, train_labels, test_labels = train_test_split(reviews, labels, test_size = 0.2)\n",
    "\n",
    "## Turn Train and Test set and labels to numpy array\n",
    "train_set = paddedVec(w2v_review_model, train_set)\n",
    "test_set = paddedVec(w2v_review_model, test_set)\n",
    "\n",
    "## Substract 1 from labels, so the classes starts from 0\n",
    "train_labels = train_labels.to_numpy() - 1\n",
    "test_labels = test_labels.to_numpy() - 1\n",
    "\n",
    "## Train & test tensors\n",
    "train_set = torch.from_numpy(train_set).float().to(device)\n",
    "test_set = torch.from_numpy(test_set).float().to(device)\n",
    "train_labels = torch.from_numpy(train_labels).type(torch.LongTensor).to(device)\n",
    "test_labels = torch.from_numpy(test_labels).type(torch.LongTensor).to(device)\n",
    "\n",
    "\n",
    "## Prepare TensorDataset and DataLoader\n",
    "training_set = TensorDataset(train_set, train_labels)\n",
    "testing_set = TensorDataset(test_set, test_labels)\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(testing_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Timetaken: 36.13s train_loss: 1.2467 accuracy: 0.7535\n",
      "Epoch: 1 Timetaken: 34.67s train_loss: 0.3255 accuracy: 0.7652\n",
      "Epoch: 2 Timetaken: 36.84s train_loss: 0.3242 accuracy: 0.7688\n",
      "Epoch: 3 Timetaken: 35.96s train_loss: 0.3230 accuracy: 0.7430\n",
      "Epoch: 4 Timetaken: 36.57s train_loss: 1.3056 accuracy: 0.7800\n",
      "Epoch: 5 Timetaken: 36.72s train_loss: 0.3140 accuracy: 0.7825\n",
      "Epoch: 6 Timetaken: 37.22s train_loss: 1.2956 accuracy: 0.7843\n",
      "Epoch: 7 Timetaken: 36.23s train_loss: 0.3150 accuracy: 0.7827\n",
      "Epoch: 8 Timetaken: 36.84s train_loss: 0.3151 accuracy: 0.7748\n",
      "Epoch: 9 Timetaken: 36.62s train_loss: 0.3136 accuracy: 0.7652\n"
     ]
    }
   ],
   "source": [
    "model = SeqRNN(300,50,2)\n",
    "model.to(device)\n",
    "epoches = 10\n",
    "every_epoch = 1\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(epoches):\n",
    "    # Train all sentences from train set\n",
    "    for i, (review, label) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(review)\n",
    "        loss = loss_func(pred,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if e % every_epoch == 0:\n",
    "        accuracy = test_accuracy_loader(model, test_loader)\n",
    "        timetaken = time.time() - start_time\n",
    "        print(\"Epoch: {} Timetaken: {:.2f}s train_loss: {:.4f} accuracy: {:.4f}\".format(e, timetaken, loss, accuracy))\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU Cache After Training\n",
    "del train_set\n",
    "del test_set\n",
    "del train_labels\n",
    "del test_labels\n",
    "del training_set\n",
    "del testing_set\n",
    "del train_loader\n",
    "del test_loader\n",
    "\n",
    "del model\n",
    "del loss_func\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #14. Pre-trained W2V RNN-Simple-Binary - PaddedVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cuda Device\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "## Keep only class 1 and 2 for Binary Classification\n",
    "df_sm = pd.concat([df[df['class'] == 1], df[df['class'] == 2]])\n",
    "## Drop Null reviews\n",
    "df_sm = df_sm.dropna(subset=['review'])\n",
    "\n",
    "# Sample less data due to memory constraint\n",
    "df_sm = df_sm.sample(20000)\n",
    "\n",
    "\n",
    "# Split train and test data\n",
    "reviews = df_sm[\"review\"]\n",
    "labels = df_sm[\"class\"]\n",
    "train_set, test_set, train_labels, test_labels = train_test_split(reviews, labels, test_size = 0.2)\n",
    "\n",
    "## Turn Train and Test set and labels to numpy array\n",
    "train_set = paddedVec(w2v_google_model, train_set)\n",
    "test_set = paddedVec(w2v_google_model, test_set)\n",
    "\n",
    "## Substract 1 from labels, so the classes starts from 0\n",
    "train_labels = train_labels.to_numpy() - 1\n",
    "test_labels = test_labels.to_numpy() - 1\n",
    "\n",
    "## Train & test tensors\n",
    "train_set = torch.from_numpy(train_set).float().to(device)\n",
    "test_set = torch.from_numpy(test_set).float().to(device)\n",
    "train_labels = torch.from_numpy(train_labels).type(torch.LongTensor).to(device)\n",
    "test_labels = torch.from_numpy(test_labels).type(torch.LongTensor).to(device)\n",
    "\n",
    "\n",
    "## Prepare TensorDataset and DataLoader\n",
    "training_set = TensorDataset(train_set, train_labels)\n",
    "testing_set = TensorDataset(test_set, test_labels)\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(testing_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Timetaken: 35.15s train_loss: 0.3144 accuracy: 0.7242\n",
      "Epoch: 1 Timetaken: 34.97s train_loss: 0.3153 accuracy: 0.7615\n",
      "Epoch: 2 Timetaken: 37.44s train_loss: 0.6367 accuracy: 0.7640\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14824/3548995200.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py39-CS544\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py39-CS544\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SeqRNN(300,50,2)\n",
    "model.to(device)\n",
    "epoches = 10\n",
    "every_epoch = 1\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(epoches):\n",
    "    # Train all sentences from train set\n",
    "    for i, (review, label) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(review)\n",
    "        loss = loss_func(pred,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if e % every_epoch == 0:\n",
    "        accuracy = test_accuracy_loader(model, test_loader)\n",
    "        timetaken = time.time() - start_time\n",
    "        print(\"Epoch: {} Timetaken: {:.2f}s train_loss: {:.4f} accuracy: {:.4f}\".format(e, timetaken, loss, accuracy))\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"binary_rnn_pre_trained.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU Cache After Training\n",
    "del train_set\n",
    "del test_set\n",
    "del train_labels\n",
    "del test_labels\n",
    "del training_set\n",
    "del testing_set\n",
    "del train_loader\n",
    "del test_loader\n",
    "\n",
    "del model\n",
    "del loss_func\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #15. Self-trained W2V RNN-Simple-Ternary - PaddedVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cuda Device\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "## Keep all classes for Ternary Classification\n",
    "df_sm = df.copy()\n",
    "## Drop Null reviews\n",
    "df_sm = df_sm.dropna(subset=['review'])\n",
    "\n",
    "# Sample less data due to memory constraint\n",
    "df_sm = df_sm.sample(20000)\n",
    "\n",
    "\n",
    "# Split train and test data\n",
    "reviews = df_sm[\"review\"]\n",
    "labels = df_sm[\"class\"]\n",
    "train_set, test_set, train_labels, test_labels = train_test_split(reviews, labels, test_size = 0.2)\n",
    "\n",
    "## Turn Train and Test set and labels to numpy array\n",
    "train_set = paddedVec(w2v_review_model, train_set)\n",
    "test_set = paddedVec(w2v_review_model, test_set)\n",
    "\n",
    "## Substract 1 from labels, so the classes starts from 0\n",
    "train_labels = train_labels.to_numpy() - 1\n",
    "test_labels = test_labels.to_numpy() - 1\n",
    "\n",
    "## Train & test tensors\n",
    "train_set = torch.from_numpy(train_set).float().to(device)\n",
    "test_set = torch.from_numpy(test_set).float().to(device)\n",
    "train_labels = torch.from_numpy(train_labels).type(torch.LongTensor).to(device)\n",
    "test_labels = torch.from_numpy(test_labels).type(torch.LongTensor).to(device)\n",
    "\n",
    "\n",
    "## Prepare TensorDataset and DataLoader\n",
    "training_set = TensorDataset(train_set, train_labels)\n",
    "testing_set = TensorDataset(test_set, test_labels)\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(testing_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SeqRNN(300,50,3)\n",
    "model.to(device)\n",
    "epoches = 10\n",
    "every_epoch = 1\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(epoches):\n",
    "    # Train all sentences from train set\n",
    "    for i, (review, label) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(review)\n",
    "        loss = loss_func(pred,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if e % every_epoch == 0:\n",
    "        accuracy = test_accuracy_loader(model, test_loader)\n",
    "        timetaken = time.time() - start_time\n",
    "        print(\"Epoch: {} Timetaken: {:.2f}s train_loss: {:.4f} accuracy: {:.4f}\".format(e, timetaken, loss, accuracy))\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model #16. Pre-trained W2V RNN-Simple-Ternary - PaddedVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cuda Device\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "## Keep all classes for Ternary Classification\n",
    "df_sm = df.copy()\n",
    "## Drop Null reviews\n",
    "df_sm = df_sm.dropna(subset=['review'])\n",
    "\n",
    "# Sample less data due to memory constraint\n",
    "df_sm = df_sm.sample(20000)\n",
    "\n",
    "\n",
    "# Split train and test data\n",
    "reviews = df_sm[\"review\"]\n",
    "labels = df_sm[\"class\"]\n",
    "train_set, test_set, train_labels, test_labels = train_test_split(reviews, labels, test_size = 0.2)\n",
    "\n",
    "## Turn Train and Test set and labels to numpy array\n",
    "train_set = paddedVec(w2v_google_model, train_set)\n",
    "test_set = paddedVec(w2v_google_model, test_set)\n",
    "\n",
    "## Substract 1 from labels, so the classes starts from 0\n",
    "train_labels = train_labels.to_numpy() - 1\n",
    "test_labels = test_labels.to_numpy() - 1\n",
    "\n",
    "## Train & test tensors\n",
    "train_set = torch.from_numpy(train_set).float().to(device)\n",
    "test_set = torch.from_numpy(test_set).float().to(device)\n",
    "train_labels = torch.from_numpy(train_labels).type(torch.LongTensor).to(device)\n",
    "test_labels = torch.from_numpy(test_labels).type(torch.LongTensor).to(device)\n",
    "\n",
    "\n",
    "## Prepare TensorDataset and DataLoader\n",
    "training_set = TensorDataset(train_set, train_labels)\n",
    "testing_set = TensorDataset(test_set, test_labels)\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(testing_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers=1):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        \n",
    "        # Initiate GRU with parameters\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers,  \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # softmax output \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    # inputs = (time_step, batch_size, input_size)\n",
    "    def forward(self,inputs,hidden):\n",
    "        # Run inputs and hidden through gru, return output sequence and hidden state\n",
    "        output,hidden = self.gru(inputs,hidden)\n",
    "        \n",
    "        # select last output in the sequences\n",
    "        output = output[:,-1,:]\n",
    "        \n",
    "        # output softmax for accuracy computing\n",
    "        output = self.out(output)\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Links\n",
    "#### I've read and referenced some of the code from the following toturials and posts:\n",
    "\n",
    "<p> https://radimrehurek.com/gensim/models/word2vec.html </p>\n",
    "<p> https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751 </p>\n",
    "\n",
    "<p> https://stackoverflow.com/questions/10686924/numpy-array-to-scipy-sparse-matrix </p>\n",
    "<p> https://stackoverflow.com/questions/29314033/drop-rows-containing-empty-cells-from-a-pandas-dataframe </p>\n",
    "<p>https://blog.csdn.net/wuwususheng/article/details/109851179?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-4.no_search_link&spm=1001.2101.3001.4242</p>\n",
    "<p>https://blog.csdn.net/sinat_33761963/article/details/104332831?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link</p>\n",
    "\n",
    "<p>https://blog.csdn.net/m0_37306360/article/details/79309849?ops_request_misc=&request_id=&biz_id=102&utm_term=pytorch%E5%A4%9A%E5%88%86%E7%B1%BB&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-79309849.pc_search_result_hbase_insert&spm=1018.2226.3001.4187</p>\n",
    "<p> https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76 </p>\n",
    "<p> https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html </p>\n",
    "<p> https://www.kaggle.com/kanncaa1/recurrent-neural-network-with-pytorch </p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
