{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0449988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re, json\n",
    "from tqdm import tqdm\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dca2d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "df = pd.read_csv(\"data/train\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])\n",
    "\n",
    "# The starting POS tag distribution from training dataset\n",
    "start_tag_distributions = df[ df['s_idx'] == 1 ][\"pos\"].value_counts(normalize=True)\n",
    "\n",
    "# All the possible pos tags from training dataset\n",
    "pos_tags = df['pos'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1b8af089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab list - for re-run the program\n",
    "if exists('vocab.txt'):\n",
    "    vocab = pd.read_csv('vocab.txt', sep='\\t', names=['word_type', 'vocab_idx', 'occurrence'])\n",
    "    # a set of unique words in the vocab for unk word assignment\n",
    "    vocab_list = set(vocab[\"word_type\"].unique().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ac3ae",
   "metadata": {},
   "source": [
    "# Taks 1  - Generate vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6a5058b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use value_count() on word_type column to get the occurrences of word types\n",
    "unique_words = df['word_type'].value_counts()\n",
    "unique_words = unique_words.reset_index()\n",
    "vocab = pd.DataFrame(unique_words)\n",
    "\n",
    "# Add index column\n",
    "vocab[\"vocab_idx\"] = vocab.index\n",
    "\n",
    "# Rename and rearrange columns\n",
    "vocab.columns = ['word_type', 'occurrence', 'vocab_idx']\n",
    "vocab = vocab[['word_type', 'vocab_idx', 'occurrence']]\n",
    "\n",
    "# set unknown word threshold, and drop word types with low frequency from vocab\n",
    "threshold = 2\n",
    "unks = vocab[ vocab['occurrence'] < threshold ]\n",
    "vocab = vocab[ vocab['occurrence'] >= threshold ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e11a4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Regex for capitalization or morphology\n",
    "\n",
    "# Non digit Regex\n",
    "none_digit_regex = '^(?:[^0-9]*)$'\n",
    "# word contains number tend to be CD and JJ\n",
    "num_cd_regex = '^(?:[0-9.,:]*)$'\n",
    "# word contains upper case letter tend to be NNP\n",
    "cap_str_regex = '.*[A-Z].*'\n",
    "# word contains a \"-\" are likely to be JJ\n",
    "jj_regex = '^.*-.*$'\n",
    "# Word with suffix \"ed\"\n",
    "vbn_regex = '^.*ed$'\n",
    "\n",
    "# unknow strings that do not contain any digit\n",
    "# unk_strs = unks[ unks['word_type'].str.match(none_digit_regex)==True ]\n",
    "\n",
    "\n",
    "\n",
    "# UPPER CASE = NNP\n",
    "unk_upper_nnp = unks[ (unks['word_type'].str.match(cap_str_regex)==True)]\n",
    "unk_strs = unks[ unks['word_type'].str.match(cap_str_regex)==False ]\n",
    "\n",
    "# DIGITS = CD\n",
    "unk_cd = unk_strs[ unk_strs['word_type'].str.match(num_cd_regex)==True ]\n",
    "unk_strs = unk_strs[ unk_strs['word_type'].str.match(num_cd_regex)==False ]\n",
    "\n",
    "# CONTAINS \"-\" = JJ\n",
    "unks_jj = unk_strs[ unk_strs['word_type'].str.match(jj_regex)==True ]\n",
    "unk_strs = unk_strs[ unk_strs['word_type'].str.match(jj_regex)==False ]\n",
    "\n",
    "# SUFFIX \"ed\" = VBN\n",
    "unk_vnb = unk_strs[ unk_strs['word_type'].str.match(vbn_regex)==True ]\n",
    "unk_strs = unk_strs[ unk_strs['word_type'].str.match(vbn_regex)==False ]\n",
    "\n",
    "# Rest of Unknowns\n",
    "unks_df = pd.DataFrame([[\"<unk>\", 0, unk_strs.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "\n",
    "# Generate unknown categories\n",
    "unk_upper_nnp = pd.DataFrame([[\"<unk_upper_nnp>\", 0, unk_upper_nnp.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "unk_cd = pd.DataFrame([[\"<unk_nums_cd>\", 0, unk_cd.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "unks_jj = pd.DataFrame([[\"<unk_jj>\", 0, unks_jj.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "unk_vnb = pd.DataFrame([[\"<unk_vbn>\", 0, unk_vnb.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "\n",
    "# Append unknown categories to the rest of unknowns\n",
    "unks_df = unks_df.append(unk_upper_nnp, ignore_index=True)\n",
    "unks_df = unks_df.append(unk_cd, ignore_index=True)\n",
    "unks_df = unks_df.append(unks_jj, ignore_index=True)\n",
    "unks_df = unks_df.append(unk_vnb, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b1c543e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creak vocab DF and store it to a file\n",
    "vocab = unks_df.append(vocab, ignore_index=True)\n",
    "\n",
    "# reindex and update vocab_idx values\n",
    "vocab[\"vocab_idx\"] = vocab.index\n",
    "\n",
    "vocab = vocab.reset_index()\n",
    "vocab = vocab.drop(columns=['index'])\n",
    "vocab.to_csv('vocab.txt', sep='\\t', header=False, index=False)\n",
    "\n",
    "# a set of unique words in the vocab for unk word assignment\n",
    "vocab_list = set(vocab[\"word_type\"].unique().flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf528e5",
   "metadata": {},
   "source": [
    "Threshold: 2, Vocab size: 23197, total unknown words occurrence: 20011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388bbe5",
   "metadata": {},
   "source": [
    "#### What is the selected threshold for unknown words replacement?\n",
    "<p> The selected threshold is 2 </p>\n",
    "    \n",
    "#### What is the total size of your vocabulary\n",
    "<p> The size of my vocabulary is 23197 </p>\n",
    "\n",
    "#### what is the total occurrences of the special token < unk > after replacement?\n",
    "<p> The total occurrences of unk is 20011 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950c2ee",
   "metadata": {},
   "source": [
    "<p>lowercase: Threshold: 2, Vocab size: 21158, unk occurrence: 17401</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f783d0d",
   "metadata": {},
   "source": [
    "# Taks 2  - Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4b9ac3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the input word is in vocab list, if not, categorize it based the word format\n",
    "def checkWord(word):\n",
    "    # Non digit Regex\n",
    "    none_digit_regex = '^(?:[^0-9]*)$'\n",
    "    # word contains number tend to be CD and JJ\n",
    "    num_cd_regex = '^(?:[0-9.,:]*)$'\n",
    "    # word contains upper case letter tend to be NNP\n",
    "    cap_str_regex = '.*[A-Z].*'\n",
    "    # word contains a \"-\" are likely to be JJ\n",
    "    jj_regex = '^.*-.*$'\n",
    "    # Word with suffix \"ed\"\n",
    "    vbn_regex = '^.*ed$'\n",
    "\n",
    "    if word in vocab_list:\n",
    "        return word\n",
    "    \n",
    "    if bool(re.match(cap_str_regex, word)):\n",
    "        return '<unk_upper_nnp>'\n",
    "    elif bool(re.match(num_cd_regex, word)):\n",
    "        return '<unk_nums_cd>'\n",
    "    elif bool(re.match(jj_regex, word)):\n",
    "        return '<unk_jj>'\n",
    "    elif bool(re.match(vbn_regex, word)):\n",
    "        return '<unk_vbn>'\n",
    "    else:\n",
    "        return '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c4993795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputing hmm.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 912095/912095 [00:44<00:00, 20598.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitions: 1351, Emissions:30347\n"
     ]
    }
   ],
   "source": [
    "# for calculating count(tag)\n",
    "pos_distributions = df['pos'].value_counts().to_dict()\n",
    "\n",
    "# keep track of the transitions and emissions\n",
    "transitions = {}\n",
    "emissions = {}\n",
    "\n",
    "# keep track of the keys in transitions and emissions\n",
    "e_keys = set()\n",
    "t_keys = set()\n",
    "\n",
    "prev_pos = None\n",
    "\n",
    "print(\"Outputing hmm.json...\")\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    cur_word = row['word_type']\n",
    "    cur_pos = row['pos']\n",
    "    # replace low frequent word with <unk>\n",
    "#     if cur_word not in vocab_list:\n",
    "#         cur_word = \"<unk>\"\n",
    "    cur_word = checkWord(cur_word)\n",
    "    \n",
    "    e_key = cur_pos+\",\"+cur_word\n",
    "    if e_key in e_keys:\n",
    "        emissions[e_key]+=(1/pos_distributions[cur_pos])\n",
    "    else:\n",
    "        emissions[e_key]=(1/pos_distributions[cur_pos])\n",
    "        e_keys.add(e_key)\n",
    "    # skip transition for the first word in a sentence\n",
    "    if row['s_idx'] != 1:\n",
    "        t_key = prev_pos+\",\"+cur_pos\n",
    "        if t_key in t_keys:\n",
    "            transitions[t_key]+=(1/pos_distributions[prev_pos])\n",
    "        else:\n",
    "            transitions[t_key]=(1/pos_distributions[prev_pos])\n",
    "            t_keys.add(t_key)\n",
    "    prev_pos = cur_pos\n",
    "\n",
    "    \n",
    "print(\"Transitions: {}, Emissions:{}\".format(len(transitions), len(emissions)))\n",
    "\n",
    "# put transitions and emissions into hmm_model\n",
    "hmm_model = {\"transition\": transitions, \"emission\" : emissions}\n",
    "# store hmm into a file\n",
    "\n",
    "with open('hmm.json', 'w') as fp:\n",
    "    json.dump(hmm_model, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502d33f",
   "metadata": {},
   "source": [
    "#### How many transition and emission parameters in your HMM?\n",
    "<p> for threshold = 2, there are 1351 pairs of transitions,  30347 pairs of emissions </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a00560",
   "metadata": {},
   "source": [
    "# Task 3: Greedy Decoding withHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "258b61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists('hmm.json'):\n",
    "    hmm_model = json.load(open('hmm.json',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ae9130f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy decoding function, return accuracy and output_array according to parameters\n",
    "# dataset: entire dev dataset, output: boolean, is_dev: boolean\n",
    "def greedy(dataset, output, is_dev):\n",
    "    # for accuracy computation\n",
    "    total, correct = 0, 0\n",
    "    \n",
    "    # keep track of prev state\n",
    "    prev_tag = None\n",
    "    \n",
    "    # output file\n",
    "    output_array = []\n",
    "    \n",
    "    # Iterate over dataset, print progress\n",
    "    for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        word = row['word_type']\n",
    "        if output: \n",
    "            output_row = [row['s_idx'], word]\n",
    "        \n",
    "        if is_dev:\n",
    "            target = row['pos']\n",
    "        \n",
    "        # Check if word is in vocab, if not categorize unks\n",
    "        word = checkWord(word)\n",
    "\n",
    "        pred = None\n",
    "        max_s = -1\n",
    "\n",
    "        # For starting word:\n",
    "        if row[\"s_idx\"] == 1:\n",
    "            # for each possible starting tag    \n",
    "            for tag in start_tag_distributions.keys():\n",
    "                t,e = 0,0\n",
    "                # t(s_j)\n",
    "                t = start_tag_distributions[tag]\n",
    "                # e(x|s) = 0 if the emission is not seen in training data\n",
    "                if tag+','+word in hmm_model['emission']:\n",
    "                    e = hmm_model['emission'][tag+','+word]\n",
    "                s = e*t\n",
    "                # argmax s and update predication\n",
    "                if s > max_s:\n",
    "                    max_s = s\n",
    "                    pred = tag  \n",
    "        # For the rest of the sentence\n",
    "        else:\n",
    "            # for each possible tag\n",
    "            for tag in pos_tags:\n",
    "                t,e = 0,0\n",
    "                \n",
    "                # t(s_j|s_j-1) = 0 if the transition is not seen in training data\n",
    "                if prev_tag+','+tag in hmm_model['transition']:\n",
    "                    t = hmm_model['transition'][prev_tag+','+tag]\n",
    "                    # e(x|s) = 0 if the emission is not seen in training data\n",
    "                    if tag+','+word in hmm_model['emission']: \n",
    "                        e = hmm_model['emission'][tag+','+word]\n",
    "                s = e*t\n",
    "                # argmax s and update predication\n",
    "                if s > max_s:\n",
    "                    max_s = s\n",
    "                    pred = tag        \n",
    "        if(output):   \n",
    "            output_row.append(pred)\n",
    "            # Add empty row between sentences\n",
    "            if row[\"s_idx\"] == 1:\n",
    "                output_array.append([None, None, None])\n",
    "            output_array.append(output_row)\n",
    "            \n",
    "        # remember the current predication as the prev_tag for next observation\n",
    "        prev_tag = pred\n",
    "        # increment correct if predicted right\n",
    "        if is_dev:\n",
    "            if pred == target:\n",
    "                correct +=1\n",
    "        total +=1\n",
    "\n",
    "    # output , remove the first Nan\n",
    "    return round(correct/total*100, 3), output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2320e2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Greedy] Testing on Dev data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131768/131768 [00:11<00:00, 11110.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Dataset Accuracy: 94.482%\n"
     ]
    }
   ],
   "source": [
    "# Dev dataset evaluation\n",
    "dev_df = pd.read_csv(\"data/dev\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])\n",
    "\n",
    "is_output, is_dev = False, True\n",
    "\n",
    "print(\"[Greedy] Testing on Dev data...\")\n",
    "\n",
    "accuracy, output = greedy(dev_df, is_output, is_dev)\n",
    "\n",
    "print(\"Dev Dataset Accuracy: {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "209f1029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Greedy] Generating predications on Test data  greedy.out...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129654/129654 [00:12<00:00, 10581.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test dataset predication\n",
    "test_df = pd.read_csv(\"data/test\", sep='\\t', names=[\"s_idx\", \"word_type\"])\n",
    "\n",
    "is_output, is_dev = True, False\n",
    "\n",
    "print(\"[Greedy] Generating predications on Test data  greedy.out...\")\n",
    "accuracy, output = greedy(test_df, is_output, is_dev)\n",
    "\n",
    "# Remove 1st empty line from output, due to the way output was generated\n",
    "Predictions = np.array(output[1:])\n",
    "\n",
    "# Store predications to greedy.out\n",
    "greedy_output_df = pd.DataFrame(Predictions, columns = [\"s_idx\", \"word_type\", \"pos\"])\n",
    "greedy_output_df.to_csv('greedy.out', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2eaf40",
   "metadata": {},
   "source": [
    "#### What is the accuracy on the dev data?\n",
    "<p> Threshold: 2, Greedy accuracy on Dev Dataset: 94.482%</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745c6e3",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding withHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8a83714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Algorithm Implementation, take dataset contains one sentences from pos 1 to pos m\n",
    "# dataset: df, output: boolean, is_dev: boolean\n",
    "def viterbi(dataset):\n",
    "    # pi table\n",
    "    pi = {}\n",
    "    # path table\n",
    "    paths = {}\n",
    "    # best tag for each position\n",
    "    best = None\n",
    "    # position \n",
    "    j = 0\n",
    "    \n",
    "    # Iterate over dataset, print progress\n",
    "#     for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "    for i, row in dataset.iterrows():\n",
    "        j = row['s_idx']\n",
    "        # the <unk> tag was already replaced in input dataset\n",
    "        word = row['word_type']\n",
    "        # Keep track of the pi value for each pos tag\n",
    "        # {\"DT\" : 0.001  }\n",
    "        pi_pos = {}\n",
    "        # keep track of the path for each pos in pi_pos\n",
    "        pi_path = {}\n",
    "    \n",
    "        if j == 1:\n",
    "            # reset pi for each sentence\n",
    "            pi = {}\n",
    "            paths = {}\n",
    "            \n",
    "            for tag in pos_tags:\n",
    "                # initialize transition and emission\n",
    "                t,e = 0,0\n",
    "                # start word t = start tag distribution\n",
    "                if tag in start_tag_distributions.keys():\n",
    "                    t = start_tag_distributions[tag]\n",
    "                    if tag+','+word in hmm_model['emission']:\n",
    "                        e = hmm_model['emission'][tag+','+word]\n",
    "                # record pi for each starting tag\n",
    "                pi_pos[tag] = e*t\n",
    "                # start the path for each starting tag\n",
    "                pi_path[tag] = [tag]\n",
    "            # record the pi and paths at each position j\n",
    "            pi[j] = pi_pos\n",
    "            paths[j] = pi_path\n",
    "                  \n",
    "        else:\n",
    "            for cur_tag in pos_tags:\n",
    "                pi_pos[cur_tag] = -1\n",
    "\n",
    "                # pi[j-1][prev_tag], t(cur_tag|prev_tag), e(word|cur_tag)\n",
    "                for prev_tag in pos_tags:\n",
    "                    prev_pi = pi[j-1][prev_tag]\n",
    "                    # Skip to the next prev_tag if pi[j-1, prev_tag] = 0\n",
    "                    cur_pi = 0\n",
    "                    # cur_pi = 0 if pre_pi = 0 so only consider the non zero prev_pi\n",
    "                    if prev_pi != 0:\n",
    "                        t,e = 0,0\n",
    "                        # if transition exists\n",
    "                        if prev_tag+','+cur_tag in hmm_model['transition']:\n",
    "                                t = hmm_model['transition'][prev_tag+','+cur_tag]\n",
    "                                # if emission exists\n",
    "                                if cur_tag+','+word in hmm_model['emission']: \n",
    "                                    e = hmm_model['emission'][cur_tag+','+word]\n",
    "\n",
    "                        cur_pi = prev_pi*e*t\n",
    "                    \n",
    "                    # update pi at each position as well as update the path to get to the best pi\n",
    "                    if cur_pi > pi_pos[cur_tag]:\n",
    "                        pi_pos[cur_tag] = cur_pi\n",
    "                        pi_path[cur_tag] = paths[j-1][prev_tag][:]\n",
    "                        pi_path[cur_tag].append(cur_tag)\n",
    "                        \n",
    "            # record the pi and paths at each position j           \n",
    "            pi[j] = pi_pos\n",
    "            paths[j] = pi_path\n",
    "        \n",
    "    # with j being the last position, get the best tag with highest pi value\n",
    "    best = max(pi[j], key=pi[j].get) \n",
    "    # return the sequence that led to the best tag\n",
    "#     print(pi)\n",
    "#     print(paths)\n",
    "    return paths[j][best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "515847fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoder(dataset, is_output, is_dev):\n",
    "\n",
    "    # Represent each sentence key = word, value = pos, e.g. {\"word\", 1}\n",
    "    sentence = []\n",
    "    targets = []\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Output\n",
    "    output = []\n",
    "\n",
    "    for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        # increment total\n",
    "        total += 1\n",
    "        # Get word and sentence_idx of each row\n",
    "        j = row['s_idx']\n",
    "        if is_dev:\n",
    "            target = row['pos']\n",
    "        word = row['word_type']\n",
    "        \n",
    "        #if word not in vocab_list:\n",
    "        word = checkWord(word) \n",
    "    \n",
    "        # hold sentence from each row\n",
    "        w_row = [j, word]\n",
    "\n",
    "        if j == 1:\n",
    "            # Process previous complete sentence stored in dict sentence\n",
    "            if sentence:     \n",
    "                sen_df = pd.DataFrame(sentence, columns =['s_idx', 'word_type'])\n",
    "                #print(sen_df)\n",
    "                # get predictions from viterbi\n",
    "                preds = np.array(viterbi(sen_df))\n",
    "                # compare preds and target, increment correct\n",
    "                if is_dev:\n",
    "                    targets = np.array(targets)\n",
    "                    correct += np.sum(preds == targets)\n",
    "                # Generate output \n",
    "                for i in range(len(preds)):\n",
    "                    o_row = sentence[i][:]\n",
    "                    o_row.append(preds[i])\n",
    "                    output.append(o_row)\n",
    "                output.append([None,None,None])\n",
    "                # empty sentence and targets for the next sentence\n",
    "                sentence = []\n",
    "                targets = []\n",
    "        sentence.append(w_row)\n",
    "        if is_dev:\n",
    "            targets.append(target)\n",
    "\n",
    "    # Process the last complete sentece from input df        \n",
    "    if sentence:\n",
    "        sen_df = pd.DataFrame(sentence, columns =['s_idx', 'word_type'])\n",
    "        #print(sen_df)\n",
    "        # get predictions from viterbi\n",
    "        preds = np.array(viterbi(sen_df))\n",
    "        # compare preds and target, increment correct\n",
    "        if is_dev:\n",
    "            targets = np.array(targets)\n",
    "            correct += np.sum(preds == targets)\n",
    "        # Generate output\n",
    "        for i in range(len(preds)):\n",
    "            o_row = sentence[i][:]\n",
    "            o_row.append(preds[i])\n",
    "            output.append(o_row)\n",
    "#         output.append([None,None,None])\n",
    "    accuracy = round(correct/total*100, 3)\n",
    "    \n",
    "    return accuracy, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "98c6248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Viterbi] Testing on Dev data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131768/131768 [01:48<00:00, 1214.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Dataset Accuracy: 95.338%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_df = pd.read_csv(\"data/dev\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])\n",
    "print(\"[Viterbi] Testing on Dev data...\")\n",
    "accuracy, output = viterbi_decoder(dev_df, False, True)\n",
    "print(\"Dev Dataset Accuracy: {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac144cf",
   "metadata": {},
   "source": [
    "#### What is the accuracy on the dev data?\n",
    "<p> Threshold: 2, Viterbi accuracy on Dev Dataset: 95.233% </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "99adf285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Viterbi] Generating predications on Test data viterbi.out...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129654/129654 [01:56<00:00, 1115.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Output a viterbi.out on test dataset\n",
    "test_df = pd.read_csv(\"data/test\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])            \n",
    "print(\"[Viterbi] Generating predications on Test data viterbi.out...\")\n",
    "# Accuracy is 0 here, output contains predications for each sentence, separated by rows with None values\n",
    "accuracy, output = viterbi_decoder(test_df, True, False)\n",
    "\n",
    "# Store predications to greedy.out\n",
    "Predictions = np.array(output)\n",
    "greedy_output_df = pd.DataFrame(Predictions, columns = [\"s_idx\", \"word_type\", \"pos\"])\n",
    "greedy_output_df.to_csv('viterbi.out', sep='\\t', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
