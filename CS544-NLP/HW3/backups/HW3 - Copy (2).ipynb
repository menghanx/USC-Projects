{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0449988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re, time, json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "dca2d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "df = pd.read_csv(\"data/train\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])\n",
    "\n",
    "# The starting POS tag distribution from training dataset\n",
    "start_tag_distributions = df[ df['s_idx'] == 1 ][\"pos\"].value_counts(normalize=True)\n",
    "\n",
    "# All the possible pos tags from training dataset\n",
    "pos_tags = df['pos'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1b8af089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab list\n",
    "vocab = pd.read_csv('vocab.txt', sep='\\t', names=['word_type', 'vocab_idx', 'occurrence'])\n",
    "# unique words in vocab for unk word assignment\n",
    "vocab_list = set(vocab[\"word_type\"].unique().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1572d704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s_idx  word_type  pos\n",
       "3      1988       CD     34\n",
       "8      10         CD     33\n",
       "11     30         CD     32\n",
       "9      8          CD     32\n",
       "4      1          CD     29\n",
       "                         ..\n",
       "13     2.3        CD      1\n",
       "       2.4225     CD      1\n",
       "       2.45       CD      1\n",
       "       2.5        CD      1\n",
       "110    50,000     CD      1\n",
       "Length: 14013, dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do something to the number strings\n",
    "df[ df['word_type'].str.match('\\d\\.?\\d*')==True ].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ac3ae",
   "metadata": {},
   "source": [
    "# Taks 1  - Generate vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a5058b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 2, Vocab size: 23183, <unk> occurrence: 20011\n"
     ]
    }
   ],
   "source": [
    "# Use value_count() on word_type column to get the occurrences of word types\n",
    "unique_words = df['word_type'].value_counts()\n",
    "unique_words = unique_words.reset_index()\n",
    "vocab = pd.DataFrame(unique_words)\n",
    "\n",
    "# Add index column\n",
    "vocab[\"vocab_idx\"] = vocab.index\n",
    "\n",
    "# Rename and rearrange columns\n",
    "vocab.columns = ['word_type', 'occurrence', 'vocab_idx']\n",
    "vocab = vocab[['word_type', 'vocab_idx', 'occurrence']]\n",
    "\n",
    "# set unknown word threshold, and drop word types with low frequency from vocab\n",
    "threshold = 2\n",
    "unks = vocab[ vocab['occurrence'] < threshold ]\n",
    "vocab = vocab[ vocab['occurrence'] >= threshold ]\n",
    "\n",
    "# sum the occurrences of all the unknown words and put it in the first row of vocab\n",
    "unks_df = pd.DataFrame([[\"<unk>\", 0, unks.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "vocab = unks_df.append(vocab, ignore_index=True)\n",
    "\n",
    "# reindex and update vocab_idx values\n",
    "vocab[\"vocab_idx\"] = vocab.index\n",
    "\n",
    "# unique words in vocab for unk word assignment\n",
    "vocab_list = set(vocab[\"word_type\"].unique().flatten())\n",
    "\n",
    "print(\"Threshold: {}, Vocab size: {}, <unk> occurrence: {}\".format(threshold, vocab.shape[0], unks.occurrence.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04804d9c",
   "metadata": {},
   "source": [
    "<p>no preprocess: Threshold: 2, Vocab size: 23183, unk occurrence: 20011</p>\n",
    "<p>lowercase: Threshold: 2, Vocab size: 21158, unk occurrence: 17401</p>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f2060af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab.to_csv('vocab'+'_'+str(threshold)+'.txt', sep='\\t', header=False, index=False)\n",
    "vocab.to_csv('vocab.txt', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388bbe5",
   "metadata": {},
   "source": [
    "#### What is the selected threshold for unknown words replacement?\n",
    "<p> The selected threshold is 2 </p>\n",
    "    \n",
    "#### What is the total size of your vocabulary\n",
    "<p> The size of my vocabulary is 23183 </p>\n",
    "\n",
    "#### what is the total occurrences of the special token < unk > after replacement?\n",
    "<p> The total occurrences of unk is 20011 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f783d0d",
   "metadata": {},
   "source": [
    "# Taks 2  - Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c4993795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 912095/912095 [00:39<00:00, 23141.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitions: 1351, Emissions:30303\n"
     ]
    }
   ],
   "source": [
    "# for calculating count(tag)\n",
    "pos_distributions = df['pos'].value_counts().to_dict()\n",
    "\n",
    "# keep track of the transitions and emissions\n",
    "transitions = {}\n",
    "emissions = {}\n",
    "\n",
    "# keep track of the keys in transitions and emissions\n",
    "e_keys = set()\n",
    "t_keys = set()\n",
    "\n",
    "prev_pos = None\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    cur_word = row['word_type']\n",
    "    cur_pos = row['pos']\n",
    "    # replace low frequent word with <unk>\n",
    "    if cur_word not in vocab_list:\n",
    "        cur_word = \"<unk>\"\n",
    "        \n",
    "    e_key = cur_pos+\",\"+cur_word\n",
    "    if e_key in e_keys:\n",
    "        emissions[e_key]+=(1/pos_distributions[cur_pos])\n",
    "    else:\n",
    "        emissions[e_key]=(1/pos_distributions[cur_pos])\n",
    "        e_keys.add(e_key)\n",
    "    # skip transition for the first word in a sentence\n",
    "    if row['s_idx'] != 1:\n",
    "        t_key = prev_pos+\",\"+cur_pos\n",
    "        if t_key in t_keys:\n",
    "            transitions[t_key]+=(1/pos_distributions[prev_pos])\n",
    "        else:\n",
    "            transitions[t_key]=(1/pos_distributions[prev_pos])\n",
    "            t_keys.add(t_key)\n",
    "    prev_pos = cur_pos\n",
    "\n",
    "    \n",
    "print(\"Transitions: {}, Emissions:{}\".format(len(transitions), len(emissions)))\n",
    "\n",
    "# put transitions and emissions into hmm_model\n",
    "hmm_model = {\"transition\": transitions, \"emission\" : emissions}\n",
    "# store hmm into a file\n",
    "with open('hmm.json', 'w') as fp:\n",
    "    json.dump(hmm_model, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502d33f",
   "metadata": {},
   "source": [
    "#### How many transition and emission parameters in your HMM?\n",
    "<p> 1351 pairs of transitions,  23373 pairs of emissions </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a00560",
   "metadata": {},
   "source": [
    "# Task 3: Greedy Decoding withHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "258b61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm_model = json.load(open('hmm'+'_'+str(threshold)+'.json',))\n",
    "hmm_model = json.load(open('hmm.json',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ae9130f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy decoding function, return accuracy and output_array according to parameters\n",
    "# dataset: df, output: boolean, is_dev: boolean\n",
    "def greedy(dataset, output, is_dev):\n",
    "    # for accuracy computation\n",
    "    total, correct = 0, 0\n",
    "    \n",
    "    # keep track of prev state\n",
    "    prev_tag = None\n",
    "    \n",
    "    # output file\n",
    "    output_array = []\n",
    "    \n",
    "    # Iterate over dataset, print progress\n",
    "    for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        word = row['word_type']\n",
    "        if output: \n",
    "            output_row = [row['s_idx'], word]\n",
    "        \n",
    "        if is_dev:\n",
    "            target = row['pos']\n",
    "        \n",
    "        if word not in vocab_list:\n",
    "            word = \"<unk>\"\n",
    "\n",
    "        pred = None\n",
    "        max_s = -1\n",
    "\n",
    "        # For starting word:\n",
    "        if row[\"s_idx\"] == 1:\n",
    "            # for each possible starting tag    \n",
    "            for tag in start_tag_distributions.keys():\n",
    "                t,e = 0,0\n",
    "                # t(s_j)\n",
    "                t = start_tag_distributions[tag]\n",
    "                # e(x|s) = 0 if the emission is not seen in training data\n",
    "                if tag+','+word in hmm_model['emission']:\n",
    "                    e = hmm_model['emission'][tag+','+word]\n",
    "                s = e*t\n",
    "                # argmax s and update predication\n",
    "                if s > max_s:\n",
    "                    max_s = s\n",
    "                    pred = tag  \n",
    "        # For the rest of the sentence\n",
    "        else:\n",
    "            # for each possible tag\n",
    "            for tag in pos_tags:\n",
    "                t,e = 0,0\n",
    "                \n",
    "                # t(s_j|s_j-1) = 0 if the transition is not seen in training data\n",
    "                if prev_tag+','+tag in hmm_model['transition']:\n",
    "                    t = hmm_model['transition'][prev_tag+','+tag]\n",
    "                    # e(x|s) = 0 if the emission is not seen in training data\n",
    "                    if tag+','+word in hmm_model['emission']: \n",
    "                        e = hmm_model['emission'][tag+','+word]\n",
    "                s = e*t\n",
    "                # argmax s and update predication\n",
    "                if s > max_s:\n",
    "                    max_s = s\n",
    "                    pred = tag        \n",
    "        if(output):   \n",
    "            output_row.append(pred)\n",
    "            # Add empty row between sentences\n",
    "            if row[\"s_idx\"] == 1:\n",
    "                output_array.append([None, None, None])\n",
    "            output_array.append(output_row)\n",
    "            \n",
    "        # remember the current predication as the prev_tag for next observation\n",
    "        prev_tag = pred\n",
    "        # increment correct if predicted right\n",
    "        if is_dev:\n",
    "            if pred == target:\n",
    "                correct +=1\n",
    "        total +=1\n",
    "\n",
    "    # output , remove the first Nan\n",
    "    return round(correct/total*100, 3), output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "2320e2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131768/131768 [00:12<00:00, 10244.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Dataset Accuracy: 93.503%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dev dataset evaluation\n",
    "dev_df = pd.read_csv(\"data/dev\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])\n",
    "\n",
    "is_output, is_dev = False, True\n",
    "\n",
    "accuracy, output = greedy(dev_df, is_output, is_dev)\n",
    "\n",
    "print(\"Dev Dataset Accuracy: {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "17f681ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129654/129654 [00:11<00:00, 11590.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test dataset predication\n",
    "test_df = pd.read_csv(\"data/test\", sep='\\t', names=[\"s_idx\", \"word_type\"])\n",
    "\n",
    "is_output, is_dev = True, False\n",
    "\n",
    "accuracy, output = greedy(test_df, is_output, is_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "209f1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predications to greedy.out\n",
    "# Remove 1st empty line from output\n",
    "Predictions = np.array(output[1:])\n",
    "greedy_output_df = pd.DataFrame(Predictions, columns = [\"s_idx\", \"word_type\", \"pos\"])\n",
    "greedy_output_df.to_csv('greedy.out', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2eaf40",
   "metadata": {},
   "source": [
    "#### What is the accuracy on the dev data?\n",
    "<p> 93.50% </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745c6e3",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding withHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8a83714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Algorithm Implementation, take dataset contains one sentences from pos 1 to pos m\n",
    "# dataset: df, output: boolean, is_dev: boolean\n",
    "def viterbi(dataset):\n",
    "    # pi table\n",
    "    pi = {}\n",
    "    # path table\n",
    "    paths = {}\n",
    "    # best tag for each position\n",
    "    best = None\n",
    "    # position \n",
    "    j = 0\n",
    "    \n",
    "    # Iterate over dataset, print progress\n",
    "#     for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "    for i, row in dataset.iterrows():\n",
    "        j = row['s_idx']\n",
    "        # the <unk> tag was already replaced in input dataset\n",
    "        word = row['word_type']\n",
    "        # Keep track of the pi value for each pos tag\n",
    "        # {\"DT\" : 0.001  }\n",
    "        pi_pos = {}\n",
    "        # keep track of the path for each pos in pi_pos\n",
    "        pi_path = {}\n",
    "    \n",
    "        if j == 1:\n",
    "            # reset pi for each sentence\n",
    "            pi = {}\n",
    "            paths = {}\n",
    "            \n",
    "            for tag in pos_tags:\n",
    "                # initialize transition and emission\n",
    "                t,e = 0,0\n",
    "                # start word t = start tag distribution\n",
    "                if tag in start_tag_distributions.keys():\n",
    "                    t = start_tag_distributions[tag]\n",
    "                    if tag+','+word in hmm_model['emission']:\n",
    "                        e = hmm_model['emission'][tag+','+word]\n",
    "                # record pi for each starting tag\n",
    "                pi_pos[tag] = e*t\n",
    "                # start the path for each starting tag\n",
    "                pi_path[tag] = [tag]\n",
    "            # record the pi and paths at each position j\n",
    "            pi[j] = pi_pos\n",
    "            paths[j] = pi_path\n",
    "                  \n",
    "        else:\n",
    "            for cur_tag in pos_tags:\n",
    "                pi_pos[cur_tag] = -1\n",
    "\n",
    "                # pi[j-1][prev_tag], t(cur_tag|prev_tag), e(word|cur_tag)\n",
    "                for prev_tag in pos_tags:\n",
    "                    prev_pi = pi[j-1][prev_tag]\n",
    "                    # Skip to the next prev_tag if pi[j-1, prev_tag] = 0\n",
    "                    cur_pi = 0\n",
    "                    # cur_pi = 0 if pre_pi = 0 so only consider the non zero prev_pi\n",
    "                    if prev_pi != 0:\n",
    "                        t,e = 0,0\n",
    "                        # if transition exists\n",
    "                        if prev_tag+','+cur_tag in hmm_model['transition']:\n",
    "                                t = hmm_model['transition'][prev_tag+','+cur_tag]\n",
    "                                # if emission exists\n",
    "                                if cur_tag+','+word in hmm_model['emission']: \n",
    "                                    e = hmm_model['emission'][cur_tag+','+word]\n",
    "\n",
    "                        cur_pi = prev_pi*e*t\n",
    "                    \n",
    "                    # update pi at each position as well as update the path to get to the best pi\n",
    "                    if cur_pi > pi_pos[cur_tag]:\n",
    "                        pi_pos[cur_tag] = cur_pi\n",
    "                        pi_path[cur_tag] = paths[j-1][prev_tag][:]\n",
    "                        pi_path[cur_tag].append(cur_tag)\n",
    "                        \n",
    "            # record the pi and paths at each position j           \n",
    "            pi[j] = pi_pos\n",
    "            paths[j] = pi_path\n",
    "        \n",
    "    # with j being the last position, get the best tag with highest pi value\n",
    "    best = max(pi[j], key=pi[j].get) \n",
    "    # return the sequence that led to the best tag\n",
    "#     print(pi)\n",
    "#     print(paths)\n",
    "    return paths[j][best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "06d3ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoder(dataset, is_output, is_dev):\n",
    "\n",
    "    # Represent each sentence key = word, value = pos, e.g. {\"word\", 1}\n",
    "    sentence = []\n",
    "    targets = []\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Output\n",
    "    output = []\n",
    "\n",
    "    for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        # increment total\n",
    "        total += 1\n",
    "        # Get word and sentence_idx of each row\n",
    "        j = row['s_idx']\n",
    "        if is_dev:\n",
    "            target = row['pos']\n",
    "        word = row['word_type']\n",
    "        if word not in vocab_list:\n",
    "                word = \"<unk>\"\n",
    "        w_row = [j, word]\n",
    "\n",
    "        if j == 1:\n",
    "            # Process previous complete sentence stored in dict sentence\n",
    "            if sentence:     \n",
    "                sen_df = pd.DataFrame(sentence, columns =['s_idx', 'word_type'])\n",
    "                #print(sen_df)\n",
    "                # get predictions from viterbi\n",
    "                preds = np.array(viterbi(sen_df))\n",
    "                # compare preds and target, increment correct\n",
    "                if is_dev:\n",
    "                    targets = np.array(targets)\n",
    "                    correct += np.sum(preds == targets)\n",
    "                # Generate output \n",
    "                for i in range(len(preds)):\n",
    "                    o_row = sentence[i][:]\n",
    "                    o_row.append(preds[i])\n",
    "                    output.append(o_row)\n",
    "                output.append([None,None,None])\n",
    "                # empty sentence and targets for the next sentence\n",
    "                sentence = []\n",
    "                targets = []\n",
    "        sentence.append(w_row)\n",
    "        if is_dev:\n",
    "            targets.append(target)\n",
    "\n",
    "    # Process the last complete sentece from input df        \n",
    "    if sentence:\n",
    "        sen_df = pd.DataFrame(sentence, columns =['s_idx', 'word_type'])\n",
    "        #print(sen_df)\n",
    "        # get predictions from viterbi\n",
    "        preds = np.array(viterbi(sen_df))\n",
    "        # compare preds and target, increment correct\n",
    "        if is_dev:\n",
    "            targets = np.array(targets)\n",
    "            correct += np.sum(preds == targets)\n",
    "        # Generate output\n",
    "        for i in range(len(preds)):\n",
    "            o_row = sentence[i][:]\n",
    "            o_row.append(preds[i])\n",
    "            output.append(o_row)\n",
    "#         output.append([None,None,None])\n",
    "    accuracy = round(correct/total*100, 2)\n",
    "    \n",
    "    return accuracy, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5ab526fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131768/131768 [01:42<00:00, 1291.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Dataset Accuracy: 94.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_df = pd.read_csv(\"data/dev\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])            \n",
    "accuracy, output = viterbi_decoder(dev_df, False, True)\n",
    "print(\"Dev Dataset Accuracy: {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "052e0e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129654/129654 [01:36<00:00, 1347.09it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"data/test\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])            \n",
    "accuracy, output = viterbi_decoder(test_df, True, False)\n",
    "# Store predications to greedy.out\n",
    "# Remove 1st empty line from output\n",
    "Predictions = np.array(output)\n",
    "greedy_output_df = pd.DataFrame(Predictions, columns = [\"s_idx\", \"word_type\", \"pos\"])\n",
    "greedy_output_df.to_csv('viterbi.out', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e107d1",
   "metadata": {},
   "source": [
    "threshold 2, viterbi dev accuracy: 94.77%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac144cf",
   "metadata": {},
   "source": [
    "#### What is the accuracy on the dev data?\n",
    "<p> 94.77% </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "51007444",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### Viterbi algorithm test\n",
    "# d = {'DT': 0.8, 'NN': 0.2, 'VB': 0}\n",
    "# start_tag_distributions = pd.Series(data=d, index=['DT', 'NN', 'VB'])\n",
    "\n",
    "# trans = {}\n",
    "\n",
    "# tags = [\"DT\", \"NN\", \"VB\"]\n",
    "# pos_tags = [\"DT\", \"NN\", \"VB\"]\n",
    "\n",
    "# for tag in tags:\n",
    "#     for tag2 in tags:\n",
    "#         trans[tag+\",\"+tag2] = 0\n",
    "\n",
    "# trans['DT,DT'] = 0\n",
    "# trans['DT,NN'] = 0.9\n",
    "# trans['DT,VB'] = 0.1\n",
    "# trans['NN,DT'] = 0\n",
    "# trans['NN,NN'] = 0.5\n",
    "# trans['NN,VB'] = 0.5\n",
    "# trans['VB,DT'] = 0.5\n",
    "# trans['VB,NN'] = 0.5\n",
    "# trans['VB,VB'] = 0\n",
    "\n",
    "# words = [\"the\", \"fans\", \"watch\", \"the\", \"race\"]\n",
    "# vocab_list = words\n",
    "\n",
    "\n",
    "# emis = {}\n",
    "\n",
    "# for tag in tags:\n",
    "#     for word in words:\n",
    "# #         print(\"emis['{},{}'] =\".format(tag,word) )\n",
    "#         emis[tag+\",\"+word] = 0\n",
    "        \n",
    "# emis['DT,the'] = 0.2\n",
    "# emis['DT,fans'] = 0\n",
    "# emis['DT,watch'] = 0\n",
    "# emis['DT,race'] = 0\n",
    "# emis['NN,the'] = 0\n",
    "# emis['NN,fans'] = 0.1\n",
    "# emis['NN,watch'] = 0.3\n",
    "# emis['NN,race'] = 0.1\n",
    "# emis['VB,the'] = 0\n",
    "# emis['VB,fans'] = 0.2\n",
    "# emis['VB,watch'] = 0.15\n",
    "# emis['VB,race'] = 0.3\n",
    "\n",
    "# hmm_model = {'transition': trans, 'emission': emis}\n",
    "\n",
    "# dev_df = pd.read_csv(\"test/dev\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])\n",
    "# #dev_df = dev_df.head(37)\n",
    "\n",
    "# print(words)\n",
    "# viterbi(dev_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
