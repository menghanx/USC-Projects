{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b0449988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re, json\n",
    "from tqdm import tqdm\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dca2d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "df = pd.read_csv(\"data/train\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])\n",
    "\n",
    "# The starting POS tag distribution from training dataset\n",
    "start_tag_distributions = df[ df['s_idx'] == 1 ][\"pos\"].value_counts(normalize=True)\n",
    "\n",
    "# All the possible pos tags from training dataset\n",
    "pos_tags = df['pos'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b8af089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab list - for re-run the program\n",
    "if exists('vocab.txt'):\n",
    "    vocab = pd.read_csv('vocab.txt', sep='\\t', names=['word_type', 'vocab_idx', 'occurrence'])\n",
    "    # a set of unique words in the vocab for unk word assignment\n",
    "    vocab_list = set(vocab[\"word_type\"].unique().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256ac3ae",
   "metadata": {},
   "source": [
    "# Taks 1  - Generate vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6a5058b3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use value_count() on word_type column to get the occurrences of word types\n",
    "unique_words = df['word_type'].value_counts()\n",
    "unique_words = unique_words.reset_index()\n",
    "vocab = pd.DataFrame(unique_words)\n",
    "\n",
    "# Add index column\n",
    "vocab[\"vocab_idx\"] = vocab.index\n",
    "\n",
    "# Rename and rearrange columns\n",
    "vocab.columns = ['word_type', 'occurrence', 'vocab_idx']\n",
    "vocab = vocab[['word_type', 'vocab_idx', 'occurrence']]\n",
    "\n",
    "# set unknown word threshold, and drop word types with low frequency from vocab\n",
    "threshold = 2\n",
    "unks = vocab[ vocab['occurrence'] < threshold ]\n",
    "vocab = vocab[ vocab['occurrence'] >= threshold ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3e188cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN     2114\n",
       "NNS    1394\n",
       "JJ     1081\n",
       "VBG     591\n",
       "VB      376\n",
       "VBZ     342\n",
       "RB      317\n",
       "VBP      89\n",
       "JJR      49\n",
       "FW       40\n",
       "JJS      31\n",
       "VBN      26\n",
       "CD       24\n",
       "VBD      23\n",
       "RBR       6\n",
       "NNP       5\n",
       "IN        4\n",
       "UH        3\n",
       "PRP       2\n",
       "CC        1\n",
       "MD        1\n",
       "WDT       1\n",
       "SYM       1\n",
       "Name: pos, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Conditional Random Fields - Process Unknown Words\n",
    "### Define Regex for capitalization or morphology\n",
    "\n",
    "# Non digit Regex\n",
    "none_digit_regex = '^(?:[^0-9]*)$'\n",
    "# word contains number tend to be CD and JJ\n",
    "num_cd_regex = '^(?:[0-9.,:]*)$'\n",
    "# word contains upper case letter tend to be NNP\n",
    "cap_str_regex = '.*[A-Z].*'\n",
    "# word contains a \"-\" are likely to be JJ\n",
    "jj_regex = '^.*-.*$'\n",
    "# Word with suffix \"ed\"\n",
    "vbn_regex = '^.*ed$'\n",
    "\n",
    "# unknow strings that do not contain any digit\n",
    "# unk_strs = unks[ unks['word_type'].str.match(none_digit_regex)==True ]\n",
    "\n",
    "\n",
    "\n",
    "# UPPER CASE = NNP\n",
    "unk_upper_nnp = unks[ (unks['word_type'].str.match(cap_str_regex)==True)]\n",
    "unk_strs = unks[ unks['word_type'].str.match(cap_str_regex)==False ]\n",
    "\n",
    "# DIGITS = CD\n",
    "unk_cd = unk_strs[ unk_strs['word_type'].str.match(num_cd_regex)==True ]\n",
    "unk_strs = unk_strs[ unk_strs['word_type'].str.match(num_cd_regex)==False ]\n",
    "\n",
    "# CONTAINS \"-\" = JJ\n",
    "unks_jj = unk_strs[ unk_strs['word_type'].str.match(jj_regex)==True ]\n",
    "unk_strs = unk_strs[ unk_strs['word_type'].str.match(jj_regex)==False ]\n",
    "\n",
    "# SUFFIX \"ed\" = VBN\n",
    "unk_vnb = unk_strs[ unk_strs['word_type'].str.match(vbn_regex)==True ]\n",
    "unk_strs = unk_strs[ unk_strs['word_type'].str.match(vbn_regex)==False ]\n",
    "\n",
    "# Rest of Unknowns\n",
    "unks_df = pd.DataFrame([[\"<unk>\", 0, unk_strs.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "\n",
    "# Generate unknown categories\n",
    "unk_upper_nnp = pd.DataFrame([[\"<unk_upper_nnp>\", 0, unk_upper_nnp.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "unk_cd = pd.DataFrame([[\"<unk_nums_cd>\", 0, unk_cd.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "unks_jj = pd.DataFrame([[\"<unk_jj>\", 0, unks_jj.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "unk_vnb = pd.DataFrame([[\"<unk_vbn>\", 0, unk_vnb.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "\n",
    "# Append unknown categories to the rest of unknowns\n",
    "unks_df = unks_df.append(unk_upper_nnp, ignore_index=True)\n",
    "unks_df = unks_df.append(unk_cd, ignore_index=True)\n",
    "unks_df = unks_df.append(unks_jj, ignore_index=True)\n",
    "unks_df = unks_df.append(unk_vnb, ignore_index=True)\n",
    "\n",
    "\n",
    "combined = pd.merge(unk_strs, df)\n",
    "\n",
    "combined[\"pos\"].value_counts()\n",
    "\n",
    "#combined[ combined['pos'] == 'NNS' ]\n",
    "\n",
    "#combined.word_type.to_csv('outfile.txt', index=False)\n",
    "\n",
    "\n",
    "# unk strs with uppercase letter\n",
    "# upper_unks = unk_strs[ (unk_strs['word_type'].str.match(cap_str_regex)==True)]\n",
    "# Rest of the lowercase strs\n",
    "# unk_strs = unk_strs[ (unk_strs['word_type'].str.match(cap_str_regex)==False)]\n",
    "# add uppercase word occurance\n",
    "# unk_upper_nnp = pd.DataFrame([[\"<unk_upper_nnp>\", 0, upper_unks.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "\n",
    "# unknown words ends with \"ed\" and contains \"-\"\n",
    "# unk_past_jj = unk_strs[ (unk_strs['word_type'].str.match(past_tense_jj_regex)==True)]\n",
    "# rest of the unknown\n",
    "# unk_strs = unk_strs[ (unk_strs['word_type'].str.match(past_tense_jj_regex)==False)]\n",
    "# add past jj occurance\n",
    "# unk_past_jj = pd.DataFrame([[\"<unk_past_jj>\", 0, unk_past_jj.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "\n",
    "# sum the occurrences of all the unknown words and put it in the first row of vocab\n",
    "# unks_df = pd.DataFrame([[\"<unk>\", 0, unk_strs.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "# unks_df = unks_df.append(unk_upper_nnp, ignore_index=True)\n",
    "# unks_df = unks_df.append(unk_past_jj, ignore_index=True)\n",
    "\n",
    "# Get all unknown numbers\n",
    "# unk_nums = unks[ unks['word_type'].str.contains(none_digit_regex)==False ]\n",
    "\n",
    "# divide unk nums into jj and cd\n",
    "# unk_nums_cd = unk_nums[ unk_nums['word_type'].str.contains(num_cd_regex)==True ]\n",
    "# unk_nums_jj = unk_nums[ unk_nums['word_type'].str.contains(num_cd_regex)==False]\n",
    "# unk_nums_cd = pd.DataFrame([[\"<unk_nums_cd>\", 0, unk_nums_cd.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "# unk_nums_jj = pd.DataFrame([[\"<unk_nums_jj>\", 0, unk_nums_jj.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "\n",
    "# unks_df = unks_df.append(unk_nums_cd, ignore_index=True)\n",
    "# unks_df = unks_df.append(unk_nums_jj, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0184711e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_type</th>\n",
       "      <th>vocab_idx</th>\n",
       "      <th>occurrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>6521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;unk_upper_nnp&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>6963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;unk_nums_cd&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;unk_jj&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>2874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;unk_vbn&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23182</th>\n",
       "      <td>transports</td>\n",
       "      <td>23182</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23183</th>\n",
       "      <td>employee-health</td>\n",
       "      <td>23183</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23184</th>\n",
       "      <td>looting</td>\n",
       "      <td>23184</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23185</th>\n",
       "      <td>diapers</td>\n",
       "      <td>23185</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23186</th>\n",
       "      <td>precarious</td>\n",
       "      <td>23186</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23187 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word_type  vocab_idx  occurrence\n",
       "0                <unk>          0        6521\n",
       "1      <unk_upper_nnp>          1        6963\n",
       "2        <unk_nums_cd>          2        2700\n",
       "3             <unk_jj>          3        2874\n",
       "4            <unk_vbn>          4         953\n",
       "...                ...        ...         ...\n",
       "23182       transports      23182           2\n",
       "23183  employee-health      23183           2\n",
       "23184          looting      23184           2\n",
       "23185          diapers      23185           2\n",
       "23186       precarious      23186           2\n",
       "\n",
       "[23187 rows x 3 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unks_df = pd.DataFrame([[\"<unk>\", 0, unks.occurrence.sum()]], columns = ['word_type', 'vocab_idx', 'occurrence'])\n",
    "# creak vocab DF and store it to a file\n",
    "vocab = unks_df.append(vocab, ignore_index=True)\n",
    "vocab.to_csv('vocab.txt', sep='\\t', header=False, index=False)\n",
    "\n",
    "\n",
    "# reindex and update vocab_idx values\n",
    "vocab[\"vocab_idx\"] = vocab.index\n",
    "\n",
    "# a set of unique words in the vocab for unk word assignment\n",
    "vocab_list = set(vocab[\"word_type\"].unique().flatten())\n",
    "\n",
    "#print(\"Threshold: {}, \\nVocab size: {}, \\n<unk> occurrence: {}, \\n<unk_upper_nnp> occurrence: {}, \\n<unk_nums_cd> occurrence: {}, \\n<unk_nums_jj> occurrence: {}, \\n<unk_past_jj> occurrence: {}\"\n",
    "      #.format(threshold, vocab.shape[0], unk_strs.occurrence.sum(), unk_upper_nnp.occurrence.sum(), unk_nums_cd.occurrence.sum(), unk_nums_jj.occurrence.sum(), unk_past_jj.occurrence.sum()))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cd04e882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_type</th>\n",
       "      <th>vocab_idx</th>\n",
       "      <th>occurrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>6521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;unk_upper_nnp&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>6963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;unk_nums_cd&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;unk_jj&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>2874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;unk_vbn&gt;</td>\n",
       "      <td>4</td>\n",
       "      <td>2874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23182</th>\n",
       "      <td>transports</td>\n",
       "      <td>23182</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23183</th>\n",
       "      <td>employee-health</td>\n",
       "      <td>23183</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23184</th>\n",
       "      <td>looting</td>\n",
       "      <td>23184</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23185</th>\n",
       "      <td>diapers</td>\n",
       "      <td>23185</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23186</th>\n",
       "      <td>precarious</td>\n",
       "      <td>23186</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23187 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word_type  vocab_idx  occurrence\n",
       "0                <unk>          0        6521\n",
       "1      <unk_upper_nnp>          1        6963\n",
       "2        <unk_nums_cd>          2        2700\n",
       "3             <unk_jj>          3        2874\n",
       "4            <unk_vbn>          4        2874\n",
       "...                ...        ...         ...\n",
       "23182       transports      23182           2\n",
       "23183  employee-health      23183           2\n",
       "23184          looting      23184           2\n",
       "23185          diapers      23185           2\n",
       "23186       precarious      23186           2\n",
       "\n",
       "[23187 rows x 3 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4b9ac3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the input word is in vocab list, if not, categorize it based the word format\n",
    "def checkWord(word):\n",
    "    # Non digit Regex\n",
    "    none_digit_regex = '^(?:[^0-9]*)$'\n",
    "    # word contains number tend to be CD and JJ\n",
    "    num_cd_regex = '^(?:[0-9.,:]*)$'\n",
    "    # word contains upper case letter tend to be NNP\n",
    "    cap_str_regex = '.*[A-Z].*'\n",
    "    # word contains a \"-\" are likely to be JJ\n",
    "    jj_regex = '^.*-.*$'\n",
    "    # Word with suffix \"ed\"\n",
    "    vbn_regex = '^.*ed$'\n",
    "\n",
    "    if word in vocab_list:\n",
    "        return word\n",
    "    \n",
    "    if bool(re.match(cap_str_regex, word)):\n",
    "        return '<unk_upper_nnp>'\n",
    "    elif bool(re.match(num_cd_regex, word)):\n",
    "        return '<unk_nums_cd>'\n",
    "    elif bool(re.match(jj_regex, word)):\n",
    "        return '<unk_jj>'\n",
    "    elif bool(re.match(vbn_regex, word)):\n",
    "        return '<unk_vbn>'\n",
    "    else:\n",
    "        return '<unk>'\n",
    "\n",
    "#     # Does not contains digits\n",
    "#     if bool(re.match(none_digit_regex, word)):\n",
    "#         # upper case letter\n",
    "#         if bool(re.match(cap_str_regex, word)):\n",
    "#             return '<unk_upper_nnp>'\n",
    "#         # word ends with \"ed\" and contains dash\n",
    "#         elif bool(re.match(past_tense_jj_regex, word)):\n",
    "#             return 'unk_past_jj'\n",
    "#         else:\n",
    "#             return '<unk>'\n",
    "#     # contains at least 1 digit\n",
    "#     else:\n",
    "#         # number contains digit and :,-. only\n",
    "#         if bool(re.match(num_cd_regex, word)):\n",
    "#             return '<unk_nums_cd>'\n",
    "#         else:\n",
    "#             return '<unk_nums_jj>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e6d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the input word is in vocab list, if not, categorize it based the word format\n",
    "def checkWord(word):\n",
    "    # Non digit Regex\n",
    "    none_digit_regex = '^(?:[^0-9]*)$'\n",
    "    # word contains number tend to be CD and JJ\n",
    "    num_cd_regex = '^(?:[0-9.,:]*)$'\n",
    "    # word contains upper case letter tend to be NNP\n",
    "    cap_str_regex = '.*[A-Z].*'\n",
    "    # word ends with \"ed\" and contains a \"-\" are likely to be JJ\n",
    "    past_tense_jj_regex = '.*\\-.*ed$'\n",
    "\n",
    "    if word in vocab_list:\n",
    "        return word\n",
    "    # Does not contains digits\n",
    "    if bool(re.match(none_digit_regex, word)):\n",
    "        # upper case letter\n",
    "        if bool(re.match(cap_str_regex, word)):\n",
    "            return '<unk_upper_nnp>'\n",
    "        # word ends with \"ed\" and contains dash\n",
    "        elif bool(re.match(past_tense_jj_regex, word)):\n",
    "            return 'unk_past_jj'\n",
    "        else:\n",
    "            return '<unk>'\n",
    "    # contains at least 1 digit\n",
    "    else:\n",
    "        # number contains digit and :,-. only\n",
    "        if bool(re.match(num_cd_regex, word)):\n",
    "            return '<unk_nums_cd>'\n",
    "        else:\n",
    "            return '<unk_nums_jj>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf528e5",
   "metadata": {},
   "source": [
    "Threshold: 2, Vocab size: 23186, total unknown words occurrence: 20011"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e388bbe5",
   "metadata": {},
   "source": [
    "#### What is the selected threshold for unknown words replacement?\n",
    "<p> The selected threshold is 2 </p>\n",
    "    \n",
    "#### What is the total size of your vocabulary\n",
    "<p> The size of my vocabulary is 23183 </p>\n",
    "\n",
    "#### what is the total occurrences of the special token < unk > after replacement?\n",
    "<p> The total occurrences of unk is 20011 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950c2ee",
   "metadata": {},
   "source": [
    "<p>lowercase: Threshold: 2, Vocab size: 21158, unk occurrence: 17401</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f783d0d",
   "metadata": {},
   "source": [
    "# Taks 2  - Model Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c4993795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputing hmm.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 912095/912095 [00:40<00:00, 22655.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transitions: 1351, Emissions:30347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# for calculating count(tag)\n",
    "pos_distributions = df['pos'].value_counts().to_dict()\n",
    "\n",
    "# keep track of the transitions and emissions\n",
    "transitions = {}\n",
    "emissions = {}\n",
    "\n",
    "# keep track of the keys in transitions and emissions\n",
    "e_keys = set()\n",
    "t_keys = set()\n",
    "\n",
    "prev_pos = None\n",
    "\n",
    "print(\"Outputing hmm.json...\")\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    cur_word = row['word_type']\n",
    "    cur_pos = row['pos']\n",
    "    # replace low frequent word with <unk>\n",
    "#     if cur_word not in vocab_list:\n",
    "#         cur_word = \"<unk>\"\n",
    "    cur_word = checkWord(cur_word)\n",
    "    \n",
    "    e_key = cur_pos+\",\"+cur_word\n",
    "    if e_key in e_keys:\n",
    "        emissions[e_key]+=(1/pos_distributions[cur_pos])\n",
    "    else:\n",
    "        emissions[e_key]=(1/pos_distributions[cur_pos])\n",
    "        e_keys.add(e_key)\n",
    "    # skip transition for the first word in a sentence\n",
    "    if row['s_idx'] != 1:\n",
    "        t_key = prev_pos+\",\"+cur_pos\n",
    "        if t_key in t_keys:\n",
    "            transitions[t_key]+=(1/pos_distributions[prev_pos])\n",
    "        else:\n",
    "            transitions[t_key]=(1/pos_distributions[prev_pos])\n",
    "            t_keys.add(t_key)\n",
    "    prev_pos = cur_pos\n",
    "\n",
    "    \n",
    "print(\"Transitions: {}, Emissions:{}\".format(len(transitions), len(emissions)))\n",
    "\n",
    "# put transitions and emissions into hmm_model\n",
    "hmm_model = {\"transition\": transitions, \"emission\" : emissions}\n",
    "# store hmm into a file\n",
    "\n",
    "# with open('hmm.json', 'w') as fp:\n",
    "#     json.dump(hmm_model, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1502d33f",
   "metadata": {},
   "source": [
    "#### How many transition and emission parameters in your HMM?\n",
    "<p> for threshold = 2, there are 1351 pairs of transitions,  30331 pairs of emissions </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a00560",
   "metadata": {},
   "source": [
    "# Task 3: Greedy Decoding withHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258b61f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists('hmm.json'):\n",
    "    hmm_model = json.load(open('hmm.json',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ae9130f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy decoding function, return accuracy and output_array according to parameters\n",
    "# dataset: entire dev dataset, output: boolean, is_dev: boolean\n",
    "def greedy(dataset, output, is_dev):\n",
    "    # for accuracy computation\n",
    "    total, correct = 0, 0\n",
    "    \n",
    "    # keep track of prev state\n",
    "    prev_tag = None\n",
    "    \n",
    "    # output file\n",
    "    output_array = []\n",
    "    \n",
    "    # Iterate over dataset, print progress\n",
    "    for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        word = row['word_type']\n",
    "        if output: \n",
    "            output_row = [row['s_idx'], word]\n",
    "        \n",
    "        if is_dev:\n",
    "            target = row['pos']\n",
    "        \n",
    "        # Check if word is in vocab, if not categorize unks\n",
    "        word = checkWord(word)\n",
    "\n",
    "        pred = None\n",
    "        max_s = -1\n",
    "\n",
    "        # For starting word:\n",
    "        if row[\"s_idx\"] == 1:\n",
    "            # for each possible starting tag    \n",
    "            for tag in start_tag_distributions.keys():\n",
    "                t,e = 0,0\n",
    "                # t(s_j)\n",
    "                t = start_tag_distributions[tag]\n",
    "                # e(x|s) = 0 if the emission is not seen in training data\n",
    "                if tag+','+word in hmm_model['emission']:\n",
    "                    e = hmm_model['emission'][tag+','+word]\n",
    "                s = e*t\n",
    "                # argmax s and update predication\n",
    "                if s > max_s:\n",
    "                    max_s = s\n",
    "                    pred = tag  \n",
    "        # For the rest of the sentence\n",
    "        else:\n",
    "            # for each possible tag\n",
    "            for tag in pos_tags:\n",
    "                t,e = 0,0\n",
    "                \n",
    "                # t(s_j|s_j-1) = 0 if the transition is not seen in training data\n",
    "                if prev_tag+','+tag in hmm_model['transition']:\n",
    "                    t = hmm_model['transition'][prev_tag+','+tag]\n",
    "                    # e(x|s) = 0 if the emission is not seen in training data\n",
    "                    if tag+','+word in hmm_model['emission']: \n",
    "                        e = hmm_model['emission'][tag+','+word]\n",
    "                s = e*t\n",
    "                # argmax s and update predication\n",
    "                if s > max_s:\n",
    "                    max_s = s\n",
    "                    pred = tag        \n",
    "        if(output):   \n",
    "            output_row.append(pred)\n",
    "            # Add empty row between sentences\n",
    "            if row[\"s_idx\"] == 1:\n",
    "                output_array.append([None, None, None])\n",
    "            output_array.append(output_row)\n",
    "            \n",
    "        # remember the current predication as the prev_tag for next observation\n",
    "        prev_tag = pred\n",
    "        # increment correct if predicted right\n",
    "        if is_dev:\n",
    "            if pred == target:\n",
    "                correct +=1\n",
    "        total +=1\n",
    "\n",
    "    # output , remove the first Nan\n",
    "    return round(correct/total*100, 3), output_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2320e2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Greedy] Testing on Dev data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131768/131768 [00:12<00:00, 10325.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Dataset Accuracy: 94.482%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dev dataset evaluation\n",
    "dev_df = pd.read_csv(\"data/dev\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])\n",
    "\n",
    "is_output, is_dev = False, True\n",
    "\n",
    "print(\"[Greedy] Testing on Dev data...\")\n",
    "\n",
    "accuracy, output = greedy(dev_df, is_output, is_dev)\n",
    "\n",
    "print(\"Dev Dataset Accuracy: {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "209f1029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Greedy] Generating predications on Test data  greedy.out...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129654/129654 [00:12<00:00, 10674.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test dataset predication\n",
    "test_df = pd.read_csv(\"data/test\", sep='\\t', names=[\"s_idx\", \"word_type\"])\n",
    "\n",
    "is_output, is_dev = True, False\n",
    "\n",
    "print(\"[Greedy] Generating predications on Test data  greedy.out...\")\n",
    "accuracy, output = greedy(test_df, is_output, is_dev)\n",
    "\n",
    "# Remove 1st empty line from output, due to the way output was generated\n",
    "Predictions = np.array(output[1:])\n",
    "\n",
    "# Store predications to greedy.out\n",
    "greedy_output_df = pd.DataFrame(Predictions, columns = [\"s_idx\", \"word_type\", \"pos\"])\n",
    "greedy_output_df.to_csv('greedy.out', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2eaf40",
   "metadata": {},
   "source": [
    "#### What is the accuracy on the dev data?\n",
    "<p> Threshold: 2, Greedy accuracy on Dev Dataset: 94.219% </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745c6e3",
   "metadata": {},
   "source": [
    "# Task 4: Viterbi Decoding withHMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a83714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi Algorithm Implementation, take dataset contains one sentences from pos 1 to pos m\n",
    "# dataset: df, output: boolean, is_dev: boolean\n",
    "def viterbi(dataset):\n",
    "    # pi table\n",
    "    pi = {}\n",
    "    # path table\n",
    "    paths = {}\n",
    "    # best tag for each position\n",
    "    best = None\n",
    "    # position \n",
    "    j = 0\n",
    "    \n",
    "    # Iterate over dataset, print progress\n",
    "#     for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "    for i, row in dataset.iterrows():\n",
    "        j = row['s_idx']\n",
    "        # the <unk> tag was already replaced in input dataset\n",
    "        word = row['word_type']\n",
    "        # Keep track of the pi value for each pos tag\n",
    "        # {\"DT\" : 0.001  }\n",
    "        pi_pos = {}\n",
    "        # keep track of the path for each pos in pi_pos\n",
    "        pi_path = {}\n",
    "    \n",
    "        if j == 1:\n",
    "            # reset pi for each sentence\n",
    "            pi = {}\n",
    "            paths = {}\n",
    "            \n",
    "            for tag in pos_tags:\n",
    "                # initialize transition and emission\n",
    "                t,e = 0,0\n",
    "                # start word t = start tag distribution\n",
    "                if tag in start_tag_distributions.keys():\n",
    "                    t = start_tag_distributions[tag]\n",
    "                    if tag+','+word in hmm_model['emission']:\n",
    "                        e = hmm_model['emission'][tag+','+word]\n",
    "                # record pi for each starting tag\n",
    "                pi_pos[tag] = e*t\n",
    "                # start the path for each starting tag\n",
    "                pi_path[tag] = [tag]\n",
    "            # record the pi and paths at each position j\n",
    "            pi[j] = pi_pos\n",
    "            paths[j] = pi_path\n",
    "                  \n",
    "        else:\n",
    "            for cur_tag in pos_tags:\n",
    "                pi_pos[cur_tag] = -1\n",
    "\n",
    "                # pi[j-1][prev_tag], t(cur_tag|prev_tag), e(word|cur_tag)\n",
    "                for prev_tag in pos_tags:\n",
    "                    prev_pi = pi[j-1][prev_tag]\n",
    "                    # Skip to the next prev_tag if pi[j-1, prev_tag] = 0\n",
    "                    cur_pi = 0\n",
    "                    # cur_pi = 0 if pre_pi = 0 so only consider the non zero prev_pi\n",
    "                    if prev_pi != 0:\n",
    "                        t,e = 0,0\n",
    "                        # if transition exists\n",
    "                        if prev_tag+','+cur_tag in hmm_model['transition']:\n",
    "                                t = hmm_model['transition'][prev_tag+','+cur_tag]\n",
    "                                # if emission exists\n",
    "                                if cur_tag+','+word in hmm_model['emission']: \n",
    "                                    e = hmm_model['emission'][cur_tag+','+word]\n",
    "\n",
    "                        cur_pi = prev_pi*e*t\n",
    "                    \n",
    "                    # update pi at each position as well as update the path to get to the best pi\n",
    "                    if cur_pi > pi_pos[cur_tag]:\n",
    "                        pi_pos[cur_tag] = cur_pi\n",
    "                        pi_path[cur_tag] = paths[j-1][prev_tag][:]\n",
    "                        pi_path[cur_tag].append(cur_tag)\n",
    "                        \n",
    "            # record the pi and paths at each position j           \n",
    "            pi[j] = pi_pos\n",
    "            paths[j] = pi_path\n",
    "        \n",
    "    # with j being the last position, get the best tag with highest pi value\n",
    "    best = max(pi[j], key=pi[j].get) \n",
    "    # return the sequence that led to the best tag\n",
    "#     print(pi)\n",
    "#     print(paths)\n",
    "    return paths[j][best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "515847fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoder(dataset, is_output, is_dev):\n",
    "\n",
    "    # Represent each sentence key = word, value = pos, e.g. {\"word\", 1}\n",
    "    sentence = []\n",
    "    targets = []\n",
    "\n",
    "    # Compute accuracy\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Output\n",
    "    output = []\n",
    "\n",
    "    for i, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        # increment total\n",
    "        total += 1\n",
    "        # Get word and sentence_idx of each row\n",
    "        j = row['s_idx']\n",
    "        if is_dev:\n",
    "            target = row['pos']\n",
    "        word = row['word_type']\n",
    "        \n",
    "        #if word not in vocab_list:\n",
    "        word = checkWord(word) \n",
    "    \n",
    "        # hold sentence from each row\n",
    "        w_row = [j, word]\n",
    "\n",
    "        if j == 1:\n",
    "            # Process previous complete sentence stored in dict sentence\n",
    "            if sentence:     \n",
    "                sen_df = pd.DataFrame(sentence, columns =['s_idx', 'word_type'])\n",
    "                #print(sen_df)\n",
    "                # get predictions from viterbi\n",
    "                preds = np.array(viterbi(sen_df))\n",
    "                # compare preds and target, increment correct\n",
    "                if is_dev:\n",
    "                    targets = np.array(targets)\n",
    "                    correct += np.sum(preds == targets)\n",
    "                # Generate output \n",
    "                for i in range(len(preds)):\n",
    "                    o_row = sentence[i][:]\n",
    "                    o_row.append(preds[i])\n",
    "                    output.append(o_row)\n",
    "                output.append([None,None,None])\n",
    "                # empty sentence and targets for the next sentence\n",
    "                sentence = []\n",
    "                targets = []\n",
    "        sentence.append(w_row)\n",
    "        if is_dev:\n",
    "            targets.append(target)\n",
    "\n",
    "    # Process the last complete sentece from input df        \n",
    "    if sentence:\n",
    "        sen_df = pd.DataFrame(sentence, columns =['s_idx', 'word_type'])\n",
    "        #print(sen_df)\n",
    "        # get predictions from viterbi\n",
    "        preds = np.array(viterbi(sen_df))\n",
    "        # compare preds and target, increment correct\n",
    "        if is_dev:\n",
    "            targets = np.array(targets)\n",
    "            correct += np.sum(preds == targets)\n",
    "        # Generate output\n",
    "        for i in range(len(preds)):\n",
    "            o_row = sentence[i][:]\n",
    "            o_row.append(preds[i])\n",
    "            output.append(o_row)\n",
    "#         output.append([None,None,None])\n",
    "    accuracy = round(correct/total*100, 3)\n",
    "    \n",
    "    return accuracy, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "98c6248f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Viterbi] Testing on Dev data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131768/131768 [01:41<00:00, 1293.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Dataset Accuracy: 95.338%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_df = pd.read_csv(\"data/dev\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])\n",
    "print(\"[Viterbi] Testing on Dev data...\")\n",
    "accuracy, output = viterbi_decoder(dev_df, False, True)\n",
    "print(\"Dev Dataset Accuracy: {}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac144cf",
   "metadata": {},
   "source": [
    "#### What is the accuracy on the dev data?\n",
    "<p> Threshold: 2, Viterbi accuracy on Dev Dataset: 95.226% </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99adf285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Viterbi] Generating predications on Test data viterbi.out...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129654/129654 [01:44<00:00, 1240.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Output a viterbi.out on test dataset\n",
    "test_df = pd.read_csv(\"data/test\", sep='\\t', names=[\"s_idx\", \"word_type\", \"pos\"])            \n",
    "print(\"[Viterbi] Generating predications on Test data viterbi.out...\")\n",
    "# Accuracy is 0 here, output contains predications for each sentence, separated by rows with None values\n",
    "accuracy, output = viterbi_decoder(test_df, True, False)\n",
    "\n",
    "# Store predications to greedy.out\n",
    "Predictions = np.array(output)\n",
    "greedy_output_df = pd.DataFrame(Predictions, columns = [\"s_idx\", \"word_type\", \"pos\"])\n",
    "greedy_output_df.to_csv('viterbi.out', sep='\\t', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
