{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16df99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re, json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.datasets as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from scipy import spatial\n",
    "\n",
    "# store train sentences \n",
    "train_file = 'data/train'\n",
    "dev_file = 'data/dev'\n",
    "test_file = 'data/test'\n",
    "dummy_file ='data/dummy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5472c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train/test file, each line as {s_idx, word, tag} tuple, store in a list\n",
    "def readFile(file):\n",
    "    f = open(file)\n",
    "    lines = f.readlines()\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            words.append(line.strip().split(' '))\n",
    "    return words\n",
    "\n",
    "# read files, group words by sentence, return list of sentences\n",
    "def readData(file):\n",
    "    f = open(file)\n",
    "    lines = f.readlines()\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            sentences.append(sentence.copy())\n",
    "            sentence.clear()\n",
    "        else:\n",
    "            sentence.append(line.strip().split(' '))\n",
    "    # append the last sentence\n",
    "    sentences.append(sentence.copy())\n",
    "    return sentences\n",
    "\n",
    "# word = [idx, word, tag]  train_data = list of sentences in term of list of words\n",
    "train_data = readData(train_file)\n",
    "\n",
    "dev_data = readData(dev_file)\n",
    "# word = [idx, word]\n",
    "test_data = readData(test_file)\n",
    "\n",
    "# Dummy test data\n",
    "dummy_file ='data/dummy'\n",
    "dummy_data = readData(dummy_file)\n",
    "\n",
    "# Preapare training data\n",
    "def processData(tuples):\n",
    "    training_data = []\n",
    "    for t in tuples:\n",
    "        training_data.append( ( [ word[1] if word[1] in word_to_idx else '<unk>' for word in t ], [ word[2] for word in t ] ) )\n",
    "    return training_data\n",
    "\n",
    "# Convert sequence into tensor\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def generateEvalFile1(model, input_data, file_name, word_to_idx):\n",
    "    # Reset the file\n",
    "    open(file_name, 'w').close()\n",
    "    f = open(file_name, \"a\")\n",
    "    \n",
    "    # model eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    for t in input_data:\n",
    "        sentence = [ word[1] if word[1] in word_to_idx else '<unk>' for word in t]\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(sentence, word_to_idx).to(device)\n",
    "            tag_scores = model(inputs) \n",
    "            preds = [idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()]\n",
    "            for word, pred in zip(t, preds):\n",
    "                f.write(f'{word[0]} {word[1]} {word[2]} {pred}\\n')\n",
    "            f.write('\\n')      \n",
    "    f.close()\n",
    "    \n",
    "def processTestData(tuples):\n",
    "    training_data = []\n",
    "    for t in tuples:\n",
    "        training_data.append( ( [ word[1] if word[1] in word_to_idx else '<unk>' for word in t ] ) )\n",
    "    return training_data\n",
    "\n",
    "def generateTestPred1(model, input_data, file_name):\n",
    "    # Reset the file\n",
    "    open(file_name, 'w').close()\n",
    "    f = open(file_name, \"a\")\n",
    "    \n",
    "    # model eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    for t in input_data:\n",
    "\n",
    "        sentence = [ word[1] if word[1] in word_to_idx else '<unk>' for word in t]\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(sentence, word_to_idx).to(device)\n",
    "            tag_scores = model(inputs) \n",
    "            preds = [idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()]\n",
    "            for word, pred in zip(t, preds):\n",
    "                f.write(f'{word[0]} {word[1]} {pred}\\n')\n",
    "            f.write('\\n')      \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF: index - s_idx - word - tag\n",
    "train_lines = readFile(train_file)\n",
    "df = pd.DataFrame(train_lines, columns = [\"s_idx\", \"word\", \"tag\"])\n",
    "\n",
    "# Randomly select some rare words to be <unk> words\n",
    "unique_words = df[\"word\"].value_counts().reset_index()\n",
    "unique_words.columns = [\"word\", \"freq\"]\n",
    "\n",
    "# Rare ward threshold\n",
    "threshold = 3\n",
    "\n",
    "# words with freq > threshold\n",
    "vocab_words = unique_words[ unique_words['freq'] > threshold ]\n",
    "# words with freq <= threshold\n",
    "rare_words = unique_words[ unique_words['freq'] <= threshold ]\n",
    "\n",
    "print(\"vocab words:\", vocab_words.shape[0])\n",
    "print(\"rare words:\", rare_words.shape[0])\n",
    "\n",
    "# Randomly select 3000 words from rare words to set as unknown words\n",
    "# unk_count = len(rare_words)\n",
    "# unk_words = rare_words.sample(unk_count)\n",
    "\n",
    "# drop the selected rare words from vocab\n",
    "# rare_words = rare_words.drop(unk_words.index)\n",
    "\n",
    "# build new vocab = freq_words + rest_rare_words + <unk>\n",
    "# vocab_words = vocab_words.append(rare_words, ignore_index=True)\n",
    "\n",
    "# custom words unk, pad etc\n",
    "custom_vocab = ['<unk>']\n",
    "# custom_vocab = ['<unk>', '<pad>']\n",
    "\n",
    "# main vocab list, to generate embedding\n",
    "vocab_set = set(custom_vocab + vocab_words['word'].unique().tolist())\n",
    "vocab_size = len(vocab_set)\n",
    "\n",
    "# all the vocab\n",
    "word_to_idx = {word:i for i, word in enumerate(vocab_set)}\n",
    "\n",
    "# all the unique tags\n",
    "unique_tags = set(df[\"tag\"].unique())\n",
    "tag_to_idx = {tag:i for i, tag in enumerate(unique_tags)}\n",
    "idx_to_tag = {i:tag for i, tag in enumerate(unique_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952d2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = processData(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef93632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "vocab_size = len(word_to_idx)\n",
    "tagset_size = len(tag_to_idx)\n",
    "\n",
    "lstm_layer = 1\n",
    "lstm_dropout = 0.33\n",
    "linear_out_dim = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "1b0967c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMTagger(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "#         super(LSTMTagger, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "        \n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "#         self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "#     # sentence [seq, batch, embed_dim]\n",
    "#     def forward(self, sentence):\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "#         lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "#         tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "#         return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61549b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, linear_out_dim, \n",
    "                 lstm_layer, lstm_dropout):\n",
    "        super(BLSTM, self).__init__()\n",
    "        # word embedding\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim,linear_out_dim)\n",
    "        self.fc = nn.Linear(linear_out_dim, tagset_size)\n",
    "        self.dropout = nn.Dropout(lstm_dropout)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        # Embedding layer + LSTM input dropout\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        embeds = self.dropout(embeds)\n",
    "        # BLSTM layer + LSTM output dropout\n",
    "        lstm_out, _ = self.bilstm(embeds.view(len(sentence), 1, -1))\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # Linear layer + elu\n",
    "        linear_out = F.elu(self.linear(lstm_out.view(len(sentence), -1)))\n",
    "        # classifier\n",
    "        tag_space = self.fc(linear_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc803e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "lr = 0.1\n",
    "epochs = 50\n",
    "print_every = 10\n",
    "\n",
    "# model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, tagset_size).to(device)\n",
    "model = BLSTM(embedding_dim, hidden_dim, vocab_size, tagset_size, linear_out_dim, lstm_layer, \n",
    "              lstm_dropout).to(device)\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebd71895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:36<00:00, 411.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Before training\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(training_data[0][0], word_to_idx).to(device)\n",
    "#     tag_scores = model(inputs)\n",
    "#     print([idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()])\n",
    "\n",
    "\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    for sentence, tags in tqdm(training_data, total=len(training_data)):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx).to(device)\n",
    "        targets = prepare_sequence(tags, tag_to_idx).to(device)\n",
    "        \n",
    "        tag_scores = model(sentence_in)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1)%print_every == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    \n",
    "# After training\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(training_data[0][0], word_to_idx).to(device)\n",
    "#     tag_scores = model(inputs)\n",
    "#     print([idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a39c2b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM(\n",
       "  (word_embeddings): Embedding(6183, 100)\n",
       "  (bilstm): LSTM(100, 256, bidirectional=True)\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blstm1_name = \"blstm1\"\n",
    "PATH = f\"{blstm1_name}.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(model, PATH)\n",
    "\n",
    "# Load\n",
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26a22468",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateEvalFile1(model, dev_data, \"dev1.out\",word_to_idx)\n",
    "\n",
    "generateTestPred1(model, test_data, \"test1.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9969e9a5",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f7b2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect glove.6B.100d.txt to be unzipped \n",
    "embeddings_dict = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "#     vector = torch.tensor(np.asarray(values[1:], dtype='float32'))\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_dict[word] = vector\n",
    "f.close()\n",
    "\n",
    "train_lines = readFile(train_file)\n",
    "df_train = pd.DataFrame(train_lines, columns = [\"s_idx\", \"word\", \"tag\"])\n",
    "\n",
    "dev_lines = readFile(dev_file)\n",
    "df_dev = pd.DataFrame(dev_lines, columns = [\"s_idx\", \"word\", \"tag\"])\n",
    "\n",
    "test_lines = readFile(test_file)\n",
    "df_test = pd.DataFrame(test_lines, columns = [\"s_idx\", \"word\"])\n",
    "\n",
    "combo_df = df_train.append(df_dev).append(df_test)\n",
    "\n",
    "# main vocab list, to generate embedding\n",
    "vocab_set = set( combo_df['word'].unique().tolist())\n",
    "vocab_size = len(vocab_set)\n",
    "\n",
    "# all the vocab\n",
    "word_to_idx = {word:i for i, word in enumerate(vocab_set)}\n",
    "\n",
    "# all the unique tags\n",
    "unique_tags = set(df_train[\"tag\"].unique())\n",
    "tag_to_idx = {tag:i for i, tag in enumerate(unique_tags)}\n",
    "idx_to_tag = {i:tag for i, tag in enumerate(unique_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "841be858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def findReplacement(word):\n",
    "    if word.lower() in embeddings_dict:\n",
    "        return word.lower()\n",
    "    else:\n",
    "        return \"unk\"\n",
    "    \n",
    "\n",
    "def processData2(tuples):\n",
    "    training_data = []\n",
    "    for t in tuples:\n",
    "        training_data.append( ( [ word[1] if word[1] in embeddings_dict else findReplacement(word[1]) for word in t ], [ word[2] for word in t ] ) )\n",
    "    return training_data\n",
    "\n",
    "def prepare_glove_sequence(seq, to_ix):\n",
    "    embeds = np.array([embeddings_dict[word] for word in seq])\n",
    "    return torch.tensor(embeds)\n",
    "\n",
    "def generateEvalFile2(model, input_data, file_name, word_to_idx):\n",
    "    # Reset the file\n",
    "    open(file_name, 'w').close()\n",
    "    f = open(file_name, \"a\")\n",
    "    \n",
    "    # model eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    for t in input_data:\n",
    "        sentence = [ word[1] if word[1] in embeddings_dict else findReplacement(word[1]) for word in t ]\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_glove_sequence(sentence, word_to_idx).to(device)\n",
    "            tag_scores = model(inputs) \n",
    "            preds = [idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()]\n",
    "            for word, pred in zip(t, preds):\n",
    "                f.write(f'{word[0]} {word[1]} {word[2]} {pred}\\n')\n",
    "            f.write('\\n')      \n",
    "    f.close()\n",
    "    \n",
    "def generateTestPred2(model, input_data, file_name):\n",
    "    # Reset the file\n",
    "    open(file_name, 'w').close()\n",
    "    f = open(file_name, \"a\")\n",
    "    \n",
    "    # model eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    for t in input_data:\n",
    "        sentence = [ word[1] if word[1] in embeddings_dict else findReplacement(word[1]) for word in t ]\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_glove_sequence(sentence, word_to_idx).to(device)\n",
    "            tag_scores = model(inputs) \n",
    "            preds = [idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()]\n",
    "            for word, pred in zip(t, preds):\n",
    "                f.write(f'{word[0]} {word[1]} {pred}\\n')\n",
    "            f.write('\\n')      \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7baff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM2(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, linear_out_dim, lstm_layer, lstm_dropout):\n",
    "        super(BLSTM2, self).__init__()\n",
    "        \n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim,linear_out_dim)\n",
    "        self.fc = nn.Linear(linear_out_dim, tagset_size)\n",
    "        self.dropout = nn.Dropout(lstm_dropout)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        # Sentence input is tensor\n",
    "        sentence = self.dropout(sentence)\n",
    "        # BLSTM layer + LSTM output dropout\n",
    "        lstm_out, _ = self.bilstm(sentence.view(len(sentence), 1, -1))\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # Linear layer + elu\n",
    "        linear_out = F.elu(self.linear(lstm_out.view(len(sentence), -1)))\n",
    "        # classifier\n",
    "        tag_space = self.fc(linear_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90466bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = processData2(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41cfcf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "tagset_size = len(unique_tags)\n",
    "\n",
    "lstm_layer = 1\n",
    "lstm_dropout = 0.33\n",
    "linear_out_dim = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Hyperparameter\n",
    "lr = 0.05\n",
    "epochs = 50\n",
    "print_every = 10\n",
    "\n",
    "blstm2 = BLSTM2( embedding_dim, hidden_dim, tagset_size, linear_out_dim, lstm_layer, lstm_dropout ).to(device)\n",
    "\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(blstm2.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94f3c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:35<00:00, 423.57it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for sentence, tags in tqdm(training_data, total=len(training_data)):\n",
    "        blstm2.zero_grad()\n",
    "        \n",
    "        sentence = prepare_glove_sequence(sentence, word_to_idx).to(device)\n",
    "        tags = prepare_sequence(tags, tag_to_idx).to(device)\n",
    "        \n",
    "        tag_scores = blstm2(sentence)\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1)%print_every == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9118a94a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM2(\n",
       "  (bilstm): LSTM(100, 256, bidirectional=True)\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blstm1_name = \"blstm2\"\n",
    "PATH = f\"{blstm1_name}.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(blstm2, PATH)\n",
    "\n",
    "# Load\n",
    "blstm2 = torch.load(PATH)\n",
    "blstm2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "18fa5c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateEvalFile2(blstm2, dev_data, \"dev2.out\", word_to_idx)\n",
    "\n",
    "generateTestPred2(blstm2, test_data, \"test2.out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
