{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16df99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re, json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.datasets as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab5472c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab words: 6182\n",
      "rare words: 17442\n"
     ]
    }
   ],
   "source": [
    "# store train sentences \n",
    "train_file = 'data/train'\n",
    "dev_file = 'data/dev'\n",
    "test_file = 'data/test'\n",
    "dummy_file ='data/dummy'\n",
    "\n",
    "# read train/test file, each line as {s_idx, word, tag} tuple, store in a list\n",
    "def readFile(file):\n",
    "    f = open(file)\n",
    "    lines = f.readlines()\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            words.append(line.strip().split(' '))\n",
    "    return words\n",
    "\n",
    "# DF: index - s_idx - word - tag\n",
    "train_lines = readFile(train_file)\n",
    "df = pd.DataFrame(train_lines, columns = [\"s_idx\", \"word\", \"tag\"])\n",
    "\n",
    "# Randomly select some rare words to be <unk> words\n",
    "unique_words = df[\"word\"].value_counts().reset_index()\n",
    "unique_words.columns = [\"word\", \"freq\"]\n",
    "\n",
    "# Rare ward threshold\n",
    "threshold = 3\n",
    "\n",
    "# words with freq > threshold\n",
    "vocab_words = unique_words[ unique_words['freq'] > threshold ]\n",
    "# words with freq <= threshold\n",
    "rare_words = unique_words[ unique_words['freq'] <= threshold ]\n",
    "\n",
    "print(\"vocab words:\", vocab_words.shape[0])\n",
    "print(\"rare words:\", rare_words.shape[0])\n",
    "\n",
    "# Randomly select 3000 words from rare words to set as unknown words\n",
    "# unk_count = len(rare_words)\n",
    "# unk_words = rare_words.sample(unk_count)\n",
    "\n",
    "# drop the selected rare words from vocab\n",
    "# rare_words = rare_words.drop(unk_words.index)\n",
    "\n",
    "# build new vocab = freq_words + rest_rare_words + <unk>\n",
    "# vocab_words = vocab_words.append(rare_words, ignore_index=True)\n",
    "\n",
    "# custom words unk, pad etc\n",
    "custom_vocab = ['<unk>']\n",
    "# custom_vocab = ['<unk>', '<pad>']\n",
    "\n",
    "# main vocab list, to generate embedding\n",
    "vocab_set = custom_vocab + vocab_words['word'].unique().tolist()\n",
    "vocab_size = len(vocab_set)\n",
    "\n",
    "# all the vocab\n",
    "word_to_idx = {word:i for i, word in enumerate(vocab_set)}\n",
    "\n",
    "# all the unique tags\n",
    "unique_tags = set(df[\"tag\"].unique())\n",
    "tag_to_idx = {tag:i for i, tag in enumerate(unique_tags)}\n",
    "idx_to_tag = {i:tag for i, tag in enumerate(unique_tags)}\n",
    "\n",
    "# read files, group words by sentence, return list of sentences\n",
    "def readData(file):\n",
    "    f = open(file)\n",
    "    lines = f.readlines()\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            sentences.append(sentence.copy())\n",
    "            sentence.clear()\n",
    "        else:\n",
    "            sentence.append(line.strip().split(' '))\n",
    "    # append the last sentence\n",
    "    sentences.append(sentence.copy())\n",
    "    return sentences\n",
    "\n",
    "# word = [idx, word, tag]  train_data = list of sentences in term of list of words\n",
    "train_data = readData(train_file)\n",
    "\n",
    "dev_data = readData(dev_file)\n",
    "# word = [idx, word]\n",
    "test_data = readData(test_file)\n",
    "\n",
    "# Dummy test data\n",
    "dummy_file ='data/dummy'\n",
    "dummy_data = readData(dummy_file)\n",
    "\n",
    "# Preapare training data\n",
    "def processData(tuples):\n",
    "    training_data = []\n",
    "    for t in tuples:\n",
    "        training_data.append( ( [ word[1] if word[1] in word_to_idx else '<unk>' for word in t ], [ word[2] for word in t ] ) )\n",
    "    return training_data\n",
    "\n",
    "# Convert sequence into tensor\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "def generateEvalFile1(model, input_data, file_name, word_to_idx):\n",
    "    # Reset the file\n",
    "    open(file_name, 'w').close()\n",
    "    f = open(file_name, \"a\")\n",
    "    \n",
    "    # model eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    for t in input_data:\n",
    "        sentence = [ word[1] if word[1] in word_to_idx else '<unk>' for word in t]\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(sentence, word_to_idx).to(device)\n",
    "            tag_scores = model(inputs) \n",
    "            preds = [idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()]\n",
    "            for word, pred in zip(t, preds):\n",
    "                f.write(f'{word[0]} {word[1]} {word[2]} {pred}\\n')\n",
    "            f.write('\\n')      \n",
    "    f.close()\n",
    "    \n",
    "def processTestData(tuples):\n",
    "    training_data = []\n",
    "    for t in tuples:\n",
    "        training_data.append( ( [ word[1] if word[1] in word_to_idx else '<unk>' for word in t ] ) )\n",
    "    return training_data\n",
    "\n",
    "def generateTestPred1(model, input_data, file_name):\n",
    "    # Reset the file\n",
    "    open(file_name, 'w').close()\n",
    "    f = open(file_name, \"a\")\n",
    "    \n",
    "    # model eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    for t in input_data:\n",
    "\n",
    "        sentence = [ word[1] if word[1] in word_to_idx else '<unk>' for word in t]\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_sequence(sentence, word_to_idx).to(device)\n",
    "            tag_scores = model(inputs) \n",
    "            preds = [idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()]\n",
    "            for word, pred in zip(t, preds):\n",
    "                f.write(f'{word[0]} {word[1]} {pred}\\n')\n",
    "            f.write('\\n')      \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "952d2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = processData(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef93632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "vocab_size = len(word_to_idx)\n",
    "tagset_size = len(tag_to_idx)\n",
    "\n",
    "lstm_layer = 1\n",
    "lstm_dropout = 0.33\n",
    "linear_out_dim = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "1b0967c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMTagger(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "#         super(LSTMTagger, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "        \n",
    "#         self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "#         self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "#     # sentence [seq, batch, embed_dim]\n",
    "#     def forward(self, sentence):\n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "#         lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "#         tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "#         return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61549b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, linear_out_dim, \n",
    "                 lstm_layer, lstm_dropout):\n",
    "        super(BLSTM, self).__init__()\n",
    "        # word embedding\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim,linear_out_dim)\n",
    "        self.fc = nn.Linear(linear_out_dim, tagset_size)\n",
    "        self.dropout = nn.Dropout(lstm_dropout)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        # Embedding layer + LSTM input dropout\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        embeds = self.dropout(embeds)\n",
    "        # BLSTM layer + LSTM output dropout\n",
    "        lstm_out, _ = self.bilstm(embeds.view(len(sentence), 1, -1))\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # Linear layer + elu\n",
    "        linear_out = F.elu(self.linear(lstm_out.view(len(sentence), -1)))\n",
    "        # classifier\n",
    "        tag_space = self.fc(linear_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d5c712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "lr = 0.05\n",
    "epochs = 100\n",
    "print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc803e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, tagset_size).to(device)\n",
    "model = BLSTM(embedding_dim, hidden_dim, vocab_size, tagset_size, linear_out_dim, lstm_layer, \n",
    "              lstm_dropout).to(device)\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d640086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I-LOC': 0,\n",
       " 'B-MISC': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-LOC': 3,\n",
       " 'I-MISC': 4,\n",
       " 'O': 5,\n",
       " 'B-ORG': 6,\n",
       " 'I-ORG': 7,\n",
       " 'B-PER': 8}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_file ='data/dummy'\n",
    "dummy_data = readData(dummy_file)\n",
    "dummy_data = processData(dummy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebd71895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:40<00:00, 368.94it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 393.03it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 408.38it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 395.13it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 398.70it/s]\n",
      "100%|██████████| 14987/14987 [00:43<00:00, 342.83it/s]\n",
      "100%|██████████| 14987/14987 [00:41<00:00, 361.54it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 391.90it/s]\n",
      "100%|██████████| 14987/14987 [00:39<00:00, 378.73it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 393.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1921e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:40<00:00, 370.48it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 400.10it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 398.05it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 397.98it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 403.75it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 405.01it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 397.01it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 400.27it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 400.07it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 413.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9605e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:37<00:00, 403.35it/s]\n",
      "100%|██████████| 14987/14987 [00:40<00:00, 370.70it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 385.33it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 401.13it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 406.33it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 408.06it/s]\n",
      "100%|██████████| 14987/14987 [00:39<00:00, 375.42it/s]\n",
      "100%|██████████| 14987/14987 [00:39<00:00, 383.95it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 398.13it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 405.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1921e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:37<00:00, 395.96it/s]\n",
      "100%|██████████| 14987/14987 [00:43<00:00, 348.10it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 392.46it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 392.43it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 412.21it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 402.30it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 406.43it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 406.21it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 411.32it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 386.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:37<00:00, 403.67it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 390.86it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 394.21it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 399.11it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 393.46it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 403.06it/s]\n",
      "100%|██████████| 14987/14987 [00:41<00:00, 362.68it/s]\n",
      "100%|██████████| 14987/14987 [00:42<00:00, 353.94it/s]\n",
      "100%|██████████| 14987/14987 [00:40<00:00, 372.44it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 389.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:37<00:00, 398.26it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 395.60it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 384.75it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 400.98it/s]\n",
      "100%|██████████| 14987/14987 [00:39<00:00, 376.90it/s]\n",
      "100%|██████████| 14987/14987 [00:40<00:00, 373.78it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 393.43it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 396.57it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 397.68it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 399.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:39<00:00, 377.98it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 387.85it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 393.29it/s]\n",
      "100%|██████████| 14987/14987 [00:39<00:00, 378.94it/s]\n",
      "100%|██████████| 14987/14987 [00:39<00:00, 375.68it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 392.96it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 390.59it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 407.10it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 405.02it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 391.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:37<00:00, 398.35it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 402.48it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 404.26it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 404.46it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 396.27it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 408.44it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 414.60it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 402.21it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 405.64it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 388.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:36<00:00, 410.70it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 414.04it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 402.49it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 403.51it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 396.18it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 412.88it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 404.78it/s]\n",
      "100%|██████████| 14987/14987 [00:36<00:00, 405.88it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 399.81it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 400.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:38<00:00, 391.56it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 398.72it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 399.56it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 391.65it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 384.45it/s]\n",
      "100%|██████████| 14987/14987 [00:37<00:00, 398.85it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 387.30it/s]\n",
      "100%|██████████| 14987/14987 [00:38<00:00, 386.20it/s]\n",
      "100%|██████████| 14987/14987 [00:39<00:00, 384.11it/s]\n",
      "100%|██████████| 14987/14987 [00:39<00:00, 379.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Before training\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(training_data[0][0], word_to_idx).to(device)\n",
    "#     tag_scores = model(inputs)\n",
    "#     print([idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()])\n",
    "\n",
    "\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    for sentence, tags in tqdm(training_data, total=len(training_data)):\n",
    "        model.zero_grad()\n",
    "        \n",
    "        sentence_in = prepare_sequence(sentence, word_to_idx).to(device)\n",
    "        targets = prepare_sequence(tags, tag_to_idx).to(device)\n",
    "        \n",
    "        tag_scores = model(sentence_in)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1)%print_every == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    \n",
    "# After training\n",
    "# with torch.no_grad():\n",
    "#     inputs = prepare_sequence(training_data[0][0], word_to_idx).to(device)\n",
    "#     tag_scores = model(inputs)\n",
    "#     print([idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a39c2b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BLSTM(\n",
       "  (word_embeddings): Embedding(6183, 100)\n",
       "  (bilstm): LSTM(100, 256, bidirectional=True)\n",
       "  (linear): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc): Linear(in_features=128, out_features=9, bias=True)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blstm1_name = \"blstm1\"\n",
    "PATH = f\"{blstm1_name}.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(model, PATH)\n",
    "\n",
    "# Load\n",
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "881c8abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateEvalFile1(model, dev_data, \"dev1.out\",word_to_idx)\n",
    "\n",
    "generateTestPred1(model, test_data, \"test1.out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88641620",
   "metadata": {},
   "source": [
    "# PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffcd887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect glove.6B.100d.txt to be unzipped \n",
    "embeddings_dict = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "#     vector = torch.tensor(np.asarray(values[1:], dtype='float32'))\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_dict[word] = vector\n",
    "f.close()\n",
    "\n",
    "train_lines = readFile(train_file)\n",
    "df_train = pd.DataFrame(train_lines, columns = [\"s_idx\", \"word\", \"tag\"])\n",
    "\n",
    "dev_lines = readFile(dev_file)\n",
    "df_dev = pd.DataFrame(dev_lines, columns = [\"s_idx\", \"word\", \"tag\"])\n",
    "\n",
    "test_lines = readFile(test_file)\n",
    "df_test = pd.DataFrame(test_lines, columns = [\"s_idx\", \"word\"])\n",
    "\n",
    "combo_df = df_train.append(df_dev).append(df_test)\n",
    "\n",
    "# main vocab list, to generate embedding\n",
    "vocab_set = set( vocab_words['word'].unique().tolist())\n",
    "vocab_size = len(vocab_set)\n",
    "\n",
    "# all the vocab\n",
    "word_to_idx = {word:i for i, word in enumerate(vocab_set)}\n",
    "\n",
    "# all the unique tags\n",
    "unique_tags = set(df_train[\"tag\"].unique())\n",
    "tag_to_idx = {tag:i for i, tag in enumerate(unique_tags)}\n",
    "idx_to_tag = {i:tag for i, tag in enumerate(unique_tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6751b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findReplacement(word):\n",
    "    if word.lower() in embeddings_dict:\n",
    "        return word.lower()\n",
    "    else:\n",
    "        return \"unk\"\n",
    "    \n",
    "\n",
    "def processData2(tuples):\n",
    "    training_data = []\n",
    "    for t in tuples:\n",
    "        training_data.append( ( [ word[1] if word[1] in embeddings_dict else findReplacement(word[1]) for word in t ], [ word[2] for word in t ] ) )\n",
    "    return training_data\n",
    "\n",
    "def prepare_glove_sequence(seq, to_ix):\n",
    "    embeds = [embeddings_dict[word] for word in seq]\n",
    "    return torch.tensor(embeds)\n",
    "\n",
    "def generateEvalFile2(model, input_data, file_name, word_to_idx):\n",
    "    # Reset the file\n",
    "    open(file_name, 'w').close()\n",
    "    f = open(file_name, \"a\")\n",
    "    \n",
    "    # model eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    for t in input_data:\n",
    "        sentence = [ word[1] if word[1] in embeddings_dict else findReplacement(word[1]) for word in t ]\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_glove_sequence(sentence, word_to_idx).to(device)\n",
    "            tag_scores = model(inputs) \n",
    "            preds = [idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()]\n",
    "            for word, pred in zip(t, preds):\n",
    "                f.write(f'{word[0]} {word[1]} {word[2]} {pred}\\n')\n",
    "            f.write('\\n')      \n",
    "    f.close()\n",
    "    \n",
    "def generateTestPred2(model, input_data, file_name):\n",
    "    # Reset the file\n",
    "    open(file_name, 'w').close()\n",
    "    f = open(file_name, \"a\")\n",
    "    \n",
    "    # model eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    for t in input_data:\n",
    "        sentence = [ word[1] if word[1] in embeddings_dict else findReplacement(word[1]) for word in t ]\n",
    "        with torch.no_grad():\n",
    "            inputs = prepare_glove_sequence(sentence, word_to_idx).to(device)\n",
    "            tag_scores = model(inputs) \n",
    "            preds = [idx_to_tag[i] for i in torch.argmax(tag_scores, dim=1).tolist()]\n",
    "            for word, pred in zip(t, preds):\n",
    "                f.write(f'{word[0]} {word[1]} {pred}\\n')\n",
    "            f.write('\\n')      \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e851304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BLSTM2(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, linear_out_dim, lstm_layer, lstm_dropout):\n",
    "        super(BLSTM2, self).__init__()\n",
    "        \n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=True)\n",
    "        self.linear = nn.Linear(2*hidden_dim,linear_out_dim)\n",
    "        self.fc = nn.Linear(linear_out_dim, tagset_size)\n",
    "        self.dropout = nn.Dropout(lstm_dropout)\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        # Sentence input is tensor\n",
    "        sentence = self.dropout(sentence)\n",
    "        # BLSTM layer + LSTM output dropout\n",
    "        lstm_out, _ = self.bilstm(sentence.view(len(sentence), 1, -1))\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # Linear layer + elu\n",
    "        linear_out = F.elu(self.linear(lstm_out.view(len(sentence), -1)))\n",
    "        # classifier\n",
    "        tag_space = self.fc(linear_out)\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10fc1986",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = processData2(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c40d4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "tagset_size = len(unique_tags)\n",
    "\n",
    "lstm_layer = 1\n",
    "lstm_dropout = 0.33\n",
    "linear_out_dim = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Hyperparameter\n",
    "lr = 0.05\n",
    "epochs = 50\n",
    "print_every = 10\n",
    "\n",
    "blstm2 = BLSTM2( embedding_dim, hidden_dim, tagset_size, linear_out_dim, lstm_layer, lstm_dropout ).to(device)\n",
    "\n",
    "loss_function = nn.NLLLoss().to(device)\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(blstm2.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59a82d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14987/14987 [00:39<00:00, 375.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for sentence, tags in tqdm(training_data, total=len(training_data)):\n",
    "        blstm2.zero_grad()\n",
    "        \n",
    "        sentence = prepare_glove_sequence(sentence, word_to_idx).to(device)\n",
    "        tags = prepare_sequence(tags, tag_to_idx).to(device)\n",
    "        \n",
    "        tag_scores = blstm2(sentence)\n",
    "        loss = loss_function(tag_scores, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch+1)%print_every == 0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blstm1_name = \"blstm2\"\n",
    "PATH = f\"{blstm1_name}.pt\"\n",
    "\n",
    "# Save\n",
    "torch.save(blstm2, PATH)\n",
    "\n",
    "# Load\n",
    "blstm2 = torch.load(PATH)\n",
    "blstm2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d50cf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generateEvalFile2(blstm2, dev_data, \"dev2.out\", word_to_idx)\n",
    "\n",
    "generateTestPred2(blstm2, test_data, \"test2.out\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
