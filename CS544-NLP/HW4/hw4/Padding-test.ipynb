{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750231ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re, json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch as t\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.datasets as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4f98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = 'data/train'\n",
    "dev_file = 'data/dev'\n",
    "test_file = 'data/test'\n",
    "dummy_file ='data/dummy'\n",
    "\n",
    "# read train/test file, each line as {s_idx, word, tag} tuple, store in a list\n",
    "def readFile(file):\n",
    "    f = open(file)\n",
    "    lines = f.readlines()\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            words.append(line.strip().split(' '))\n",
    "    return words\n",
    "\n",
    "# DF: index - s_idx - word - tag\n",
    "train_lines = readFile(train_file)\n",
    "df = pd.DataFrame(train_lines, columns = [\"s_idx\", \"word\", \"tag\"])\n",
    "\n",
    "# Randomly select some rare words to be <unk> words\n",
    "unique_words = df[\"word\"].value_counts().reset_index()\n",
    "unique_words.columns = [\"word\", \"freq\"]\n",
    "threshold = 3\n",
    "# words with freq > threshold\n",
    "vocab_words = unique_words[ unique_words['freq'] > threshold ]\n",
    "# words with freq <= threshold\n",
    "rare_words = unique_words[ unique_words['freq'] <= threshold ]\n",
    "\n",
    "# custom words unk, pad etc\n",
    "# custom_vocab = ['<unk>']\n",
    "custom_vocab = ['<unk>', '<pad>']\n",
    "\n",
    "# main vocab list, to generate embedding\n",
    "vocab_set = set(custom_vocab + vocab_words['word'].unique().tolist())\n",
    "vocab_size = len(vocab_set)\n",
    "\n",
    "# all the vocab\n",
    "word_to_idx = {word:i for i, word in enumerate(vocab_set)}\n",
    "\n",
    "# all the unique tags\n",
    "unique_tags = set(df[\"tag\"].unique().tolist())\n",
    "tag_to_idx = {tag:i for i, tag in enumerate(unique_tags)}\n",
    "idx_to_tag = {i:tag for i, tag in enumerate(unique_tags)}\n",
    "\n",
    "# read files, group words by sentence, return list of sentences\n",
    "def readData(file):\n",
    "    f = open(file)\n",
    "    lines = f.readlines()\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            sentences.append(sentence.copy())\n",
    "            sentence.clear()\n",
    "        else:\n",
    "            sentence.append(line.strip().split(' '))\n",
    "    # append the last sentence\n",
    "    sentences.append(sentence.copy())\n",
    "    return sentences\n",
    "\n",
    "# word = [idx, word, tag]  train_data = list of sentences in term of list of words\n",
    "train_data = readData(train_file)\n",
    "\n",
    "dev_data = readData(dev_file)\n",
    "# word = [idx, word]\n",
    "test_data = readData(test_file)\n",
    "\n",
    "# Dummy test data\n",
    "dummy_file ='data/dummy'\n",
    "dummy_data = readData(dummy_file)\n",
    "\n",
    "# Preapare training data\n",
    "def processData(tuples):\n",
    "    training_data = []\n",
    "    for t in tuples:\n",
    "        training_data.append( ( [ word[1] if word[1] in word_to_idx else '<unk>' for word in t ], [ word[2] for word in t ] ) )\n",
    "    return training_data\n",
    "\n",
    "# Convert sequence into tensor\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "# prepare padded data, return inputs and labels\n",
    "# def processPaddedData(tuples, max_seq_len):\n",
    "#     inputs = []\n",
    "#     labels = []\n",
    "#     PAD = '<pad>'\n",
    "#     for t in tuples:\n",
    "#         seq = [ word[1] if word[1] in word_to_idx else '<unk>' for word in t ]\n",
    "#         # pad seq\n",
    "#         if len(seq) < max_seq_len:\n",
    "#             seq += [ PAD for _ in range(max_seq_len-len(seq)) ]\n",
    "#         inputs.append(seq)\n",
    "#         labels.append( [ word[2] for word in t] )\n",
    "        \n",
    "#     return inputs, labels\n",
    "\n",
    "def processPaddedData(tuples, max_seq_len):\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    PAD = '<pad>'\n",
    "    for t in tuples:\n",
    "        seq = [ word[1] if word[1] in word_to_idx else '<unk>' for word in t ]\n",
    "        inputs.append(seq)\n",
    "        labels.append( [ word[2] for word in t] )\n",
    "        \n",
    "    return inputs, labels\n",
    "\n",
    "def seq2idx(inputs, to_ix):\n",
    "    return [ torch.tensor([to_ix[w] for w in seq]) for seq in inputs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a2fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs, labels = processPaddedData(dummy_data, word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d12d2014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding_idx 4968\n",
      "tensor([[2446, 5046, 5881],\n",
      "        [5937, 4968, 4968],\n",
      "        [1720, 1720, 4968]]) tensor([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = pad_sequence(seq2idx(seqs, word_to_idx), batch_first=True, padding_value=padding_idx)\n",
    "lengths = t.tensor([len(_) for _ in seqs])\n",
    "padding_idx = word_to_idx['<pad>']\n",
    "print(\"padding_idx\", padding_idx)\n",
    "print(a, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "389e7b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2446, 5046, 5881],\n",
       "        [1720, 1720, 4968],\n",
       "        [5937, 4968, 4968]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort seqs by length\n",
    "a_lengths, idx = lengths.sort(0, descending=True)\n",
    "_, un_idx = t.sort(idx, dim=0)\n",
    "a = a[idx]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "283722d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, 100, padding_idx=word_to_idx['<pad>'])\n",
    "lstm = nn.LSTM(100, 128, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56df4963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 100])\n",
      "torch.Size([3, 3, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0019, -0.0315, -0.0702,  ..., -0.0456, -0.1659, -0.1094],\n",
       "         [ 0.0301,  0.1060,  0.1276,  ..., -0.0611, -0.0634,  0.0340],\n",
       "         [ 0.1644, -0.2324, -0.2320,  ...,  0.0900, -0.0094,  0.1267]],\n",
       "\n",
       "        [[-0.1318, -0.1016, -0.0932,  ...,  0.1361, -0.0183, -0.0684],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.0458,  0.0207,  0.0781,  ..., -0.1244, -0.0597,  0.1081],\n",
       "         [ 0.0310,  0.1713,  0.2261,  ..., -0.0700, -0.1128,  0.0323],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "       grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_input = embedding(a)\n",
    "print(a_input.shape)\n",
    "\n",
    "a_packed_input = t.nn.utils.rnn.pack_padded_sequence(input=a_input, lengths=a_lengths, batch_first=True)\n",
    "packed_out, _ = lstm(a_packed_input)\n",
    "out, _ = pad_packed_sequence(packed_out)\n",
    "#original order\n",
    "out = t.index_select(out, 0, un_idx)\n",
    "\n",
    "# out, _ = lstm(a_input)\n",
    "print(out.shape)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc8245d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0947, -0.0699, -0.0393, -0.0997,  0.1415,  0.0559, -0.1292,\n",
       "          -0.0003,  0.0490],\n",
       "         [ 0.1884, -0.0755, -0.0905, -0.1593,  0.1813, -0.0312, -0.0448,\n",
       "           0.0479,  0.0121],\n",
       "         [ 0.1885, -0.1232,  0.0196, -0.0502,  0.0886, -0.0023, -0.0610,\n",
       "           0.0203,  0.0592]],\n",
       "\n",
       "        [[ 0.1491, -0.0374, -0.1163, -0.0381,  0.0950, -0.0411, -0.1155,\n",
       "           0.0087, -0.0763],\n",
       "         [ 0.0823, -0.0788, -0.0048, -0.0126,  0.0610,  0.0547, -0.0855,\n",
       "           0.0088,  0.0745],\n",
       "         [ 0.0823, -0.0788, -0.0048, -0.0126,  0.0610,  0.0547, -0.0855,\n",
       "           0.0088,  0.0745]],\n",
       "\n",
       "        [[ 0.0356, -0.0730,  0.0846, -0.0952,  0.1377,  0.0878, -0.0780,\n",
       "          -0.0179,  0.0475],\n",
       "         [ 0.2325, -0.0737, -0.1263, -0.2308,  0.2237, -0.0728, -0.0297,\n",
       "           0.0686, -0.0182],\n",
       "         [ 0.0823, -0.0788, -0.0048, -0.0126,  0.0610,  0.0547, -0.0855,\n",
       "           0.0088,  0.0745]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = nn.Linear(128, 9)\n",
    "fc_out = fc(out)\n",
    "fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "467d1097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 9])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "765ed00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for l in lengths:\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
