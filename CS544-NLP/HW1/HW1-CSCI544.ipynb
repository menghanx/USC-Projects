{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xmh91\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"amazon_reviews_us_Kitchen_v1_00.tsv.gz\", sep='\\t', error_bad_lines=False, warn_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of three classes, (positive, star=3, negative) :\n",
      "3856492,349547,668848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_review_count = len(raw_df[ raw_df['star_rating'] > 3 ])\n",
    "mid_review_count= len(raw_df[ raw_df['star_rating'] == 3 ])\n",
    "neg_review_count = len(raw_df[ raw_df['star_rating'] < 3 ])\n",
    "\n",
    "print(f\"\"\"\n",
    "Statistics of three classes, (positive, star=3, negative) :\n",
    "{pos_review_count},{mid_review_count},{neg_review_count}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "#pd.value_counts(df['star_rating']).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df[['star_rating', 'review_body']].dropna(subset=['review_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Reviews:\n",
    "## The reviews with rating 4,5 are labelled to be 1 and 1,2 are labelled as 0. Discard the reviews with rating 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop rating 3 rows\n",
    "three_indexes = df[ df['star_rating'] == 3 ].index\n",
    "df.drop(three_indexes, inplace = True)\n",
    "# Lebal samples according to rating\n",
    "df.loc[df['star_rating'] <= 2, 'star_rating'] = 0\n",
    "df.loc[df['star_rating'] >= 4, 'star_rating'] = 1\n",
    "df.columns = ['label', 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 200000 reviews randomly with 100,000 positive and 100,000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select reviews\n",
    "df_pos = df.loc[df['label'] == 1].sample(100000, random_state=1)\n",
    "df_neg = df.loc[df['label'] == 0].sample(100000, random_state=2)\n",
    "\n",
    "# concatenate both positive and negative review df together for cleaning and preprocessing\n",
    "df_data = df_pos.append(df_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report samples without cleaning & preprocessing:\n",
      "\t1. Very Nice!!  Exactly as described!!!!!\n",
      "\t2. Easy to use.  Ice cream in 20 minutes, couldn't be better.\n",
      "\t3. Even though this product works great, the edge can break or crack easily. The edge is where the glass doubles back to create the space between the two walls. When you have any liquids in the cup, all that weight is being cantilevered from the outer wall to the inner wall, i.e. suspended. So, if you happen to bump or shake the cup while it is full, that puts a lot of stress on that edge. I've had 1 cup chip. I also bought the water jug but that one just broke when I bumped it against the sink after filling it. Yea, the worst possible scenario. So, if you are willing to pay for the great looks and functionality buy this but do expect that they won't last as other cups that cost less.<br /><br />The double wall really does insulate the contents, hot or cold. So, your contents last longer and your hand does not get hot or cold. Great for ice cream and tea. Their tea pot is also very good. I haven't broken that one......yet.\n"
     ]
    }
   ],
   "source": [
    "# sample 3 reviews\n",
    "print(\"Report samples without cleaning & preprocessing:\")\n",
    "df_3_sample = df_data.sample(3, random_state=6)\n",
    "sample_counter=1\n",
    "for rev in df_3_sample[\"review\"]:\n",
    "    print(f\"\\t{sample_counter}. {rev}\")\n",
    "    sample_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average review length prior to data cleaning \n",
    "\n",
    "len_before_clean = df_data[\"review\"].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['review'] = df_data['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 samples\n",
    "df_3_sample['review'] = df_3_sample['review'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_url(s):\n",
    "    # parse html\n",
    "    soup = BeautifulSoup(s, \"html.parser\")\n",
    "    \n",
    "    for data in soup(['style', 'script']):\n",
    "        # remove tags\n",
    "        data.decompose()\n",
    "    # replace url with empty string and return\n",
    "    return re.sub(r\"http\\S+\", \"\", ' '.join(soup.stripped_strings))\n",
    "\n",
    "# Remove HTML markups and URL in text format\n",
    "df_data['review'] = [ remove_html_url(review) for review in df_data['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 samples\n",
    "df_3_sample['review'] = [ remove_html_url(review) for review in df_3_sample['review'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contractions\n",
    "### **unable to isntall pycontraction, nor find any workaround library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "        \"a'ight\": \"alright\",\n",
    "        \"ain't\": \"am not\",\n",
    "        \"amn't\": \"am not\",\n",
    "        \"arencha\": \"are not you\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"‘bout\": \"about\",\n",
    "        \"cannot\": \"can not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"cap’n\": \"captain\",\n",
    "        \"cause\": \"because\",\n",
    "        \"’cept\": \"except\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"dammit\": \"damn it\",\n",
    "        \"daren't\": \"dare not\",\n",
    "        \"daresn't\": \"dare not\",\n",
    "        \"dasn't\": \"dare not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"dunno\": \"do not know\",\n",
    "        \"d'ye\": \"did you\",\n",
    "        \"e'en\": \"even\",\n",
    "        \"e'er\": \"ever\",\n",
    "        \"em\": \"them\",\n",
    "        \"everybody's\": \"everybody is\",\n",
    "        \"everyone's\": \"everyone is\",\n",
    "        \"fo’c’sle\": \"forecastle\",\n",
    "        \"’gainst\": \"against\",\n",
    "        \"g'day\": \"good day\",\n",
    "        \"gimme\": \"give me\",\n",
    "        \"giv'n\": \"given\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"gon't\": \"go not\",\n",
    "        \"gotta\": \"got to\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"had've\": \"had have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"helluva\": \"hell of a\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"here's\": \"here is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"howdy\": \"how do you do\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how're\": \"how are\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'd've\": \"i would have\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"imma\": \"i am about to\",\n",
    "        \"i'm'o\": \"i am going to\",\n",
    "        \"innit\": \"is it not\",\n",
    "        \"ion\": \"i do not\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"iunno\": \"i do not know\",\n",
    "        \"kinda\": \"kind of\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"may've\": \"may have\",\n",
    "        \"methinks\": \"i think\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"‘neath\": \"beneath\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"nal\": \"and all\",\n",
    "        \"ne'er\": \"never\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"o'er\": \"over\",\n",
    "        \"ol'\": \"old\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"‘round\": \"around\",\n",
    "        \"s\": \"is\",\n",
    "        \"shalln't\": \"shall not\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"somebody's\": \"somebody is\",\n",
    "        \"someone's\": \"someone is\",\n",
    "        \"something's\": \"something is\",\n",
    "        \"so're\": \"so are\",\n",
    "        \"so’s\": \"so has\",\n",
    "        \"so’ve\": \"so have\",\n",
    "        \"that'll\": \"that will\",\n",
    "        \"that're\": \"that are\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'll\": \"there will\",\n",
    "        \"there're\": \"there are\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"these're\": \"these are\",\n",
    "        \"these've\": \"these have\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"this's\": \"this is\",\n",
    "        \"those're\": \"those are\",\n",
    "        \"those've\": \"those have\",\n",
    "        \"thout\": \"without\",\n",
    "        \"’til\": \"until\",\n",
    "        \"tis\": \"it is\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"twas\": \"it was\",\n",
    "        \"tween\": \"between\",\n",
    "        \"twere\": \"it were\",\n",
    "        \"wanna\": \"want to\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"whatcha\": \"what are you\",\n",
    "        \"what'd\": \"what did\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what're\": \"what are/what were\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where'll\": \"where will\",\n",
    "        \"where're\": \"where are\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"which'd\": \"which had\",\n",
    "        \"which'll\": \"which will\",\n",
    "        \"which're\": \"which are\",\n",
    "        \"which's\": \"which is\",\n",
    "        \"which've\": \"which have\",\n",
    "        \"who'd\": \"who would\",\n",
    "        \"who'd've\": \"who would have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who're\": \"who are\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why'd\": \"why did\",\n",
    "        \"why're\": \"why are\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"willn't\": \"will not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wonnot\": \"will not\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all'd'n't've\": \"you all would not have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all'ren't\": \"you all are not\",\n",
    "        \"y'at\": \"you at\",\n",
    "        \"yes’m\": \"yes madam\",\n",
    "        \"yessir\": \"yes sir\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"yrs\": \"years\",\n",
    "        \"ur\" : \"your\",\n",
    "        \"urs\" : \"yours\"\n",
    "}\n",
    "    \n",
    "def contractionfunction(s): \n",
    "    for word in s.split(\" \"):\n",
    "        if word in contractions:\n",
    "            s = s.replace(word, contractions[word])\n",
    "    return s\n",
    "\n",
    "df_data['review'] = [ contractionfunction(review) for review in df_data['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 samples\n",
    "df_3_sample['review'] = [ contractionfunction(review) for review in df_3_sample['review'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetical(s):\n",
    "    # remove single quote in word like \" husband's \"\n",
    "    s = re.sub(r'\\'', '', s)\n",
    "    \n",
    "    # replace non-alphabetical by whitespace\n",
    "    s = re.sub(r\"[^a-zA-Z]\", ' ', s)\n",
    "    \n",
    "    # remove punctuation and return\n",
    "    #return ' '.join([word.strip(string.punctuation) for word in s.split(\" \")])\n",
    "    return s\n",
    "\n",
    "df_data['review'] = [ remove_non_alphabetical(review) for review in df_data['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 samples\n",
    "df_3_sample['review'] = [ remove_non_alphabetical(review) for review in df_3_sample['review'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the extra spaces between the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['review'] = [ re.sub(r'\\s+', ' ', review) for review in df_data['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 samples\n",
    "df_3_sample['review'] = [ re.sub(r'\\s+', ' ', review) for review in df_3_sample['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before and after data cleaning:\n",
      "324.359335,310.915755\n"
     ]
    }
   ],
   "source": [
    "# Average review length after data cleaning \n",
    "\n",
    "len_after_clean = df_data[\"review\"].apply(len).mean()\n",
    "print(f\"\"\"Average length of reviews before and after data cleaning:\n",
    "{len_before_clean},{len_after_clean}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(s):\n",
    "    # only keep words not in stopwords set\n",
    "    filtered_words = [word for word in s.split(\" \") if word not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df_data['review'] = [ remove_stopwords(review) for review in df_data['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 samples\n",
    "df_3_sample['review'] = [ remove_stopwords(review) for review in df_3_sample['review'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# simple lemmatize\n",
    "def easy_lemmatize(s):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in s.split(\" \")]\n",
    "    return \" \".join(lemmatized_words)\n",
    "    \n",
    "# use simple lemmatize\n",
    "df_data['review'] = [ easy_lemmatize(review) for review in df_data['review'] ]\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 samples\n",
    "df_3_sample['review'] = [ easy_lemmatize(review) for review in df_3_sample['review'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of reviews before and after data preprocessing:\n",
      "310.915755, 191.158705\n"
     ]
    }
   ],
   "source": [
    "# Average review length after data cleaning & preprocessing\n",
    "\n",
    "len_after_prep = df_data[\"review\"].apply(len).mean()\n",
    "print(f\"\"\"Average length of reviews before and after data preprocessing:\n",
    "{len_after_clean}, {len_after_prep}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three sample reviews after cleaning & preprocessing:\n",
      "\t1. nice exactly described \n",
      "\t2. easy use ice cream minute could better \n",
      "\t3. even though product work great edge break crack easily edge glass double back create space two wall liquid cup weight cantilevered outer wall inner wall e suspended happen bump shake cup full put lot stress edge cup chip also bought water jug one broke bumped sink filling yea worst possible scenario willing pay great look functionality buy expect last cup cost le double wall really insulate content hot cold content last longer hand get hot cold great ice cream tea tea pot also good broken one yet \n"
     ]
    }
   ],
   "source": [
    "print(\"Three sample reviews after cleaning & preprocessing:\")\n",
    "sample_counter=1\n",
    "for rev in df_3_sample[\"review\"]:\n",
    "    print(f\"\\t{sample_counter}. {rev}\")\n",
    "    sample_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split the data to train and test dataset\n",
    "df_x = df_data['review']\n",
    "df_y = df_data['label']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, random_state = 3, test_size = 0.2)\n",
    "\n",
    "# get tf-idf vectors from training data\n",
    "vectorizer = TfidfVectorizer()\n",
    "x_train_vec = vectorizer.fit_transform(x_train)\n",
    "\n",
    "y_train=y_train.astype('int')\n",
    "\n",
    "# transform test dataset based on train dataset tf-idf vectors\n",
    "x_test_vec = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report classification metrics based on targets and predictions\n",
    "def report_metrics(targets, predictions):\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(targets, predictions)\n",
    "    precision = metrics.precision_score(targets, predictions)\n",
    "    recall = metrics.recall_score(targets, predictions)\n",
    "    f1_score = metrics.f1_score(targets, predictions)\n",
    "    \n",
    "    return f\"{accuracy},{precision},{recall},{f1_score}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Precision, Recall, and f1-score for training and testing split for Perceptron:\n",
      "0.89514375,0.8751737658171449,0.9215438508695108,0.8977604436454494,0.8549,0.8358918817103033,0.8844544095665172,0.8594877257541278\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# set iteration and learning rate\n",
    "max_iter = 1000\n",
    "eta0 = 0.1\n",
    "\n",
    "# Train the model with train dataset and train targets\n",
    "ppn_clf = Perceptron(max_iter=max_iter, eta0=eta0, random_state=1)\n",
    "ppn_clf.fit(x_train_vec, y_train)\n",
    "\n",
    "# predict on train dataset\n",
    "y_pred = ppn_clf.predict(x_train_vec)\n",
    "\n",
    "# Classification Report - Train\n",
    "output = report_metrics(y_train, y_pred)+\",\"\n",
    "\n",
    "# predict on test dataset\n",
    "y_pred = ppn_clf.predict(x_test_vec)\n",
    "\n",
    "# Classification Report - Test\n",
    "output += report_metrics(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy, Precision, Recall, and f1-score for training and testing split for Perceptron:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Precision, Recall, and f1-score for training and testing split for SVM:\n",
      "0.9324,0.9334688040942274,0.9310396597022395,0.93225264951269,0.8939,0.894899690587883,0.8934728450423518,0.894185698613743\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train the model with train dataset and train targets\n",
    "svm = LinearSVC(random_state=0, tol=1e-5)\n",
    "svm.fit(x_train_vec, y_train)\n",
    "\n",
    "# predict on train dataset\n",
    "y_pred = svm.predict(x_train_vec)\n",
    "\n",
    "# Classification Report - Train\n",
    "output = report_metrics(y_train, y_pred)+\",\"\n",
    "\n",
    "# predict on test dataset\n",
    "y_pred = svm.predict(x_test_vec)\n",
    "\n",
    "# Classification Report - Test\n",
    "output += report_metrics(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy, Precision, Recall, and f1-score for training and testing split for SVM:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Precision, Recall, and f1-score for training and testing split for Logistic Regression:\n",
      "0.91286875,0.9161820910960027,0.9087201301138497,0.9124358547569547,0.896475,0.8996437352601736,0.8933233682112606,0.8964724118102952\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the model with train dataset and train targets\n",
    "lr_clp = LogisticRegression(random_state=0, max_iter=500)\n",
    "lr_clp.fit(x_train_vec, y_train)\n",
    "\n",
    "# predict on train dataset\n",
    "y_pred = lr_clp.predict(x_train_vec)\n",
    "\n",
    "# Classification Report - Train\n",
    "output = report_metrics(y_train, y_pred)+\",\"\n",
    "\n",
    "# predict on test dataset\n",
    "y_pred = lr_clp.predict(x_test_vec)\n",
    "\n",
    "# Classification Report - Test\n",
    "output += report_metrics(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy, Precision, Recall, and f1-score for training and testing split for Logistic Regression:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, Precision, Recall, and f1-score for training and testing split for Naive Bayes:\n",
      "0.88603125,0.8907566218664099,0.8797572876266734,0.8852227880130671,0.8674,0.8739363857374393,0.8597409068261086,0.8667805294619984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train the model with train dataset and train targets\n",
    "naive_bayes_clf = MultinomialNB()\n",
    "naive_bayes_clf.fit(x_train_vec, y_train)\n",
    "\n",
    "# predict on train dataset\n",
    "y_pred = naive_bayes_clf.predict(x_train_vec)\n",
    "\n",
    "# Classification Report - Train\n",
    "output = report_metrics(y_train, y_pred)+\",\"\n",
    "\n",
    "# predict on test dataset\n",
    "y_pred = naive_bayes_clf.predict(x_test_vec)\n",
    "\n",
    "# Classification Report - Test\n",
    "output += report_metrics(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy, Precision, Recall, and f1-score for training and testing split for Naive Bayes:\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
