{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a87cb252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xmh91\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4ade12ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>good but https://www.amazon.com/gp/product/B07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>not good, i needed to solve a problem cost!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                        review_body\n",
       "0     5.0  good but https://www.amazon.com/gp/product/B07...\n",
       "1     2.0        not good, i needed to solve a problem cost!"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dummy.tsv\", sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "df['review_body'] = [BeautifulSoup(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', review, flags=re.MULTILINE)).get_text() for review in df['review_body'] ]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "04e07050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Search Images Maps Play YouTube News Gmail Drive More » Web History | Settings | Sign in Advanced search Advertising Programs Business Solutions About Google © 2021 - Privacy - Terms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "result = requests.get(\"https://www.google.com\")\n",
    "src = result.content\n",
    "\n",
    "def remove_url_html(html):\n",
    "  \n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "  \n",
    "    for data in soup(['style', 'script']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "  \n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "  \n",
    "  \n",
    "# Print the extracted data\n",
    "print(remove_tags(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "63e94762",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7060122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data frame:\n",
      "\n",
      "   rating                       review_body\n",
      "0       1              user@web.com student\n",
      "1       2     update 11/11/1991, I was born\n",
      "2       3            mr. grapes are 2bad...\n",
      "3       4           mr. grapes are 2bad...\"\n",
      "4       5  this is very small...it's rubber\n",
      "\n",
      "Updated Data frame:\n",
      "\n",
      "   rating                       review_body\n",
      "0       1              user web com student\n",
      "1       2     update             I was born\n",
      "2       3            mr  grapes are  bad   \n",
      "3       4           mr  grapes are  bad    \n",
      "4       5  this is very small   it's rubber\n"
     ]
    }
   ],
   "source": [
    "info= {\"rating\":[1,2,3,4,5], \"review_body\":[\"user@web.com student\",\n",
    "                                       \"update 11/11/1991, I was born\",\n",
    "                                       \"mr. grapes are 2bad...\",\n",
    "                                       'mr. grapes are 2bad...\"',\n",
    "                                        \"this is very small...it's rubber\"]}\n",
    " \n",
    "data = pd.DataFrame(info)\n",
    "print(\"Original Data frame:\\n\")\n",
    "print(data)\n",
    "\n",
    "def remove_non_alphabetical(s):\n",
    "    # remove numbers\n",
    "    #s = re.sub(r'\\d+', '', s)\n",
    "    \n",
    "    # replace non-alphabetical by whitespace\n",
    "    s = re.sub(r\"[^a-zA-Z'’]\", ' ', s)\n",
    "    \n",
    "    # remove punctuation and return\n",
    "    #return ' '.join([word.strip(string.punctuation) for word in s.split(\" \")])\n",
    "    return s\n",
    "                     \n",
    "data[\"review_body\"] = [ remove_non_alphabetical(review) for review in data[\"review_body\"]]\n",
    "print(\"\\nUpdated Data frame:\\n\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d5ad1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "11767f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defective  new design safety mechanisms  at ba\n"
     ]
    }
   ],
   "source": [
    " def contractionfunction(s):\n",
    "\n",
    "    contractions = {\n",
    "        \"a'ight\": \"alright\",\n",
    "        \"ain't\": \"am not\",\n",
    "        \"amn't\": \"am not\",\n",
    "        \"arencha\": \"are not you\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"‘bout\": \"about\",\n",
    "        \"cannot\": \"can not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"cap’n\": \"captain\",\n",
    "        \"cause\": \"because\",\n",
    "        \"’cept\": \"except\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"dammit\": \"damn it\",\n",
    "        \"daren't\": \"dare not\",\n",
    "        \"daresn't\": \"dare not\",\n",
    "        \"dasn't\": \"dare not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"dunno\": \"do not know\",\n",
    "        \"d'ye\": \"did you\",\n",
    "        \"e'en\": \"even\",\n",
    "        \"e'er\": \"ever\",\n",
    "        \"em\": \"them\",\n",
    "        \"everybody's\": \"everybody is\",\n",
    "        \"everyone's\": \"everyone is\",\n",
    "        \"fo’c’sle\": \"forecastle\",\n",
    "        \"’gainst\": \"against\",\n",
    "        \"g'day\": \"good day\",\n",
    "        \"gimme\": \"give me\",\n",
    "        \"giv'n\": \"given\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"gon't\": \"go not\",\n",
    "        \"gotta\": \"got to\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"had've\": \"had have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"helluva\": \"hell of a\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"here's\": \"here is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"howdy\": \"how do you do\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how're\": \"how are\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'd've\": \"i would have\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"imma\": \"i am about to\",\n",
    "        \"i'm'o\": \"i am going to\",\n",
    "        \"innit\": \"is it not\",\n",
    "        \"ion\": \"i do not\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"iunno\": \"i do not know\",\n",
    "        \"kinda\": \"kind of\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"may've\": \"may have\",\n",
    "        \"methinks\": \"i think\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"‘neath\": \"beneath\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"nal\": \"and all\",\n",
    "        \"ne'er\": \"never\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"o'er\": \"over\",\n",
    "        \"ol'\": \"old\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"‘round\": \"around\",\n",
    "        \"s\": \"is\",\n",
    "        \"shalln't\": \"shall not\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"somebody's\": \"somebody is\",\n",
    "        \"someone's\": \"someone is\",\n",
    "        \"something's\": \"something is\",\n",
    "        \"so're\": \"so are\",\n",
    "        \"so’s\": \"so has\",\n",
    "        \"so’ve\": \"so have\",\n",
    "        \"that'll\": \"that will\",\n",
    "        \"that're\": \"that are\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'll\": \"there will\",\n",
    "        \"there're\": \"there are\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"these're\": \"these are\",\n",
    "        \"these've\": \"these have\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"this's\": \"this is\",\n",
    "        \"those're\": \"those are\",\n",
    "        \"those've\": \"those have\",\n",
    "        \"thout\": \"without\",\n",
    "        \"’til\": \"until\",\n",
    "        \"tis\": \"it is\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"twas\": \"it was\",\n",
    "        \"tween\": \"between\",\n",
    "        \"twere\": \"it were\",\n",
    "        \"wanna\": \"want to\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"whatcha\": \"what are you\",\n",
    "        \"what'd\": \"what did\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what're\": \"what are/what were\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where'll\": \"where will\",\n",
    "        \"where're\": \"where are\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"which'd\": \"which had\",\n",
    "        \"which'll\": \"which will\",\n",
    "        \"which're\": \"which are\",\n",
    "        \"which's\": \"which is\",\n",
    "        \"which've\": \"which have\",\n",
    "        \"who'd\": \"who would\",\n",
    "        \"who'd've\": \"who would have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who're\": \"who are\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why'd\": \"why did\",\n",
    "        \"why're\": \"why are\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"willn't\": \"will not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wonnot\": \"will not\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all'd'n't've\": \"you all would not have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all'ren't\": \"you all are not\",\n",
    "        \"y'at\": \"you at\",\n",
    "        \"yes’m\": \"yes madam\",\n",
    "        \"yessir\": \"yes sir\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for word in s.split(\" \"):\n",
    "        if word in contractions:\n",
    "            s = s.replace(word, contractions[word])\n",
    "    return s\n",
    "\n",
    "text = \"Hey I'm Yann, how're you and how's it going ? That's interesting : I'd love to hear more about it.\".lower()\n",
    "text = \"defective  new design safety mechanisms  at ba\".lower()\n",
    "#text = ' '.join([word.strip(string.punctuation) for word in text.split(\" \")])\n",
    "#text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "res = contractionfunction(text)\n",
    "print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "98dec3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it works fine but it is definitely very fragile\n"
     ]
    }
   ],
   "source": [
    "## Remove white spaces\n",
    "\n",
    "line1 = '  it works     fine but it is definitely very fragile'\n",
    "line2 = '     not big enough capacity,it should be doubled'\n",
    "\n",
    "line1 = re.sub(' +', ' ', line1)\n",
    "print(line1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "64b959ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My link text\n"
     ]
    }
   ],
   "source": [
    "## Remove html and url\n",
    "\n",
    "info= {\"rating\":[1,2,3], \"review_body\":[\"i needed to solve a problem, cost!<br />this didn't work for that reason.  at my local big box store i can pay an average price of 32 cents for each k cup filled with coffee.  here i have to do the labor.<br />the product works but no money savings.....bummer.\",\n",
    "                                      \"website is https://www.youtube.com/watch?v=_4kHxtiuML0\",\n",
    "                                     \"not secure http://www.abc.com\"]}\n",
    " \n",
    "data = pd.DataFrame(info)\n",
    "#print(\"Original Data frame:\\n\")\n",
    "#print(data)\n",
    "\n",
    "\n",
    "\n",
    "def remove_html_url(s):\n",
    "    # parse html\n",
    "    soup = BeautifulSoup(s, \"html.parser\")\n",
    "    \n",
    "    for data in soup(['style', 'script']):\n",
    "        # remove tags\n",
    "        data.decompose()\n",
    "    # replace url with empty string and return\n",
    "    return re.sub(r\"http\\S+\", \"\", ' '.join(soup.stripped_strings))\n",
    "\n",
    "text = '<html><body><a href=\"http://moo\">My link text</a></body></html>'\n",
    "print(remove_html_url(text))\n",
    "\n",
    "data[\"review_body\"] = [remove_html_url(wat) for wat in data[\"review_body\"]]\n",
    "\n",
    "#print(\"\\nUpdated Data frame:\\n\")\n",
    "#print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "7d295d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nordic ware microwave plate cover look good online generally high rating go ahead order one arrive promptly thank advantage prime membership packaging although sparse good shape e ship damage open package take item quite disappointed cover seem make poorly manufacturing defect look like dimple raise rib obviously part design two crack cover one run radially steam vent hole top plate another along bottom rib scratch place outside plate suggest poorly make part also damage handle possible get bad sample high quality product positive review would seem suggest matter plan return item buy something local store purchase entire line anolon advance cook ware even purchase ince saut pan twice cook ware excellent every item build great deal quality purchase qt cover winsor stockpot omg difference wall paper thin bottom even close thickness pot pan would dare slow cook product long period time pot dollar go back today unused disappointed anolon product sell many folk anolon line warn item\n",
      "nordic ware microwave plate cover look good online generally high rating go ahead order one arrive promptly thank advantage prime membership packaging although sparse good shape e ship damage open package take item quite disappointed cover seem make poorly manufacturing defect look like dimple raise rib obviously part design two crack cover one run radially steam vent hole top plate another along bottom rib scratch place outside plate suggest poorly make part also damage handle possible get bad sample high quality product positive review would seem suggest matter plan return item buy something local store purchase entire line anolon advance cook ware even purchase ince saut pan twice cook ware excellent every item build great deal quality purchase qt cover winsor stockpot omg difference wall paper thin bottom even close thickness pot pan would dare slow cook product long period time pot dollar go back today unused disappointed anolon product sell many folk anolon line warn item\n",
      "--- 0.4418189525604248 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "## To be removed, took 10 mins on my desktop\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function return wordnet tag based on nltk tag, not used due to slowness\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "# Lemmatize with POS tags, too slow..\n",
    "def lemmatize_sentence(s):\n",
    "    \n",
    "    lemmatized_words = []\n",
    "    \n",
    "    #tokenize the input and get the pos_token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(s))  \n",
    "    #covert (token, pos_tag) -> (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    \n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # if no tag, don't lemmatize\n",
    "            lemmatized_words.append(word)\n",
    "        else:        \n",
    "            #else lemmatize the token with the tag\n",
    "            lemmatized_words.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# simple lemmatize\n",
    "def easy_lemmatize(s):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in s.split(\" \")]\n",
    "    return \" \".join(lemmatized_words)\n",
    "    \n",
    "# use simple lemmatize\n",
    "df_data['review'] = [ easy_lemmatize(review) for review in df_data['review'] ]\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "424d8389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'around',\n",
       " 'children',\n",
       " 'fire',\n",
       " 'for',\n",
       " 'man',\n",
       " 'out',\n",
       " 'sat',\n",
       " 'the',\n",
       " 'walk',\n",
       " 'went'}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docA = 'the man went out for a walk'\n",
    "docB = 'the children sat around the fire'\n",
    "\n",
    "set(docA.split(' ')).union(set(docB.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "950f1cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "\n",
      "      blue  bright  sky  sun\n",
      "Doc1     1       0    1    0\n",
      "Doc2     0       1    0    1\n",
      "\n",
      "TD-IDF Vectorizer\n",
      "\n",
      "          blue    bright       sky       sun\n",
      "Doc1  0.707107  0.000000  0.707107  0.000000\n",
      "Doc2  0.000000  0.707107  0.000000  0.707107\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "# set of documents\n",
    "\n",
    "train = ['The sky is blue.',\n",
    "         'The sun is bright.']\n",
    "test = ['The sun in the sky is bright',\n",
    "        'We can see the shining sun, the bright sun.']\n",
    "\n",
    "# instantiate the vectorizer object\n",
    "countvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n",
    "\n",
    "# convert th documents into a matrix\n",
    "count_wm = countvectorizer.fit_transform(train)\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(train)\n",
    "\n",
    "#count_tokens = tfidfvectorizer.get_feature_names() # no difference\n",
    "count_tokens = countvectorizer.get_feature_names()\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "\n",
    "df_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc1','Doc2'],columns = count_tokens)\n",
    "df_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Doc1','Doc2'],columns = tfidf_tokens)\n",
    "\n",
    "print(\"Count Vectorizer\\n\")\n",
    "print(df_countvect)\n",
    "print(\"\\nTD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715359c",
   "metadata": {},
   "source": [
    "## Practice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "fb78b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('smsspam.txt', sep='\\t', names=['Status', 'Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "c7c04695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Status                                            Message\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "0ce3880c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "da5a9c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "747"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.Status=='spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "8d64291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Status\"] == 'ham', \"Status\"] = 1\n",
    "df.loc[df[\"Status\"] == 'spam', \"Status\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "b01302fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Status                                            Message\n",
       "0      1  Go until jurong point, crazy.. Available only ...\n",
       "1      1                      Ok lar... Joking wif u oni...\n",
       "2      0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      1  U dun say so early hor... U c already then say...\n",
       "4      1  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "9124510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df[\"Message\"]\n",
    "df_y = df[\"Status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "90d42635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Go until jurong point, crazy.. Available only ...\n",
       "1                           Ok lar... Joking wif u oni...\n",
       "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3       U dun say so early hor... U c already then say...\n",
       "4       Nah I don't think he goes to usf, he lives aro...\n",
       "                              ...                        \n",
       "5567    This is the 2nd time we have tried 2 contact u...\n",
       "5568                 Will ü b going to esplanade fr home?\n",
       "5569    Pity, * was in mood for that. So...any other s...\n",
       "5570    The guy did some bitching but I acted like i'd...\n",
       "5571                           Rofl. Its true to its name\n",
       "Name: Message, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "4ec5108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv = CountVectorizer()\n",
    "cv = TfidfVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "6f77562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df[\"Message\"], df[\"Status\"], test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "fd73eebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1457    U sleeping now.. Or you going to take? Haha.. ...\n",
       "472     How long has it been since you screamed, princ...\n",
       "2481    Urgent! call 09066612661 from landline. Your c...\n",
       "243     Okay. No no, just shining on. That was meant t...\n",
       "1413    Wen ur lovable bcums angry wid u, dnt take it ...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "280e00e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1457    1\n",
       "472     1\n",
       "2481    0\n",
       "243     1\n",
       "1413    1\n",
       "Name: Status, dtype: object"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "557a32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traincv = cv.fit_transform([\"Hi how are you, how are you doing?\",\"Hey what's up\", \"Wow cool class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b04f549c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.70710678, 0.        , 0.70710678,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.57735027, 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027]])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_traincv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b640fba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class', 'cool', 'doing', 'hey', 'hi', 'wow']"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "cc070ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv1=CountVectorizer()\n",
    "cv1 = TfidfVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "39af266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainvc=cv1.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "368892d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=x_trainvc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "35b5d4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.19618715, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "3456770c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "167ba756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['checking', 'going', 'got', 'haha', 'lor', 'mails', 'online',\n",
       "        'replying', 'sleeping', 'spys', 'wat'], dtype='<U27')]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv1.inverse_transform(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "6b353aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U sleeping now.. Or you going to take? Haha.. I got spys wat.. Me online checking n replying mails lor..'"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "14870092",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'str' has no attribute 'len'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-360-28c0a407da4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'str' has no attribute 'len'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame([['aa aa', 'aaa', 'a'], ['bbbb', 'bbbbbb', 'aa']], \n",
    "                  columns=['Header1', 'Header2', 'Header3'])\n",
    "result = pd.DataFrame([[]])\n",
    "for col in data:\n",
    "    result[col] = data[col].apply(len).mean()\n",
    "    \n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
