{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a87cb252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\xmh91\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4ade12ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>good but https://www.amazon.com/gp/product/B07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>not good, i needed to solve a problem cost!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                        review_body\n",
       "0     5.0  good but https://www.amazon.com/gp/product/B07...\n",
       "1     2.0        not good, i needed to solve a problem cost!"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dummy.tsv\", sep='\\t', error_bad_lines=False, warn_bad_lines=False)\n",
    "\n",
    "df['review_body'] = [BeautifulSoup(re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', review, flags=re.MULTILINE)).get_text() for review in df['review_body'] ]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "04e07050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Search Images Maps Play YouTube News Gmail Drive More » Web History | Settings | Sign in Advanced search Advertising Programs Business Solutions About Google © 2021 - Privacy - Terms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "result = requests.get(\"https://www.google.com\")\n",
    "src = result.content\n",
    "\n",
    "def remove_url_html(html):\n",
    "  \n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "  \n",
    "    for data in soup(['style', 'script']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "  \n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "  \n",
    "  \n",
    "# Print the extracted data\n",
    "print(remove_tags(src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "63e94762",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "7060122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data frame:\n",
      "\n",
      "   rating                       review_body\n",
      "0       1              user@web.com student\n",
      "1       2     update 11/11/1991, I was born\n",
      "2       3            mr. grapes are 2bad...\n",
      "3       4           mr. grapes are 2bad...\"\n",
      "4       5  this is very small...it's rubber\n",
      "\n",
      "Updated Data frame:\n",
      "\n",
      "   rating                       review_body\n",
      "0       1              user web com student\n",
      "1       2     update             I was born\n",
      "2       3            mr  grapes are  bad   \n",
      "3       4           mr  grapes are  bad    \n",
      "4       5  this is very small   it's rubber\n"
     ]
    }
   ],
   "source": [
    "info= {\"rating\":[1,2,3,4,5], \"review_body\":[\"user@web.com student\",\n",
    "                                       \"update 11/11/1991, I was born\",\n",
    "                                       \"mr. grapes are 2bad...\",\n",
    "                                       'mr. grapes are 2bad...\"',\n",
    "                                        \"this is very small...it's rubber\"]}\n",
    " \n",
    "data = pd.DataFrame(info)\n",
    "print(\"Original Data frame:\\n\")\n",
    "print(data)\n",
    "\n",
    "def remove_non_alphabetical(s):\n",
    "    # remove numbers\n",
    "    #s = re.sub(r'\\d+', '', s)\n",
    "    \n",
    "    # replace non-alphabetical by whitespace\n",
    "    s = re.sub(r\"[^a-zA-Z'’]\", ' ', s)\n",
    "    \n",
    "    # remove punctuation and return\n",
    "    #return ' '.join([word.strip(string.punctuation) for word in s.split(\" \")])\n",
    "    return s\n",
    "                     \n",
    "data[\"review_body\"] = [ remove_non_alphabetical(review) for review in data[\"review_body\"]]\n",
    "print(\"\\nUpdated Data frame:\\n\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d5ad1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "11767f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defective  new design safety mechanisms  at ba\n"
     ]
    }
   ],
   "source": [
    " def contractionfunction(s):\n",
    "\n",
    "    contractions = {\n",
    "        \"a'ight\": \"alright\",\n",
    "        \"ain't\": \"am not\",\n",
    "        \"amn't\": \"am not\",\n",
    "        \"arencha\": \"are not you\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"‘bout\": \"about\",\n",
    "        \"cannot\": \"can not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"cap’n\": \"captain\",\n",
    "        \"cause\": \"because\",\n",
    "        \"’cept\": \"except\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"dammit\": \"damn it\",\n",
    "        \"daren't\": \"dare not\",\n",
    "        \"daresn't\": \"dare not\",\n",
    "        \"dasn't\": \"dare not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"dunno\": \"do not know\",\n",
    "        \"d'ye\": \"did you\",\n",
    "        \"e'en\": \"even\",\n",
    "        \"e'er\": \"ever\",\n",
    "        \"em\": \"them\",\n",
    "        \"everybody's\": \"everybody is\",\n",
    "        \"everyone's\": \"everyone is\",\n",
    "        \"fo’c’sle\": \"forecastle\",\n",
    "        \"’gainst\": \"against\",\n",
    "        \"g'day\": \"good day\",\n",
    "        \"gimme\": \"give me\",\n",
    "        \"giv'n\": \"given\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"gon't\": \"go not\",\n",
    "        \"gotta\": \"got to\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"had've\": \"had have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"helluva\": \"hell of a\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"here's\": \"here is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"howdy\": \"how do you do\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how're\": \"how are\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'd've\": \"i would have\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"imma\": \"i am about to\",\n",
    "        \"i'm'o\": \"i am going to\",\n",
    "        \"innit\": \"is it not\",\n",
    "        \"ion\": \"i do not\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"iunno\": \"i do not know\",\n",
    "        \"kinda\": \"kind of\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"may've\": \"may have\",\n",
    "        \"methinks\": \"i think\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"‘neath\": \"beneath\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"nal\": \"and all\",\n",
    "        \"ne'er\": \"never\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"o'er\": \"over\",\n",
    "        \"ol'\": \"old\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"‘round\": \"around\",\n",
    "        \"s\": \"is\",\n",
    "        \"shalln't\": \"shall not\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"somebody's\": \"somebody is\",\n",
    "        \"someone's\": \"someone is\",\n",
    "        \"something's\": \"something is\",\n",
    "        \"so're\": \"so are\",\n",
    "        \"so’s\": \"so has\",\n",
    "        \"so’ve\": \"so have\",\n",
    "        \"that'll\": \"that will\",\n",
    "        \"that're\": \"that are\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'll\": \"there will\",\n",
    "        \"there're\": \"there are\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"these're\": \"these are\",\n",
    "        \"these've\": \"these have\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"this's\": \"this is\",\n",
    "        \"those're\": \"those are\",\n",
    "        \"those've\": \"those have\",\n",
    "        \"thout\": \"without\",\n",
    "        \"’til\": \"until\",\n",
    "        \"tis\": \"it is\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"twas\": \"it was\",\n",
    "        \"tween\": \"between\",\n",
    "        \"twere\": \"it were\",\n",
    "        \"wanna\": \"want to\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"whatcha\": \"what are you\",\n",
    "        \"what'd\": \"what did\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what're\": \"what are/what were\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where'll\": \"where will\",\n",
    "        \"where're\": \"where are\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"which'd\": \"which had\",\n",
    "        \"which'll\": \"which will\",\n",
    "        \"which're\": \"which are\",\n",
    "        \"which's\": \"which is\",\n",
    "        \"which've\": \"which have\",\n",
    "        \"who'd\": \"who would\",\n",
    "        \"who'd've\": \"who would have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who're\": \"who are\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why'd\": \"why did\",\n",
    "        \"why're\": \"why are\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"willn't\": \"will not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"wonnot\": \"will not\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all'd'n't've\": \"you all would not have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all'ren't\": \"you all are not\",\n",
    "        \"y'at\": \"you at\",\n",
    "        \"yes’m\": \"yes madam\",\n",
    "        \"yessir\": \"yes sir\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for word in s.split(\" \"):\n",
    "        if word in contractions:\n",
    "            s = s.replace(word, contractions[word])\n",
    "    return s\n",
    "\n",
    "text = \"Hey I'm Yann, how're you and how's it going ? That's interesting : I'd love to hear more about it.\".lower()\n",
    "text = \"defective  new design safety mechanisms  at ba\".lower()\n",
    "#text = ' '.join([word.strip(string.punctuation) for word in text.split(\" \")])\n",
    "#text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "res = contractionfunction(text)\n",
    "print(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "98dec3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it works fine but it is definitely very fragile\n"
     ]
    }
   ],
   "source": [
    "## Remove white spaces\n",
    "\n",
    "line1 = '  it works     fine but it is definitely very fragile'\n",
    "line2 = '     not big enough capacity,it should be doubled'\n",
    "\n",
    "line1 = re.sub(' +', ' ', line1)\n",
    "print(line1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "64b959ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My link text\n"
     ]
    }
   ],
   "source": [
    "## Remove html and url\n",
    "\n",
    "info= {\"rating\":[1,2,3], \"review_body\":[\"i needed to solve a problem, cost!<br />this didn't work for that reason.  at my local big box store i can pay an average price of 32 cents for each k cup filled with coffee.  here i have to do the labor.<br />the product works but no money savings.....bummer.\",\n",
    "                                      \"website is https://www.youtube.com/watch?v=_4kHxtiuML0\",\n",
    "                                     \"not secure http://www.abc.com\"]}\n",
    " \n",
    "data = pd.DataFrame(info)\n",
    "#print(\"Original Data frame:\\n\")\n",
    "#print(data)\n",
    "\n",
    "\n",
    "\n",
    "def remove_html_url(s):\n",
    "    # parse html\n",
    "    soup = BeautifulSoup(s, \"html.parser\")\n",
    "    \n",
    "    for data in soup(['style', 'script']):\n",
    "        # remove tags\n",
    "        data.decompose()\n",
    "    # replace url with empty string and return\n",
    "    return re.sub(r\"http\\S+\", \"\", ' '.join(soup.stripped_strings))\n",
    "\n",
    "text = '<html><body><a href=\"http://moo\">My link text</a></body></html>'\n",
    "print(remove_html_url(text))\n",
    "\n",
    "data[\"review_body\"] = [remove_html_url(wat) for wat in data[\"review_body\"]]\n",
    "\n",
    "#print(\"\\nUpdated Data frame:\\n\")\n",
    "#print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7d295d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nordic ware microwave plate cover look good online generally high rating go ahead order one arrive promptly thank advantage prime membership packaging although sparse good shape e ship damage open package take item quite disappointed cover seem make poorly manufacturing defect look like dimple raise rib obviously part design two crack cover one run radially steam vent hole top plate another along bottom rib scratch place outside plate suggest poorly make part also damage handle possible get bad sample high quality product positive review would seem suggest matter plan return item buy something local store purchase entire line anolon advance cook ware even purchase ince saut pan twice cook ware excellent every item build great deal quality purchase qt cover winsor stockpot omg difference wall paper thin bottom even close thickness pot pan would dare slow cook product long period time pot dollar go back today unused disappointed anolon product sell many folk anolon line warn item\n",
      "--- 0.012965202331542969 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tag_dict = {\"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV}\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    # Get the first letter of pos tag of the word\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()   \n",
    "\n",
    "    return tag_dict.get(tag) \n",
    "\n",
    "\n",
    "def get_lemmatized(s):\n",
    "    # Lemmatize the word based on the tags\n",
    "    lemmatized_word = []\n",
    "    \n",
    "    for word in s.split(\" \"):\n",
    "        # get the pos tag of the word\n",
    "        tag = get_wordnet_pos(word)\n",
    "        if tag is None:\n",
    "            lemmatized_word.append(lemmatizer.lemmatize(word))\n",
    "        else:\n",
    "            lemmatized_word.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_word)\n",
    "\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "text = \"nordic ware microwave plate cover look good online generally high rating go ahead order one arrive promptly thank advantage prime membership packaging although sparse good shape e ship damage open package take item quite disappointed cover seem make poorly manufacturing defect look like dimple raise rib obviously part design two crack cover one run radially steam vent hole top plate another along bottom rib scratch place outside plate suggest poorly make part also damage handle possible get bad sample high quality product positive review would seem suggest matter plan return item buy something local store purchase entire line anolon advance cook ware even purchase ince saut pan twice cook ware excellent every item build great deal quality purchase qt cover winsor stockpot omg difference wall paper thin bottom even close thickness pot pan would dare slow cook product long period time pot dollar go back today unused disappointed anolon product sell many folk anolon line warn item\"\n",
    "print(lemmatize_sentence(text))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "424d8389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docA = 'the man went out for a walk'\n",
    "docB = 'the children sat around the fire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "950f1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([docA, docB])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
